# `Flink SQL`

- `Table API`和`SQL`是最上层的`API`，在`Flink`中这两种`API`被集成在一起，`SQL`执行的对象也是`Flink`中的表（`Table`），所以我们一般会认为它们是一体的。
- `SQL API` 是基于 `SQL` 标准的 `Apache Calcite` 框架实现的，可通过纯 `SQL` 来开发和运行一个`Flink` 任务。

##  `sql-client`准备

### 基于`yarn-session`模式

1. 启动`Flink`

   ~~~
   /opt/module/flink-1.17.0/bin/yarn-session.sh -d
   ~~~

2. 启动`Flink`的`sql-client`

   ~~~
   /opt/module/flink-1.17.0/bin/sql-client.sh embedded -s yarn-session
   ~~~

### 常用配置

1. 结果显示模式

   ~~~
   #默认table，还可以设置为tableau、changelog
   SET sql-client.execution.result-mode=tableau;
   ~~~

2. 执行环境

   ~~~
   SET execution.runtime-mode=streaming; #默认streaming，也可以设置batch
   ~~~

3. 默认并行度

   ~~~
   SET parallelism.default=1;
   ~~~

4. 设置状态`TTL`

   ~~~
   SET table.exec.state.ttl=1000;
   ~~~

5. 通过`sql`文件初始化

   1. 创建`sql`文件

      ~~~
      vim conf/sql-client-init.sql
      
      SET sql-client.execution.result-mode=tableau;
      CREATE DATABASE mydatabase;
      ~~~

   2. 启动时，指定`sql`文件

      ~~~
      /opt/module/flink-1.17.0/bin/sql-client.sh embedded -s yarn-session -i conf/sql-client-init.sql
      ~~~

## 流处理中的表

### 动态表和持续查询

- 流处理面对的数据是连续不断的，这导致了流处理中的“表”跟我们熟悉的关系型数据库中的表完全不同；而基于表执行的查询操作，也就有了新的含义。

#### 动态表（`Dynamic Tables`）

- 当流中有新数据到来，初始的表中会插入一行；而基于这个表定义的`SQL`查询，就应该在之前的基础上更新结果。这样得到的表就会不断地动态变化，被称为“动态表”（`Dynamic Tables`）。
- 动态表是`Flink`在`Table API`和`SQL`中的核心概念，它为流数据处理提供了表和`SQL`支持。

#### 持续查询（`Continuous Query`）

- 动态表可以像静态的批处理表一样进行查询操作。由于数据在不断变化，因此基于它定义的`SQL`查询也不可能执行一次就得到最终结果。这样一来，我们对动态表的查询也就永远不会停止，一直在随着新数据的到来而继续执行。这样的查询就被称作“持续查询”（`Continuous Query`）。对动态表定义的查询操作，都是持续查询；而持续查询的结果也会是一个动态表。
- 由于每次数据到来都会触发查询操作，因此可以认为一次查询面对的数据集，就是当前输入动态表中收到的所有数据。这相当于是对输入动态表做了一个“快照”（`snapshot`），当作有限数据集进行批处理；流式数据的到来会触发连续不断的快照查询，像动画一样连贯起来，就构成了“持续查询”。

- 持续查询的步骤如下：

  1. 流（`stream`）被转换为动态表（`dynamic table`）；
  2. 对动态表进行持续查询（`continuous query`），生成新的动态表；
  3. 生成的动态表被转换成流。

  - 这样，只要`API`将流和动态表的转换封装起来，我们就可以直接在数据流上执行`SQL`查询，用处理表的方式来做流处理了。

### 将流转换成动态表

- 如果把流看作一张表，那么流中每个数据的到来，都应该看作是对表的一次插入（`Insert`）操作，会在表的末尾添加一行数据。因为流是连续不断的，而且之前的输出结果无法改变、只能在后面追加；所以我们其实是通过一个只有插入操作（`insert-only`）的更新日志（`changelog`）流，来构建一个表。

### 用SQL持续查询

1. 更新（`Update`）查询

   ~~~
   Table urlCountTable = tableEnv.sqlQuery("SELECT user, COUNT(url) as cnt FROM EventTable GROUP BY user");
   ~~~

   - 当原始动态表不停地插入新的数据时，查询得到的`urlCountTable`会持续地进行更改。由于`count`数量可能会叠加增长，因此这里的更改操作可以是简单的插入（`Insert`），也可以是对之前数据的更新（`Update`）。这种持续查询被称为更新查询（``Update Query`），更新查询得到的结果表如果想要转换成`DataStream`，必须调用`toChangelogStream()``方法。

2. 追加（`Append`）查询

   - 上面的例子中，查询过程用到了分组聚合，结果表中就会产生更新操作。如果我们执行一个简单的条件查询，结果表中就会像原始表`EventTable`一样，只有插入（`Insert`）操作了。

     ~~~sql
     Table aliceVisitTable = tableEnv.sqlQuery("SELECT url, user FROM EventTable WHERE user = 'Cary'");
     ~~~

     - 这样的持续查询，就被称为追加查询（`Append Query`），它定义的结果表的更新日志（`changelog`）流中只有`INSERT`操作。
     - 由于窗口的统计结果是一次性写入结果表的，所以结果表的更新日志流中只会包含插入`INSERT`操作，而没有更新`UPDATE`操作。所以这里的持续查询，依然是一个追加（`Append`）查询。结果表`result`如果转换成`DataStream`，可以直接调用`toDataStream()`方法。

### 将动态表转换为流

- 与关系型数据库中的表一样，动态表也可以通过插入（`Insert`）、更新（`Update`）和删除（`Delete`）操作，进行持续的更改。将动态表转换为流或将其写入外部系统时，就需要对这些更改操作进行编码，通过发送编码消息的方式告诉外部系统要执行的操作。在`Flink`中，`Table API`和`SQL`支持三种编码方式：
  - 仅追加（`Append-only`）流
    - 仅通过插入（`Insert`）更改来修改的动态表，可以直接转换为“仅追加”流。这个流中发出的数据，其实就是动态表中新增的每一行。
  - 撤回（`Retract`）流
    - 撤回流是包含两类消息的流，添加（`add`）消息和撤回（`retract`）消息。
    - 具体的编码规则是：`INSERT`插入操作编码为`add`消息；`DELETE`删除操作编码为`retract`消息；而`UPDATE`更新操作则编码为被更改行的`retract`消息，和更新后行（新行）的`add`消息。
  - 更新插入（`Upsert`）流
    - 更新插入流中只包含两种类型的消息：更新插入（`upsert`）消息和删除（`delete`）消息。
    - 所谓的“`upsert`”其实是“`update`”和“`insert`”的合成词，所以对于更新插入流来说，`INSERT`插入操作和`UPDATE`更新操作，统一被编码为`upsert`消息；而`DELETE`删除操作则被编码为`delete`消息。
    - 需要注意的是，在代码里将动态表转换为`DataStream`时，只支持仅追加（`append-only`）和撤回（`retract`）流，我们调用`toChangelogStream()`得到的其实就是撤回流。

##  时间属性

- 基于时间的操作（比如时间窗口），需要定义相关的时间语义和时间数据来源的信息。在`Table API`和`SQL`中，会给表单独提供一个逻辑上的时间字段，专门用来在表处理程序中指示时间。
- 所以所谓的时间属性（`time attributes`），其实就是每个表模式结构（`schema`）的一部分。它可以在创建表的`DDL`里直接定义为一个字段，也可以在`DataStream`转换成表时定义。一旦定义了时间属性，它就可以作为一个普通字段引用，并且可以在基于时间的操作中使用。
- 按照时间语义的不同，可以把时间属性的定义分成事件时间（`event time`）和处理时间（`processing time`）两种情况。

### 事件时间

- 事件时间属性可以在创建表`DDL`中定义，增加一个字段，通过`WATERMARK`语句来定义事件时间属性。具体定义方式如下：

  ~~~
  CREATE TABLE EventTable(
    user STRING,
    url STRING,
    ts TIMESTAMP(3),
    WATERMARK FOR ts AS ts - INTERVAL '5' SECOND
  ) WITH (
    ...
  );
  ~~~

- 这里我们把`ts`字段定义为事件时间属性，而且基于`ts`设置了`5`秒的水位线延迟。

- 时间戳类型必须是 `TIMESTAMP` 或者`TIMESTAMP_LTZ `类型。但是时间戳一般都是秒或者是毫秒（`BIGINT` 类型），这种情况可以通过如下方式转换

  ~~~
  ts BIGINT,
  time_ltz AS TO_TIMESTAMP_LTZ(ts, 3),
  ~~~

### 处理时间

- 在定义处理时间属性时，必须要额外声明一个字段，专门用来保存当前的处理时间。

- 在创建表的`DDL`（`CREATE TABLE`语句）中，可以增加一个额外的字段，通过调用系统内置的`PROCTIME()`函数来指定当前的处理时间属性。

  ~~~
  CREATE TABLE EventTable(
    user STRING,
    url STRING,
    ts AS PROCTIME()
  ) WITH (
    ...
  );
  ~~~

### `DDL（Data Definition Language）`数据定义

#### 数据库

1. 创建数据库

   1. 语法

      ~~~
      CREATE DATABASE [IF NOT EXISTS] [catalog_name.]db_name
        [COMMENT database_comment]
        WITH (key1=val1, key2=val2, ...)
      ~~~

2. 查询数据库

   1. 查询所有数据库

      ~~~
      SHOW DATABASES
      ~~~

   2. 查询当前数据库

      ~~~
      SHOW CURRENT DATABASE
      ~~~

   3. 修改数据库

      ~~~
      ALTER DATABASE [catalog_name.]db_name SET (key1=val1, key2=val2, ...)
      ~~~

   4. 删除数据库

      ~~~
      DROP DATABASE [IF EXISTS] [catalog_name.]db_name [ (RESTRICT | CASCADE) ]
      ~~~

      - `RESTRICT`：删除非空数据库会触发异常。默认启用
      - `CASCADE`：删除非空数据库也会删除所有相关的表和函数

      ~~~
      DROP DATABASE db_flink2;
      ~~~

   5. 切换当前数据库

      ~~~
      USE database_name;
      ~~~

#### 表

##### 创建表

~~~
CREATE TABLE [IF NOT EXISTS] [catalog_name.][db_name.]table_name
  (
    { <physical_column_definition> | <metadata_column_definition> | <computed_column_definition> }[ , ...n]
    [ <watermark_definition> ]
    [ <table_constraint> ][ , ...n]
  )
  [COMMENT table_comment]
  [PARTITIONED BY (partition_column_name1, partition_column_name2, ...)]
  WITH (key1=val1, key2=val2, ...)
  [ LIKE source_table [( <like_options> )] | AS select_query ]
~~~

1. `physical_column_definition`
   - 物理列是数据库中所说的常规列。其定义了物理介质中存储的数据中字段的名称、类型和顺序。其他类型的列可以在物理列之间声明，但不会影响最终的物理列的读取。

2. `metadata_column_definition`

   - 元数据列是 `SQL` 标准的扩展，允许访问数据源本身具有的一些元数据。元数据列由 `METADATA` 关键字标识。例如，我们可以使用元数据列从`Kafka`记录中读取和写入时间戳，用于基于时间的操作（这个时间戳不是数据中的某个时间戳字段，而是数据写入 `Kafka` 时，`Kafka` 引擎给这条数据打上的时间戳标记）。`connector`和`format`文档列出了每个组件可用的元数据字段。

     ~~~
     CREATE TABLE MyTable (
       `user_id` BIGINT,
       `name` STRING,
       `record_time` TIMESTAMP_LTZ(3) METADATA FROM 'timestamp'
     ) WITH (
       'connector' = 'kafka'
       ...
     );
     ~~~

     - 如果自定义的列名称和 `Connector` 中定义 `metadata` 字段的名称一样， `FROM xxx` 子句可省略

     ~~~
     CREATE TABLE MyTable (
     `user_id` BIGINT,
     `name` STRING,
     `timestamp` TIMESTAMP_LTZ(3) METADATA
     ) WITH (
     'connector' = 'kafka'
     ...
     );
     ~~~

   - 如果自定义列的数据类型和 `Connector` 中定义的 `metadata` 字段的数据类型不一致，程序运行时会自动 `cast` 强转，但是这要求两种数据类型是可以强转的。

     ~~~
     CREATE TABLE MyTable (
     `user_id` BIGINT,
     `name` STRING,
     -- 将时间戳强转为 BIGINT
     `timestamp` BIGINT METADATA
     ) WITH (
     'connector' = 'kafka'
     ...
     );
     ~~~

   - 默认情况下，`Flink SQL planner` 认为 `metadata` 列可以读取和写入。然而，在许多情况下，外部系统提供的只读元数据字段比可写字段多。因此，可以使用VIRTUAL关键字排除元数据列的持久化(表示只读)。

     ~~~
     CREATE TABLE MyTable (
       `timestamp` BIGINT METADATA, 
       `offset` BIGINT METADATA VIRTUAL,
       `user_id` BIGINT,
       `name` STRING,
     ) WITH (
       'connector' = 'kafka'
       ...
     );
     ~~~

3. `computed_column_definition`

   - 计算列是使用语法`column_name AS computed_column_expression`生成的虚拟列。
   - 计算列就是拿已有的一些列经过一些自定义的运算生成的新列，在物理上并不存储在表中，只能读不能写。列的数据类型从给定的表达式自动派生，无需手动声明

   ~~~
   CREATE TABLE MyTable (
     `user_id` BIGINT,
     `price` DOUBLE,
     `quantity` DOUBLE,
     `cost` AS price * quanitity
   ) WITH (
     'connector' = 'kafka'
     ...
   );
   ~~~

4. 定义`Watermark`

   - `Flink SQL` 提供了几种 `WATERMARK` 生产策略：

     - 严格升序：`WATERMARK FOR rowtime_column AS rowtime_column`。
     - `Flink` 任务认为时间戳只会越来越大，也不存在相等的情况，只要相等或者小于之前的，就认为是迟到的数据。
     - 递增：`WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL '0.001' SECOND `。
       - 一般基本不用这种方式。如果设置此类，则允许有相同的时间戳出现。

     - 有界无序： `WATERMARK FOR rowtime_column AS rowtime_column – INTERVAL 'string' timeUnit `。
       - 此类策略就可以用于设置最大乱序时间，假如设置为 `WATERMARK FOR rowtime_column AS rowtime_column - INTERVAL '5' SECOND` ，则生成的是运行 `5s` 延迟的`Watermark`。一般都用这种 `Watermark` 生成策略，此类 `Watermark` 生成策略通常用于有数据乱序的场景中，而对应到实际的场景中，数据都是会存在乱序的，所以基本都使用此类策略。

5. `PRIMARY KEY`

   - 主键约束表明表中的一列或一组列是唯一的，并且它们不包含`NULL`值。主键唯一地标识表中的一行，只支持 `not enforced`。

     ~~~
     CREATE TABLE MyTable (
     `user_id` BIGINT,
     `name` STRING,
     PARYMARY KEY(user_id) not enforced
     ) WITH (
     'connector' = 'kafka'
     ...
     );
     ~~~

6. `PARTITIONED BY`

   - 创建分区表

7. `with`语句

   - 用于创建表的表属性，用于指定外部存储系统的元数据信息。配置属性时，表达式`key1=val1`的键和值都应该是字符串字面值。如下是`Kafka`的映射表：

     ~~~
     CREATE TABLE KafkaTable (
     `user_id` BIGINT,
     `name` STRING,
     `ts` TIMESTAMP(3) METADATA FROM 'timestamp'
     ) WITH (
     'connector' = 'kafka',
     'topic' = 'user_behavior',
     'properties.bootstrap.servers' = 'localhost:9092',
     'properties.group.id' = 'testGroup',
     'scan.startup.mode' = 'earliest-offset',
     'format' = 'csv'
     )
     ~~~

   - 一般 `with` 中的配置项由 `Flink SQL` 的 `Connector`（链接外部存储的连接器） 来定义，每种 `Connector` 提供的 `with` 配置项都是不同的。

8. `LIKE`

   - 用于基于现有表的定义创建表。此外，用户可以扩展原始表或排除表的某些部分。

   - 可以使用该子句重用(可能还会覆盖)某些连接器属性，或者向外部定义的表添加水印。

     ~~~
     CREATE TABLE Orders (
         `user` BIGINT,
         product STRING,
         order_time TIMESTAMP(3)
     ) WITH ( 
         'connector' = 'kafka',
         'scan.startup.mode' = 'earliest-offset'
     );
     
     CREATE TABLE Orders_with_watermark (
         -- Add watermark definition
         WATERMARK FOR order_time AS order_time - INTERVAL '5' SECOND 
     ) WITH (
         -- Overwrite the startup-mode
         'scan.startup.mode' = 'latest-offset'
     )
     LIKE Orders;
     ~~~

9. `AS select_statement（CTAS）`

   - 在一个`create-table-as-select (CTAS)`语句中，还可以通过查询的结果创建和填充表。`CTAS`是使用单个命令创建数据并向表中插入数据的最简单、最快速的方法。

     ~~~
     CREATE TABLE my_ctas_table
     WITH (
         'connector' = 'kafka',
         ...
     )
     AS SELECT id, name, age FROM source_table WHERE mod(id, 10) = 0;
     ~~~

   - 注意:`CTAS`有以下限制:

     - 暂不支持创建临时表。
     - 目前还不支持指定显式列。
     - 还不支持指定显式水印。
     - 目前还不支持创建分区表。
     - 目前还不支持指定主键约束。

##### 查看表

- 查看所有表

  ~~~
  SHOW TABLES [ ( FROM | IN ) [catalog_name.]database_name ] [ [NOT] LIKE <sql_like_pattern> ]
  ~~~

  - 如果没有指定数据库，则从当前数据库返回表。
  - `LIKE`子句中`sql pattern`的语法与`MySQL`方言的语法相同:
    - `%`匹配任意数量的字符，甚至零字符，`\%`匹配一个`%`字符。
    - `_`只匹配一个字符，`\_`只匹配一个`'_'`字符

- 查看表信息 

  ~~~
  { DESCRIBE | DESC } [catalog_name.][db_name.]table_name
  ~~~

##### 修改表

1. 修改表名

   ~~~
   ALTER TABLE [catalog_name.][db_name.]table_name RENAME TO new_table_name
   ~~~

2. 修改表属性

   ~~~
   ALTER TABLE [catalog_name.][db_name.]table_name SET (key1=val1, key2=val2, ...)
   ~~~

3. 删除表

   ~~~
   DROP [TEMPORARY] TABLE [IF EXISTS] [catalog_name.][db_name.]table_name
   ~~~

####  查询

##### 分组窗口聚合

- 从`1.13`版本开始，分组窗口聚合已经标记为过时，鼓励使用更强大、更有效的窗口`TVF`聚合，在这里简单做个介绍。

- 直接把窗口自身作为分组`key`放在`GROUP BY`之后的，所以也叫“分组窗口聚合”。`SQL`查询的分组窗口是通过 `GROUP BY` 子句定义的。类似于使用常规 `GROUP BY` 语句的查询，窗口分组语句的 `GROUP BY` 子句中带有一个窗口函数为每个分组计算出一个结果。

- `SQL`中只支持基于时间的窗口，不支持基于元素个数的窗口。

  | 分组窗口函数                         | 描述                                                         |
  | ------------------------------------ | ------------------------------------------------------------ |
  | `TUMBLE(time_attr,  interval)  `     | 定义一个滚动窗口。滚动窗口把行分配到有固定持续时间（ `interval` ）的不重叠的连续窗口。比如，`5` 分钟的滚动窗口以 `5` 分钟为间隔对行进行分组。滚动窗口可以定义在事件时间（批处理、流处理）或处理时间（流处理）上。 |
  | `HOP(time_attr, interval, interval)` | 定义一个跳跃的时间窗口（在 `Table API` 中称为滑动窗口）。滑动窗口有一个固定的持续时间（ 第二个 `interval` 参数 ）以及一个滑动的间隔（第一个 `interval` 参数 ）。若滑动间隔小于窗口的持续时间，滑动窗口则会出现重叠；因此，行将会被分配到多个窗口中。比如，一个大小为 `15` 分组的滑动窗口，其滑动间隔为 `5` 分钟，将会把每一行数据分配到 `3` 个 `15` 分钟的窗口中。滑动窗口可以定义在事件时间（批处理、流处理）或处理时间（流处理）上。 |
  | `SESSION(time_attr,  interval)  `    | 定义一个会话时间窗口。会话时间窗口没有一个固定的持续时间，但是它们的边界会根据 `interval` 所定义的不活跃时间所确定；即一个会话时间窗口在定义的间隔时间内没有时间出现，该窗口会被关闭。例如时间窗口的间隔时间是 `30` 分钟，当其不活跃的时间达到`30`分钟后，若观测到新的记录，则会启动一个新的会话时间窗口（否则该行数据会被添加到当前的窗口），且若在 `30` 分钟内没有观测到新纪录，这个窗口将会被关闭。会话时间窗口可以使用事件时间（批处理、流处理）或处理时间（流处理）。 |

- 可以使用以下辅助函数选择组窗口的开始和结束时间戳以及时间属性

  | 辅助函数                                                     | 描述                                                         |
  | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | `TUMBLE_START(time_attr,interval) `<br />`HOP_START(time_attr,interval,interval) `<br />`SESSION_START(time_attr,interval)` | 返回相对应的滚动、滑动和会话窗口范围内的下界时间戳           |
  | `TUMBLE_END(time_attr,interval)`<br />` HOP_END(time_attr,interval,interval)`<br />`SESSION_END(time_attr,interval)` | 返回相对应的滚动、滑动和会话窗口 范围以外的上界时间戳<br />注意：范围以外的上界时间戳不可以在随后基于时间的操作中，作为 行时间属性 使用，比如 `interval join`以及 分组窗口或分组窗口上的聚合 |
  | `TUMBLE_ROWTIME(time_attr,interval)`<br />`HOP_ROWTIME(time_attr,interval,interval)`<br />`SESSION_ROWTIME(time_attr,interval)` | 返回相对应的滚动、滑动和会话窗口范围以内的上界时间戳<br />返回的是一个可用于后续需要基于时间的操作的时间属性(`rowtime attribute`)。<br />比如`interval join`以及 分组窗口或分组窗口上的聚合 |
  | `TUMBLE_ROCTIME(time_attr,interval)`<br />`HOP_PROCTIME(time_attr,interval,interval)`<br />`SESSION_PROCTIME(time_attr,interval)` | 返回一个可用于后续需要基于时间的操作的处理时间参数，比如 `interval join`以及 分组窗口或分组窗口上的聚合 |

##### 窗口表值函数（`TVF`）聚合

- 对比`GroupWindow`，`TVF`窗口更有效和强大。包括：

  - 提供更多的性能优化手段
  - 支持`GroupingSets`语法
  - 可以在`window`聚合中使用`TopN`
  - 提供累积窗口

- 对于窗口表值函数，窗口本身返回的是就是一个表，所以窗口会出现在`FROM`后面，`GROUP BY`后面的则是窗口新增的字段`window_start`和`window_end`

  ~~~
  FROM TABLE(
  窗口类型(TABLE 表名, DESCRIPTOR(时间字段),INTERVAL时间…)
  )
  GROUP BY [window_start,][window_end,] --可选
  ~~~

##### `Over` 聚合

- `OVER`聚合为一系列有序行的每个输入行计算一个聚合值。与`GROUP BY`聚合相比，`OVER`聚合不会将每个组的结果行数减少为一行。相反，`OVER`聚合为每个输入行生成一个聚合值。

- 可以在事件时间或处理时间，以及指定为时间间隔、或行计数的范围内，定义`Over windows`。

  1. 语法

     ~~~
     SELECT
       agg_func(agg_col) OVER (
         [PARTITION BY col1[, col2, ...]]
         ORDER BY time_col
         range_definition),
       ...
     FROM ...
     ~~~

     - `ORDER BY`：必须是时间戳列（事件时间、处理时间），只能升序
     - `PARTITION BY`：标识了聚合窗口的聚合粒度
     - `range_definition`：这个标识聚合窗口的聚合数据范围，在 `Flink` 中有两种指定数据范围的方式。第一种为按照行数聚合，第二种为按照时间区间聚合

##### 特殊语法 —— TOP-N

1. 语法

   ~~~
   SELECT [column_list]
   FROM (
   SELECT [column_list],
   ROW_NUMBER() OVER ([PARTITION BY col1[, col2...]]
   ORDER BY col1 [asc|desc][, col2 [asc|desc]...]) AS rownum
   FROM table_name)
   WHERE rownum <= N [AND conditions]
   ~~~

   - `ROW_NUMBER() `：标识 `TopN` 排序子句
   - `PARTITION BY col1[, col2...]` ：标识分区字段，代表按照这个 `col` 字段作为分区粒度对数据进行排序取 `topN`，比如下述案例中的 `partition by key` ，就是根据需求中的搜索关键词（`key`）做为分区
   - `ORDER BY col1 [asc|desc][, col2 [asc|desc]...]` ：标识 `TopN` 的排序规则，是按照哪些字段、顺序或逆序进行排序，可以不是时间字段，也可以降序（`TopN`特殊支持）
   - `WHERE rownum <= N` ：这个子句是一定需要的，只有加上了这个子句，`Flink` 才能将其识别为一个`TopN` 的查询，其中 `N` 代表 `TopN` 的条目数
   - `[AND conditions] `：其他的限制条件也可以加上

##### 特殊语法 —— `Deduplication`去重

- 去重，也即上文介绍到的`TopN` 中 `row_number = 1` 的场景，但是这里有一点不一样在于其排序字段一定是时间属性列，可以降序，不能是其他非时间属性的普通列。

- 在 `row_number = 1` 时，如果排序字段是普通列 `planner` 会翻译成 `TopN` 算子，如果是时间属性列 `planner` 会翻译成 `Deduplication`，这两者最终的执行算子是不一样的，`Deduplication` 相比 `TopN` 算子专门做了对应的优化，性能会有很大提升。

- 如果是按照时间属性字段降序，表示取最新一条，会造成不断的更新保存最新的一条。如果是升序，表示取最早的一条，不用去更新，性能更好。

- 语法

  ~~~
  SELECT [column_list]
  FROM (
  SELECT [column_list],
  ROW_NUMBER() OVER ([PARTITION BY col1[, col2...]]
  ORDER BY time_attr [asc|desc]) AS rownum
  FROM table_name)
  WHERE rownum = 1
  ~~~

#####  联结（`Join`）查询

- 在标准`SQL`中，可以将多个表连接合并起来，从中查询出想要的信息；这种操作就是表的联结（`Join`）。在`Flink SQL`中，同样支持各种灵活的联结（`Join`）查询，操作的对象是动态表。
- 在流处理中，动态表的`Join`对应着两条数据流的`Join`操作。`Flink SQL`中的联结查询大体上也可以分为两类：`SQL`原生的联结查询方式，和流处理中特有的联结查询。

###### 常规联结查询

- 常规联结（`Regular Join`）是`SQL`中原生定义的`Join`方式，是最通用的一类联结操作。它的具体语法与标准`SQL`的联结完全相同，通过关键字`JOIN`来联结两个表，后面用关键字`ON`来指明联结条件。
- 与标准`SQL`一致，`Flink SQL`的常规联结也可以分为内联结（`INNER JOIN`）和外联结（`OUTER JOIN`），区别在于结果中是否包含不符合联结条件的行。
- `Regular Join` 包含以下几种（以 `L` 作为左流中的数据标识， `R` 作为右流中的数据标识）：
  - `Inner Join`（`Inner Equal Join`）：流任务中，只有两条流 `Join` 到才输出，输出 `+[L, R]`
  - `Left Join（Outer Equal Join）`：流任务中，左流数据到达之后，无论有没有 `Join` 到右流的数据，都会输出（`Join` 到输出 +`[L, R]` ，没 `Join` 到输出 `+[L, null]` ），如果右流之后数据到达之后，发现左流之前输出过没有 `Join` 到的数据，则会发起回撤流，先输出 `-[L, null]` ，然后输出 `+[L, R]`
  - `Right Join（Outer Equal Join）`：有 `Left Join` 一样，左表和右表的执行逻辑完全相反
  - `Full Join（Outer Equal Join）`：流任务中，左流或者右流的数据到达之后，无论有没有 `Join` 到另外一条流的数据，都会输出（对右流来说：`Join` 到输出 `+[L, R]` ，没 `Join` 到输出 `+[null, R]` ；对左流来说：`Join` 到输出 `+[L, R]` ，没 `Join` 到输出 `+[L, null]` ）。如果一条流的数据到达之后，发现之前另一条流之前输出过没有 `Join` 到的数据，则会发起回撤流（左流数据到达为例：回撤 `-[null, R]` ，输出`+[L, R]` ，右流数据到达为例：回撤 `-[L, null]` ，输出 `+[L, R]`

- `Regular Join` 的注意事项：
  - 实时 `Regular Join` 可以不是 等值 `join` 。等值 `join` 和 非等值 `join` 区别在于， 等值 `join`数据 `shuffle` 策略是 `Hash`，会按照 `Join on` 中的等值条件作为 `id` 发往对应的下游； 非等值 `join` 数据 `shuffle` 策略是 `Global`，所有数据发往一个并发，按照非等值条件进行关联
  - 流的上游是无限的数据，所以要做到关联的话，`Flink` 会将两条流的所有数据都存储在 `State` 中，所以 `Flink` 任务的 `State` 会无限增大，因此你需要为 `State` 配置合适的 `TTL`，以防止 `State` 过大。

###### 等值内联结（`INNER Equi-JOIN`）

- 内联结用`INNER JOIN`来定义，会返回两表中符合联接条件的所有行的组合，也就是所谓的笛卡尔积（`Cartesian product`）。目前仅支持等值联结条件。

###### 等值外联结（`OUTER Equi-JOIN`）

-  与内联结类似，外联结也会返回符合联结条件的所有行的笛卡尔积；另外，还可以将某一侧表中找不到任何匹配的行也单独返回。
- `Flink SQL`支持左外（`LEFT JOIN`）、右外（`RIGHT JOIN`）和全外（`FULL OUTER JOIN`），分别表示会将左侧表、右侧表以及双侧表中没有任何匹配的行返回。

###### 间隔联结查询

- `DataStream API`中的双流`Join`，包括窗口联结（`window join`）和间隔联结（`interval join`）。两条流的`Join`就对应着`SQL`中两个表的`Join`，这是流处理中特有的联结方式。目前`Flink SQL`还不支持窗口联结，而间隔联结则已经实现。
- 间隔联结（`Interval Join`）返回的，同样是符合约束条件的两条中数据的笛卡尔积。只不过这里的“约束条件”除了常规的联结条件外，还多了一个时间间隔的限制。具体语法有以下要点：
  - 两表的联结
    - 间隔联结不需要用`JOIN`关键字，直接在`FROM`后将要联结的两表列出来就可以，用逗号分隔。这与标准`SQL`中的语法一致，表示一个“交叉联结”（`Cross Join`），会返回两表中所有行的笛卡尔积。
  - 联结条件
    - 联结条件用`WHERE`子句来定义，用一个等值表达式描述。交叉联结之后再用`WHERE`进行条件筛选，效果跟内联结`INNER JOIN ... ON ...`非常类似。
  - 时间间隔限制
    - 我们可以在`WHERE`子句中，联结条件后用`AND`追加一个时间间隔的限制条件；做法是提取左右两侧表中的时间字段，然后用一个表达式来指明两者需要满足的间隔限制。具体定义方式有下面三种，这里分别用`ltime`和`rtime`表示左右表中的时间字段：
      1. `ltime = rtime`
      2. `ltime >= rtime AND ltime < rtime + INTERVAL '10' MINUTE`
      3. `ltime BETWEEN rtime - INTERVAL '10' SECOND AND rtime + INTERVAL '5' SECOND`

###### 维表联结查询

- `Lookup Join` 其实就是维表 `Join`，实时获取外部缓存的 `Join`，`Lookup` 的意思就是实时查找。
- 上面说的这几种 `Join` 都是流与流之间的 `Join`，而 `Lookup Join` 是流与 `Redis`，`Mysql`，`HBase` 这种外部存储介质的 `Join`。仅支持处理时间字段。

##### Order by 和 limit

1. `order by`
   - 支持 `Batch\Streaming`，但在实时任务中一般用的非常少。
   - 实时任务中，`Order By` 子句中必须要有时间属性字段，并且必须写在最前面且为升序。

###### SQL Hints

- 在执行查询时，可以在表名后面添加`SQL Hints`来临时修改表属性，对当前`job`生效。

## 常用 `Connector`读写

- `DataGen`和`Print`都是一种`connector`，其他`connector`参考官网：
  -  https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/overview/

## sql-client 中使用 savepoint

1. 提交一个`insert`作业，可以给作业设置名称

   ~~~
   INSERT INTO sink select  * from source;
   ~~~

2. 查看`job`列表

   ~~~
   SHOW JOBS;
   ~~~

3. 停止作业，触发`savepoint`

   ~~~
   SET state.checkpoints.dir='hdfs://hadoop102:8020/chk';
   SET state.savepoints.dir='hdfs://hadoop102:8020/sp';
   
   STOP JOB '228d70913eab60dda85c5e7f78b5782c' WITH SAVEPOINT;
   ~~~

4. 从`savepoint`恢复

   ~~~
   -- 设置从savepoint恢复的路径 
   SET execution.savepoint.path='hdfs://hadoop102:8020/sp/savepoint-37f5e6-0013a2874f0a';  
   
   -- 之后直接提交sql，就会从savepoint恢复
   
   --允许跳过无法还原的保存点状态
   set 'execution.savepoint.ignore-unclaimed-state' = 'true'; 
   ~~~

5. 恢复后重置路径

   - 指定`execution.savepoint.path`后，将影响后面执行的所有`DML`语句，可以使用`RESET`命令重置这个配置选项。

     ~~~
     RESET execution.savepoint.path;
     ~~~

   - 如果出现`reset`没生效的问题，可能是个`bug`，我们可以退出`sql-client`，再重新进，不需要重启`flink`的集群。

##  Catalog

- `Catalog` 提供了元数据信息，例如数据库、表、分区、视图以及数据库或其他外部系统中存储的函数和信息。
- 数据处理最关键的方面之一是管理元数据。元数据可以是临时的，例如临时表、`UDF`。 元数据也可以是持久化的，例如 `Hive` `MetaStore` 中的元数据。`Catalog` 提供了一个统一的`API`，用于管理元数据，并使其可以从 `Table API` 和 `SQL` 查询语句中来访问。
- `Catalog` 允许用户引用其数据存储系统中现有的元数据，并自动将其映射到 `Flink` 的相应元数据。例如，`Flink` 可以直接使用 `Hive MetaStore` 中的表的元数据，不必在`Flink`中手动重写`ddl`，也可以将 `Flink SQL` 中的元数据存储到 `Hive MetaStore` 中。`Catalog` 极大地简化了用户开始使用 `Flink` 的步骤，并极大地提升了用户体验。

### Catalog类型

- `HiveCatalog`：有两个用途，一是单纯作为 `Flink` 元数据的持久化存储，二是作为读写现有 `Hive` 元数据的接口。注意：`Hive MetaStore` 以小写形式存储所有元数据对象名称。`Hive Metastore`以小写形式存储所有元对象名称，而 `GenericInMemoryCatalog`会区分大小写。

#### HiveCatalog

1. 上传所需`jar`包到`lib`下

   ~~~
   cp flink-sql-connector-hive-3.1.3_2.12-1.17.0.jar /opt/module/flink-1.17.0/lib/
   
   cp mysql-connector-j-8.0.31.jar /opt/module/flink-1.17.0/lib/
   ~~~

2. 更换`planner`依赖

   - 只有在使用`Hive`方言或`HiveServer2`时才需要这样额外的计划器`jar`移动，但这是`Hive`集成的推荐设置。

     ~~~
     mv /opt/module/flink-1.17.0/opt/flink-table-planner_2.12-1.17.0.jar /opt/module/flink-1.17.0/lib/flink-table-planner_2.12-1.17.0.jar
     
     mv /opt/module/flink-1.17.0/lib/flink-table-planner-loader-1.17.0.jar /opt/module/flink-1.17.0/opt/flink-table-planner-loader-1.17.0.jar
     ~~~

3. 重启`flink`集群和`sql-client`

4. 启动外置的`hive metastore`服务

   - `Hive metastore`必须作为独立服务运行，也就是`hive-site`中必须配置`hive.metastore.uris`

     ~~~
     hive --service metastore &
     ~~~

5. 创建`Catalog`

   | 配置项             | 必需  | 默认值    | 类型     | 说明                                                         |
   | ------------------ | ----- | --------- | -------- | ------------------------------------------------------------ |
   | `type`             | `Yes` | `(none)`  | `String` | `Catalog`类型，创建`HiveCatalog`时必须设置为`'hive'`。       |
   | `name`             | `Yes` | `(none)`  | `String` | `Catalog`的唯一名称                                          |
   | `hive-conf-dir`    | `No`  | `(none)`  | `String` | 包含`hive -site.xml`的目录,需要`Hadoop`文件系统支持。如果没指定`hdfs`协议，则认为是本地文件系统。如果不指定该选项，则在类路径中搜索`hive-site.xml`。 |
   | `default-database` | `No`  | `default` | `String` | `Hive Catalog`使用的默认数据库                               |
   | `hive-version`     | `No`  | `(none)`  | `String` | `HiveCatalog`能够自动检测正在使用的`Hive`版本。建议不要指定`Hive`版本，除非自动检测失败。 |
   | `hadoop-conf-dir ` | `No`  | `(none)`  | `String` | `Hadoop conf`目录的路径。只支持本地文件系统路径。设置`Hadoop  conf`的推荐方法是通过`HADOOP_CONF_DIR`环境变量。只有当环境变量不适合你时才使用该选项，例如，如果你想分别配置每个`HiveCatalog`。 |

   ~~~
   CREATE CATALOG myhive WITH (
       'type' = 'hive',
       'default-database' = 'default',
       'hive-conf-dir' = '/opt/module/hive/conf'
   );
   ~~~

6. 查看Catalog

   ~~~
   SHOW CATALOGS;
   ~~~

7. 使用指定`Catalog`

   ~~~
   USE CATALOG myhive;
   ~~~

   - 建表，退出`sql-client`重进，查看`catalog`和表还在。

## 代码中使用FlinkSQL

### 需要引入的依赖

- 我们想要在代码中使用`Table API`，必须引入相关的依赖。

  ~~~
  <dependency>
      <groupId>org.apache.flink</groupId>
      <artifactId>flink-table-api-java-bridge</artifactId>
      <version>${flink.version}</version>
  </dependency>
  ~~~

- 这里的依赖是一个`Java`的“桥接器”（`bridge`），主要就是负责`Table API`和下层`DataStream API`的连接支持，按照不同的语言分为`Java`版和`Scala`版。

- 如果我们希望在本地的集成开发环境（`IDE`）里运行`Table API`和`SQL`，还需要引入以下依赖：

  ~~~
  <dependency>
      <groupId>org.apache.flink</groupId>
      <artifactId>flink-table-planner-loader</artifactId>
      <version>${flink.version}</version>
  </dependency>
  
  <dependency>
      <groupId>org.apache.flink</groupId>
      <artifactId>flink-table-runtime</artifactId>
      <version>${flink.version}</version>
  </dependency>
  
  <dependency>
      <groupId>org.apache.flink</groupId>
      <artifactId>flink-connector-files</artifactId>
      <version>${flink.version}</version>
  </dependency>
  ~~~

### **创建表环境**

- 对于`Flink`这样的流处理框架来说，数据流和表在结构上还是有所区别的。所以使用`Table API`和`SQL`需要一个特别的运行时环境，这就是所谓的“表环境”（`TableEnvironment`）。它主要负责：
  1. 注册`Catalog`和表；
  2. 执行 `SQL` 查询；
  3. 注册用户自定义函数（`UDF`）；
  4. `DataStream` 和表之间的转换。
