# `Flink` 概述

## `Flink`定义

- `Flink`官网：https://flink.apache.org/
- `Flink`核心目标：数据流上的有状态计算(`Stateful Computations over Data Streams`)
- `Apache Flink`是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算

### 有界流和无界流

1. 无界数据流
   - 有定义流的开始，但没有定义流的结束
   - 它们会无休止的产生数据
   - 无界流的数据必须持续处理，即数据被摄取后需要立刻处理。不能等到所有数据都到达再处理，因为输入是无限的
2. 有界数据流
   - 有定义流的开始，也有定义流的结束
   - 有界流可以在摄取所有数据后再进行计算
   - 有界流所有数据可以被排序，所以并不需要有序摄取
   - 有界流处理通常被称为批处理

### 有状态数据流

- 把流处理需要的额外数据保存成一个“状态”，然后针对这条数据进行处理，并且更新状态。就是所谓的"有状态的流处理"
  - 状态在内存中：
    - 优点：速度快
    - 缺点：可靠性差
  - 状态在分布式系统中
    - 优点：可靠性高
    - 缺点：速度慢

## `Flink`特点

- 目标：低延迟、高吞吐、结果的准确性和良好的容错性
  - 高吞吐和低延迟：每秒处理数百万个事件，毫秒级延迟
  - 结果的准确性：`Flink`提供了事件时间(`event-time`)和处理时间(`processing-time`)语义。对于乱序事件流，事件时间语义仍然提供一致且准确的结果
  - 精确一次(`exactly-once`)：状态一致性保证
  - 可以连接到最常用的外部系统：如`kafka`、`Hive`、`JDBC`、`HDFS`、`Redis`等
  - 高可用

## `Flink`分层`API`

- 特点：

  - 越顶层越抽象，表达含义越简明，使用越方便
  - 越底层越具体，表达能力越丰富，使用越灵活

- 分层：

  - 最高层语言：`SQL`

    - `SQL`这一层在语法与表达能力上与`Table API`类似，但是以`SQL`查询表达式的形式表现程序。

  - 声明式领域专用语言：`TableAPI`

    - `Table API`是以表为中心的声明式编程，其中表可能会动态变化。`Table API`遵循关系模型，表有二维数据结构，类似于关系数据库中表，同时`API`提供可比较的操作，例如：`select`、`project`、`join`、`group-by`、`aggreagte`等

  - 核心`API`：`DataStream`

    - 封装了底层处理函数，提供了通用的模块，比如转换(`transformations`,包括`map`、`flatmap`等)，连接(`joins`)，聚合(`aggreagtions`)，窗口(`windows`)操作等。

  - 底层`API`：有状态流处理

    - 通过底层`API`(处理函数)，对最原始数据加工处理。底层`API`与`DataStream API`相集成，可以处理复杂的计算

## `Flink`项目配置
### `maven`配置

~~~
<properties>
        <flink.version>1.17.0</flink.version>
</properties>


    <dependencies>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-java</artifactId>
            <version>${flink.version}</version>
        </dependency>

     <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-clients</artifactId>
            <version>${flink.version}</version>
     </dependency>
</dependencies>

~~~

### `WordCount`流处理代码

~~~
public class StreamWordCount {

    public static void main(String[] args) throws Exception {
    
        // 1. 创建流式执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        
        // 2. 读取文件
        DataStreamSource<String> lineStream = env.readTextFile("input/words.txt");
        
        // 3. 转换、分组、求和，得到统计结果
        SingleOutputStreamOperator<Tuple2<String, Long>> sum = lineStream.flatMap(new FlatMapFunction<String, Tuple2<String, Long>>() {
            @Override
            public void flatMap(String line, Collector<Tuple2<String, Long>> out) throws Exception {

                String[] words = line.split(" ");

                for (String word : words) {
                    out.collect(Tuple2.of(word, 1L));
                }
            }
        }).keyBy(data -> data.f0)
           .sum(1);

        // 4. 打印
        sum.print();
        
        // 5. 执行
        env.execute();
    }
}
~~~

- tips:

  - `Flink`本身是流批统一的处理架构，批量的数据集本质上也是流，没有必要用两套不同的`API`来实现。所以从`Flink 1.12`开始，官方推荐的做法是直接使用`DataStream API`，在提交任务时通过将执行模式设为`BATCH`来进行批处理：

    ~~~
    $ bin/flink run -Dexecution.runtime-mode=BATCH BatchWordCount.jar
    ~~~

# `Flink`部署

## `Flink`集群角色

- `Flink`提交作业和执行任务，需要几个关键组件：
  - 客户端(`Client`)：代码由客户端获取并做转换，之后提交给`JobManager`
  - `JobManager`是`Flink`集群里的"管事人"，对作业进行中央调度管理；而它获取到要执行的作业后，会进一步处理转换，然后分发任务给众多的`TaskManager`
  - `TaskManager`，真正"干活的人"，数据的处理操作都是它们来做的

## 部署模式

- `Flink`为各种场景提供了不同的部署模式，主要有以下三种：会话模式(`Session Mode`)、单作业模式(`Per-Job Mode`)、应用模式(`Application Mode`)
- 主要区别：集群的生命周期以及资源的分配方式；以及应用的`main`方法到底哪里执行——客户端(`Client`)还是`JobManager`

### 会话模式(`Session Mode`)

- 需要先启动一个集群，保持一个会话，在这个会话中通过客户端提交作业。集群启动时所有资源都已经确定，所以所有提交的作业会竞争集群中的资源
- 会话模式比较适合单个规模小、执行时间短的大量作业

### 单作业模式(`Per-Job Mode`)

- 考虑为每个提交的作业启动一个集群，这就是所谓的单作业(`Per-Job`)模式
- 这些特性使得单作业模式在生产环境运行更加稳定，所以是实际应用的首选模式
- `Flink`本身无法直接这样运行，所以单作业模式一般需要借助一些资源管理框架来启动集群，比如`YARN`、`Kubernetes(K8S)`

### 应用模式(`Application Mode`)

- `JobManager`只为执行这一个应用而存在，执行结束之后`JobManager`关闭了，这就是所谓的应用模式
- 应用模式与单作业模式，都是提交作业之后才创建集群；单作业模式是通过客户端来提交的，客户端解析出的每一个作业对应一个集群，而应用模式下，是直接由`JobManager`执行应用程序的

## 运行模式

### `Standalone`运行模式

- 独立模式是独立运行的，不依赖任何外部的资源管理平台；当然独立也是有代价的；如果资源不足，或者出现故障，没有自动扩展或重分配资源的保证，必须手动处理。

#### 集群规划

| 节点服务器 | `hadoop102`                     | `hadoop103`   | `hadoop104`   |
| ---------- | ------------------------------- | ------------- | ------------- |
| 角色       | `JobManager`<br />`TaskManager` | `TaskManager` | `TaskManager` |

#### 下载并解压安装包

1. 下载安装包`flink-1.17.0-bin-scala_2.12.tgz`，将该`jar`包上传到`hadoop102`节点服务器的`/opt/software`路径上

2. 在`/opt/software`路径上解压`flink-1.17.0-bin-scala_2.12.tgz`到`/opt/module`路径上。

   ~~~
   tar -zxvf flink-1.17.0-bin-scala_2.12.tgz -C /opt/module/
   ~~~

#### 修改集群配置

1. 进入`conf`路径，修改`flink-conf.yaml`文件，指定`hadoop102`节点服务器为`JobManager `

   ~~~
   vim flink-conf.yaml
   ~~~

   - 修改如下内容：

   ~~~
   # JobManager节点地址.
   jobmanager.rpc.address: hadoop102
   jobmanager.bind-host: 0.0.0.0
   rest.address: hadoop102
   rest.bind-address: 0.0.0.0
   # TaskManager节点地址.需要配置为当前机器名
   taskmanager.bind-host: 0.0.0.0
   taskmanager.host: hadoop102
   ~~~

2. 修改`workers`文件，指定`hadoop102`、`hadoop103`和`hadoop104`为`TaskManager`

   ~~~
   vim workers
   ~~~

   - 修改如下内容

   ~~~
   hadoop102
   hadoop103
   hadoop104
   ~~~

3. 修改`masters`文件

   ~~~
   vim masters
   ~~~

   - 修改如下内容：

   ~~~
   hadoop102:8081
   ~~~

4. 另外，在`flink-conf.yaml`文件中还可以对集群中的`JobManager`和`TaskManager`组件进行优化配置，主要配置项如下

   - `jobmanager.memory.process.size`：对`JobManager`进程可使用到的全部内存进行配置，包括`JVM`元空间和其他开销，默认为`1600M`，可以根据集群规模进行适当调整。
   - `taskmanager.memory.process.size`：对`TaskManager`进程可使用到的全部内存进行配置，包括`JVM`元空间和其他开销，默认为`1728M`，可以根据集群规模进行适当调整。
   - `taskmanager.numberOfTaskSlots`：对每个`TaskManager`能够分配的`Slot`数量进行配置，默认为`1`，可根据`TaskManager`所在的机器能够提供给`Flink`的`CPU`数量决定。所谓`Slot`就是`TaskManager`中具体运行一个任务所分配的计算资源
   - `parallelism.default`：`Flink`任务执行的并行度，默认为`1`。优先级低于代码中进行的并行度配置和任务提交时使用参数指定的并行度数量。

5. 分发安装目录

   1. 配置修改完毕后，将`Flink`安装目录发给另外两个节点服务器。

      ~~~
      xsync flink-1.17.0/
      ~~~

   2. 修改`hadoop103`的 `taskmanager.host`

      ~~~
      vim flink-conf.yaml
      ~~~

      - 修改如下内容：

      ~~~
      # TaskManager节点地址.需要配置为当前机器名
      taskmanager.host: hadoop103
      ~~~

   3. 修改`hadoop104`的 `taskmanager.host`

      ~~~
      vim flink-conf.yaml
      ~~~

      - 修改如下内容：

      ~~~
      # TaskManager节点地址.需要配置为当前机器名
      taskmanager.host: hadoop104
      ~~~

6. 启动集群

   1. 在`hadoop102`节点服务器上执行`start-cluster.sh`启动`Flink`集群：

      ~~~
      bin/start-cluster.sh
      ~~~

   2. 查看进程情况：

      ~~~
      jpsall
      ~~~

7. 访问`Web UI`

   - 启动成功后，同样可以访问`http://hadoop102:8081`对`flink`集群和任务进行监控管理。

8. 向集群提交作业

   1. 环境准备

      ~~~
      nc -lk 7777
      ~~~

   2. 程序打包

      1. 在我们编写的`Flink`入门程序的`pom.xml`文件中添加打包插件的配置，具体如下：

         ~~~xml
         <build>
             <plugins>
                 <plugin>
                     <groupId>org.apache.maven.plugins</groupId>
                     <artifactId>maven-shade-plugin</artifactId>
                     <version>3.2.4</version>
                     <executions>
                         <execution>
                             <phase>package</phase>
                             <goals>
                                 <goal>shade</goal>
                             </goals>
                             <configuration>
                                 <artifactSet>
                                     <excludes>
                                         <exclude>com.google.code.findbugs:jsr305</exclude>
                                         <exclude>org.slf4j:*</exclude>
                                         <exclude>log4j:*</exclude>
                                     </excludes>
                                 </artifactSet>
                                 <filters>
                                     <filter>
                                         <!-- Do not copy the signatures in the META-INF folder.
                                         Otherwise, this might cause SecurityExceptions when using the JAR. -->
                                         <artifact>*:*</artifact>
                                         <excludes>
                                             <exclude>META-INF/*.SF</exclude>
                                             <exclude>META-INF/*.DSA</exclude>
                                             <exclude>META-INF/*.RSA</exclude>
                                         </excludes>
                                     </filter>
                                 </filters>
                                 <transformers combine.children="append">
                                     <transformer
                                             implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer">
                                     </transformer>
                                 </transformers>
                             </configuration>
                         </execution>
                     </executions>
                 </plugin>
             </plugins>
         </build>
         
         ~~~

      2. 打包完成后，在`target`目录下即可找到所需`JAR`包，`JAR`包会有两个，`FlinkTutorial-1.0-SNAPSHOT.jar`和`FlinkTutorial-1.0-SNAPSHOT-jar-with-dependencies.jar`，因为集群中已经具备任务运行所需的所有依赖，所以建议使用`FlinkTutorial-1.0-SNAPSHOT.jar`。

   3. 在`Web UI`上提交作业

         1. 任务打包完成后，打开`Flink`的`WEB UI`页面，在右侧导航栏点击`“Submit New Job”`，然后点击按钮`“+ Add New”`，选择要上传运行的`JAR`包
         2. 点击该`JAR`包，出现任务配置页面，进行相应配置。
            - 主要配置程序入口主类的全类名，任务运行的并行度，任务运行所需的配置参数和保存点路径等。配置完成后，即可点击按钮`“Submit”`，将任务提交到集群运行。

      4. 点击该任务，可以查看任务运行的具体情况，也可以通过点击“`Cancel Job`”结束任务运行。

   4. 命令行提交作业
   
         - 除了通过`WEB UI`界面提交任务之外，也可以直接通过命令行来提交任务。可以先把`jar`包直接上传到目录`flink-1.17.0`
   
         1. 将`flink`程序运行jar包上传到`/opt/module/flink-1.17.0`路径
   
         2. 进入到`flink`的安装路径下，在命令行使用`flink run`命令提交作业。
   
            ~~~
             bin/flink run -m hadoop102:8081 -c com.atguigu.wc.SocketStreamWordCount ./FlinkTutorial-1.0-SNAPSHOT.jar
            ~~~
   
            - 这里的参数 `-m`指定了提交到的`JobManager`，`-c`指定了入口类。
   
         3. 在浏览器中打开`Web UI`，`http://hadoop102:8081`查看应用执行情况。用`netcat`输入数据，可以在`TaskManager`的标准输出（`Stdout`）看到对应的统计结果。

##### 会话模式部署

- 提前启动集群，并通过`Web`面客户端提交任务（可以多个任务，但是集群资源固定）。

##### 单作业模式部署

- `Flink`的`Standalone`集群并不支持单作业模式部署。因为单作业模式需要借助一些资源管理平台。

#####  应用模式部署

- 应用模式下不会提前创建集群，所以不能调用`start-cluster.sh`脚本。可以使用同样在`bin`目录下的`standalone-job.sh`来创建一个`JobManager`。

- 具体步骤

  1. 环境准备。在`hadoop102`中执行以下命令启动`netcat`。

     ~~~
      nc -lk 7777
     ~~~

  2. 进入到`Flink`的安装路径下，将应用程序的`jar`包放到`lib/`目录下。

     ~~~
      mv FlinkTutorial-1.0-SNAPSHOT.jar lib/
     ~~~

  3. 执行以下命令，启动`JobManager`

     ~~~
      bin/standalone-job.sh start --job-classname com.atguigu.wc.SocketStreamWordCount
     ~~~

  4. 同样是使用bin目录下的脚本，启动TaskManager

     ~~~
     bin/taskmanager.sh start
     ~~~

  5. 如果希望停掉集群，同样可以使用脚本，命令如下。

     ~~~
      bin/taskmanager.sh stop
      bin/standalone-job.sh stop
     ~~~

### `YARN`运行模式

- `YARN`上部署的过程是：客户端把`Flink`应用提交给`Yarn`的`ResourceManager`，`Yarn`的`ResourceManager`会向`Yarn`的`NodeManager`申请容器。在这些容器上，`Flink`会部署`JobManager`和`TaskManager`的实例，从而启动集群。`Flink`会根据运行在`JobManger`上的作业所需要的`Slot`数量动态分配`TaskManager`资源。

#### 相关准备和配置

1. 配置环境变量，增加环境变量配置如下

   ~~~
   sudo vim /etc/profile.d/my_env.sh
   ~~~

   ~~~
   HADOOP_HOME=/opt/module/hadoop-3.3.4
   export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
   export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
   export HADOOP_CLASSPATH=`hadoop classpath`
   ~~~

2. 启动`Hadoop`集群，包括`HDFS`和`YARN`

   ~~~
    start-dfs.sh
    start-yarn.sh
   ~~~

##### 会话模式部署

- `YARN`的会话模式与独立集群略有不同，需要首先申请一个`YARN`会话（`YARN Session`）来启动`Flink`集群。具体步骤如下：

  1. 启动集群

     1. 启动`Hadoop`集群（`HDFS`、`YARN`）。

     2. 执行脚本命令向`YARN`集群申请资源，开启一个`YARN`会话，启动`Flink`集群。

        ~~~
        bin/yarn-session.sh -nm test
        ~~~

        - 可用参数解读：
          - `-d`：分离模式，如果你不想让`Flink YARN`客户端一直前台运行，可以使用这个参数，即使关掉当前对话窗口，`YARN session`也可以后台运行。
          - `-jm`（`--jobManagerMemory`）：配置`JobManager`所需内存，默认单位`MB`。
          - `-nm`（`--name`）：配置在`YARN UI`界面上显示的任务名。
          -  `-qu`（`--queue`）：指定`YARN`队列名。
          - `-tm`（`--taskManager`）：配置每个`TaskManager`所使用内存。
          - 注意：`Flink1.11.0`版本不再使用`-n`参数和`-s`参数分别指定`TaskManager`数量和`slot`数量，`YARN`会按照需求动态分配`TaskManager`和`slot`。所以从这个意义上讲，`YARN`的会话模式也不会把集群资源固定，同样是动态分配的。
          - `YARN Session`启动之后会给出一个`Web UI`地址以及一个`YARN application ID`，如下所示，用户可以通过`Web UI`或者命令行两种方式提交作业。

  2. 提交作业

     1. 通过`Web UI`提交作业

        - 这种方式比较简单，与上文所述`Standalone`部署模式基本相同。

     2. 通过命令行提交作业

        1.  将`FlinkTutorial-1.0-SNAPSHOT.jar`任务上传至集群

        2. 执行以下命令将该任务提交到已经开启的`Yarn-Session`中运行

           ~~~
           bin/flink run -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar
           ~~~

           - 客户端可以自行确定`JobManager`的地址，也可以通过`-m`或者`-jobmanager`参数指定`JobManager`的地址，`JobManager`的地址在YARN Session的启动页面中可以找到。

##### 单作业模式部署

- 在`YARN`环境中，由于有了外部平台做资源调度，所以我们也可以直接向`YARN`提交一个单独的作业，从而启动一个`Flink`集群。

  1. 执行命令提交作业

     ~~~
      bin/flink run -d -t yarn-per-job -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar
     ~~~

     - 注意：如果启动过程中报如下异常

     ~~~
     Exception in thread “Thread-5” java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration ‘classloader.check-leaked-classloader’.
     at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders
     ~~~

     - 解决办法：在`flink`的`/opt/module/flink-1.17.0/conf/flink-conf.yaml`配置文件中设置

     ~~~
     vim flink-conf.yaml
     
     classloader.check-leaked-classloader: false
     ~~~

  2. 在`YARN`的`ResourceManager`界面查看执行情况。

  3. 可以使用命令行查看或取消作业，命令如下

     ~~~
     bin/flink list -t yarn-per-job -Dyarn.application.id=application_XXXX_YY
     
     bin/flink cancel -t yarn-per-job -Dyarn.application.id=application_XXXX_YY <jobId>
     ~~~

     - 这里的`application_XXXX_YY`是当前应用的`ID`，`<jobId>`是作业的`ID`。注意如果取消作业，整个`Flink`集群也会停掉。

##### 应用模式部署

- 应用模式同样非常简单，与单作业模式类似，直接执行`flink run-application`命令即可。

###### 命令行提交

1. 执行命令提交作业

   ~~~
   bin/flink run-application -t yarn-application -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar 
   ~~~

2. 在命令行中查看或取消作业

   ~~~
    bin/flink list -t yarn-application -Dyarn.application.id=application_XXXX_YY
    
    bin/flink cancel -t yarn-application -Dyarn.application.id=application_XXXX_YY <jobId>
   ~~~

###### 上传`HDFS`提交

- 可以通过`yarn.provided.lib.dirs`配置选项指定位置，将`flink`的依赖上传到远程。

1. 上传`flink`的`lib`和`plugins`到`HDFS`上

   ~~~
    hadoop fs -mkdir /flink-dist
    hadoop fs -put lib/ /flink-dist
    hadoop fs -put plugins/ /flink-dist
   ~~~

2. 上传自己的`jar`包到`HDFS`

   ~~~
   hadoop fs -mkdir /flink-jars
   hadoop fs -put FlinkTutorial-1.0-SNAPSHOT.jar /flink-jars
   ~~~

3. 提交作业

   ~~~
   bin/flink run-application -t yarn-application	-Dyarn.provided.lib.dirs="hdfs://hadoop102:8020/flink-dist"	-c com.atguigu.wc.SocketStreamWordCount  hdfs://hadoop102:8020/flink-jars/FlinkTutorial-1.0-SNAPSHOT.jar
   ~~~

   - 这种方式下，`flink`本身的依赖和用户`jar`可以预先上传到`HDFS`，而不需要单独发送到集群，这就使得作业提交更加轻量了。

### 历史服务器

- 运行 `Flink job` 的集群一旦停止，只能去 `yarn` 或本地磁盘上查看日志，不再可以查看作业挂掉之前的运行的 `Web UI`。`Flink`提供了历史服务器，用来在相应的 `Flink` 集群关闭后查询已完成作业的统计信息。通过 `History Server` 我们才能查询这些已完成作业的统计信息，无论是正常退出还是异常退出。
- 此外，它对外提供了 `REST API`，它接受 `HTTP` 请求并使用 `JSON` 数据进行响应。`Flink` 任务停止后，`JobManager` 会将已经完成任务的统计信息进行存档，`History Server` 进程则在任务停止后可以对任务统计信息进行查询。

1. 创建存储目录

   ~~~
   hadoop fs -mkdir -p /logs/flink-job
   ~~~

2. 在` **flink-config.yaml`中添加如下配置

   ~~~
   jobmanager.archive.fs.dir: hdfs://hadoop102:8020/logs/flink-job
   historyserver.web.address: hadoop102
   historyserver.web.port: 8082
   historyserver.archive.fs.dir: hdfs://hadoop102:8020/logs/flink-job
   historyserver.archive.fs.refresh-interval: 5000
   ~~~

3. 启动历史服务器

   ~~~
   bin/historyserver.sh start
   ~~~

4. 停止历史服务器

   ~~~
   bin/historyserver.sh stop
   ~~~

5. 在浏览器地址栏输入：`http://hadoop102:8082`查看已经停止的`job` 的统计信息













