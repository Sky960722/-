# `Flink DataStream API`

- `DataStream API`是`Flink`的核心层`API`。一个`Flink`程序，其实就是对`DataStream`的各种转换。

## 执行环境（`Execution Environment`）

- `Flink`程序可以在各种上下文环境中运行：我们可以在本地`JVM`中执行程序，也可以提交到远程集群上运行。
- 不同的环境，代码的提交运行的过程会有所不同。这就要求我们在提交作业执行计算时，首先必须获取当前`Flink`的运行环境，从而建立起与`Flink`框架之间的联系。

### 创建执行环境

- 我们要获取的执行环境，是`StreamExecutionEnvironment`类的对象，这是所有`Flink`程序的基础。

#### 1. getExecutionEnvironment

- 最简单的方式，就是直接调用`getExecutionEnvironment`方法。它会根据当前运行的上下文直接得到正确的结果：如果程序是独立运行的，就返回一个本地执行环境；如果是创建了jar包，然后从命令行调用它并提交到集群执行，那么就返回集群的执行环境。

  ~~~
  StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
  ~~~

#### 2. createLocalEnvironment

- 这个方法返回一个本地执行环境。可以在调用时传入一个参数，指定默认的并行度；如果不传入，则默认并行度就是本地的CPU核心数。

  ~~~
  StreamExecutionEnvironment localEnv = StreamExecutionEnvironment.createLocalEnvironment();
  ~~~

#### 3.createRemoteEnvironment

- 这个方法返回集群执行环境。需要在调用时指定`JobManager`的主机名和端口号，并指定要在集群中运行的Jar包。

  ~~~
  StreamExecutionEnvironment remoteEnv = StreamExecutionEnvironment
    		.createRemoteEnvironment(
      		"host",                   // JobManager主机名
      		1234,                     // JobManager进程端口号
     			"path/to/jarFile.jar"  // 提交给JobManager的JAR包
  		); 
  ~~~

### 执行模式（`Execution Mode`）

- 从`Flink 1.12`开始，官方推荐的做法是直接使用`DataStream API`，在提交任务时通过将执行模式设为`BATCH`来进行批处理。不建议使用`DataSet API`。

  ~~~
  // 流处理环境
  StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
  ~~~

- `DataStream API`执行模式包括：流执行模式、批执行模式和自动模式。

  - 流执行模式（`Streaming`）
    - 这是`DataStream API`最经典的模式，一般用于需要持续实时处理的无界数据流。默认情况下，程序使用的就是`Streaming`执行模式。
  - 批执行模式（`Batch`）
    - 专门用于批处理的执行模式。
  - 自动模式（`AutoMatic`）
    - 在这种模式下，将由程序根据输入数据源是否有界，来自动选择执行模式。

- 批执行模式的使用。主要有两种方式：

  1. 通过命令行配置

     ~~~
     bin/flink run -Dexecution.runtime-mode=BATCH ...
     ~~~

  2. 通过代码配置

     ~~~
     StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
     env.setRuntimeMode(RuntimeExecutionMode.BATCH);
     ~~~

     - 实际应用中一般不会在代码中配置，而是使用命令行，这样更加灵活。

- 触发程序执行

  - 需要注意的是，写完输出（`sink`）操作并不代表程序已经结束。因为当`main()`方法被调用时，其实只是定义了作业的每个执行操作，然后添加到数据流图中；这时并没有真正处理数据——因为数据可能还没来。

  - `Flink`是由事件驱动的，只有等到数据到来，才会触发真正的计算，这也被称为“延迟执行”或“懒执行”。

  - 所以我们需要显式地调用执行环境的`execute()`方法，来触发程序执行。`execute()`方法将一直等待作业完成，然后返回一个执行结果（`JobExecutionResult`）。

    ~~~
    env.execute();
    ~~~

## 源算子（`Source`)

- `Flink`可以从各种来源获取数据，然后构建`DataStream`进行转换处理。一般将数据的输入来源称为数据源（`data source`），而读取数据的算子就是源算子（`source operator`）。

- 从`Flink1.12`开始，主要使用流批统一的新`Source`架构：

  ~~~
  DataStreamSource<String> stream = env.fromSource(…)
  ~~~

### 准备工作

| 字段名 | 数据类型 | 说明             |
| ------ | -------- | ---------------- |
| id     | String   | 水位传感器类型   |
| ts     | Long     | 传感器记录时间戳 |
| vc     | Integer  | 水位记录         |

~~~java
public class WaterSensor {
    public String id;
    public Long ts;
    public Integer vc;

    public WaterSensor() {
    }

    public WaterSensor(String id, Long ts, Integer vc) {
        this.id = id;
        this.ts = ts;
        this.vc = vc;
    }

    public String getId() {
        return id;
    }

    public void setId(String id) {
        this.id = id;
    }

    public Long getTs() {
        return ts;
    }

    public void setTs(Long ts) {
        this.ts = ts;
    }

    public Integer getVc() {
        return vc;
    }

    public void setVc(Integer vc) {
        this.vc = vc;
    }

    @Override
    public String toString() {
        return "WaterSensor{" +
                "id='" + id + '\'' +
                ", ts=" + ts +
                ", vc=" + vc +
                '}';
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) {
            return true;
        }
        if (o == null || getClass() != o.getClass()) {
            return false;
        }
        WaterSensor that = (WaterSensor) o;
        return Objects.equals(id, that.id) &&
                Objects.equals(ts, that.ts) &&
                Objects.equals(vc, that.vc);
    }

    @Override
    public int hashCode() {

        return Objects.hash(id, ts, vc);
    }
}

~~~

- 这里需要注意，我们定义的`WaterSensor`，有这样几个特点：
  - 类是公有（`public`）的
  - 有一个无参的构造方法
  - 所有属性都是公有（`public`）的
  - 所有属性的类型都是可以序列化的
  - `Flink`会把这样的类作为一种特殊的`POJO`（`Plain Ordinary Java Object`简单的`Java`对象，实际就是普通`JavaBeans`）数据类型来对待，方便数据的解析和序列化。另外我们在类中还重写了`toString`方法，主要是为了测试输出显示更清晰。

### 从集合中读取数据

~~~java
public static void main(String[] args) throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    List<Integer> data = Arrays.asList(1, 22, 3);
    DataStreamSource<Integer> ds = env.fromCollection(data);
    stream.print();
    env.execute();
}
~~~

### 从文件读取数据

- 读取文件，需要添加文件连接器依赖:

  ~~~
   <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-connector-files</artifactId>
              <version>${flink.version}</version>
  </dependency>
  ~~~

  ~~~java
  public static void main(String[] args) throws Exception {
          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
          FileSource<String> fileSource = FileSource.forRecordStreamFormat(new TextLineInputFormat(), new Path("input/word.txt")).build();
          env.fromSource(fileSource,WatermarkStrategy.noWatermarks(),"file").print();
          env.execute();
  }
  
  ~~~

  - 说明：
    - 参数可以是目录，也可以是文件；还可以从`HDFS`目录下读取，使用路径`hdfs://...；`
    - 路径可以是相对路径，也可以是绝对路径；
    - 相对路径是从系统属性`user.dir`获取路径：`idea`下是`project`的根目录，`standalone`模式下是集群节点根目录；

### 从`Socket`读取数据

- 不论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。

  ~~~
  DataStream<String> stream = env.socketTextStream("localhost", 7777);
  ~~~

###  从`Kafka`读取数据

- `Flink`官方提供了连接工具`flink-connector-kafka`，直接帮我们实现了一个消费者`FlinkKafkaConsumer`，它就是用来读取`Kafka`数据的`SourceFunction`。

- 所以想要以`Kafka`作为数据源获取数据，我们只需要引入`Kafka`连接器的依赖。`Flink`官方提供的是一个通用的`Kafka`连接器，它会自动跟踪最新版本的`Kafka`客户端。

  ~~~xml
  <dependency>
      <groupId>org.apache.flink</groupId>
      <artifactId>flink-connector-kafka</artifactId>
      <version>${flink.version}</version>
  </dependency>
  ~~~

  ~~~java
  public class SourceKafka {
      public static void main(String[] args) throws Exception {
  
          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
  
          KafkaSource<String> kafkaSource = KafkaSource.<String>builder()
              .setBootstrapServers("hadoop102:9092")
              .setTopics("topic_1")
              .setGroupId("atguigu")
              .setStartingOffsets(OffsetsInitializer.latest())
              .setValueOnlyDeserializer(new SimpleStringSchema()) 
              .build();
  
          DataStreamSource<String> stream = env.fromSource(kafkaSource, WatermarkStrategy.noWatermarks(), "kafka-source");
  
          stream.print("Kafka");
  
          env.execute();
      }
  }
  ~~~

### 从数据生成器读取数据

- `Flink`从`1.11`开始提供了一个内置的`DataGen` 连接器，主要是用于生成一些随机数，用于在没有数据源的时候，进行流任务的测试以及性能测试等。

  ~~~xml
          <dependency>
              <groupId>org.apache.flink</groupId>
              <artifactId>flink-connector-datagen</artifactId>
              <version>${flink.version}</version>
          </dependency>
  ~~~

  ~~~java
  public class DataGeneratorDemo {
      public static void main(String[] args) throws Exception {
  
          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
          env.setParallelism(1);
  
          DataGeneratorSource<String> dataGeneratorSource =
                  new DataGeneratorSource<>(
                          new GeneratorFunction<Long, String>() {
                              @Override
                              public String map(Long value) throws Exception {
                                  return "Number:"+value;
                              }
                          },
                          Long.MAX_VALUE,
                          RateLimiterStrategy.perSecond(10),
                          Types.STRING
                  );
          env
             .fromSource(dataGeneratorSource, WatermarkStrategy.noWatermarks(), "datagenerator")
             .print();
          env.execute();
      }
  }
  ~~~

  ### `Flink`支持的数据类型

  1. `Flink`的类型系统

     - `Flink`使用“类型信息”（`TypeInformation`）来统一表示数据类型。`TypeInformation`类是`Flink`中所有类型描述符的基类。它涵盖了类型的一些基本属性，并为每个数据类型生成特定的序列化器、反序列化器和比较器。

  2. `Flink`支持的数据类型

     - 对于常见的`Java`和`Scala`数据类型，`Flink`都是支持的。`Flink`在内部，`Flink`对支持不同的类型进行了划分，这些类型可以在`Types`工具类中找到：

     1. 基本类型

        - 所有`Java`基本类型及其包装类，再加上`Void`、`String`、`Date`、`BigDecimal`和`BigInteger`。

     2. 数组类型

        - 包括基本类型数组（`PRIMITIVE_ARRAY`）和对象数组（`OBJECT_ARRAY`）。

     3. 复合数据类型

        - `Java`元组类型（`TUPLE`）：这是`Flink`内置的元组类型，是`Java API`的一部分。最多25个字段，也就是从`Tuple0~Tuple25`，不支持空字段。
        - `Scala` 样例类及`Scala`元组：不支持空字段。
        - 行类型（`ROW`）：可以认为是具有任意个字段的元组，并支持空字段。
        - `POJO`：`Flink`自定义的类似于`Java bean`模式的类。

     4. 辅助类型

        - `Option`、`Either`、`List`、`Map`等。

     5. 泛型类型（`GENERIC`）

        - `Flink`支持所有的`Java`类和`Scala`类。不过如果没有按照上面`POJO`类型的要求来定义，就会被`Flink`当作泛型类来处理。`Flink`会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由`Flink`本身序列化的，而是由`Kryo`序列化的。

     6. 类型提示（`Type Hints`）

        - `Flink`还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于`Java`中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的。

        - 为了解决这类问题，`Java API`提供了专门的“类型提示”（`type hints`）。

          ~~~
          .map(word -> Tuple2.of(word, 1L))
          .returns(Types.TUPLE(Types.STRING, Types.LONG));
          ~~~

        - `Flink`还专门提供了`TypeHint`类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。

          ~~~
          returns(new TypeHint<Tuple2<Integer, SomeType>>(){})
          ~~~

## 转换算子（`Transformation`)

- 数据源读入数据之后，我们就可以使用各种转换算子，将一个或多个`DataStream`转换为新的`DataStream`。

### 基本转换算子（`map/ filter/ flatMap`)

#### 映射（`map`）

- `map`是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。

  ~~~java
  public class TransMap {
      public static void main(String[] args) throws Exception {
  
          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
  
          DataStreamSource<WaterSensor> stream = env.fromElements(
                  new WaterSensor("sensor_1", 1, 1),
                  new WaterSensor("sensor_2", 2, 2)
          );
  
          // 方式一：传入匿名类，实现MapFunction
          stream.map(new MapFunction<WaterSensor, String>() {
              @Override
              public String map(WaterSensor e) throws Exception {
                  return e.id;
              }
          }).print();
  
          // 方式二：传入MapFunction的实现类
          // stream.map(new UserMap()).print();
  
          env.execute();
      }
  
      public static class UserMap implements MapFunction<WaterSensor, String> {
          @Override
          public String map(WaterSensor e) throws Exception {
              return e.id;
          }
      }
  }
  ~~~

####  过滤（`filter`）

- `filter`转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为`true`则元素正常输出，若为`false`则元素被过滤掉。

  ~~~
  public class TransFilter {
      public static void main(String[] args) throws Exception {
  
          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
  
          DataStreamSource<WaterSensor> stream = env.fromElements(
                  
  new WaterSensor("sensor_1", 1, 1),
  new WaterSensor("sensor_1", 2, 2),
  new WaterSensor("sensor_2", 2, 2),
  new WaterSensor("sensor_3", 3, 3)
          );
  
          // 方式一：传入匿名类实现FilterFunction
          stream.filter(new FilterFunction<WaterSensor>() {
              @Override
              public boolean filter(WaterSensor e) throws Exception {
                  return e.id.equals("sensor_1");
              }
          }).print();
  
          // 方式二：传入FilterFunction实现类
          // stream.filter(new UserFilter()).print();
          
          env.execute();
      }
      public static class UserFilter implements FilterFunction<WaterSensor> {
          @Override
          public boolean filter(WaterSensor e) throws Exception {
              return e.id.equals("sensor_1");
          }
      }
  }
  ~~~

#### 扁平映射（`flatMap`）

- flatMap操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。

  ~~~
  public class TransFlatmap {
      public static void main(String[] args) throws Exception {
  
          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
  
          DataStreamSource<WaterSensor> stream = env.fromElements(
                  
  new WaterSensor("sensor_1", 1, 1),
  new WaterSensor("sensor_1", 2, 2),
  new WaterSensor("sensor_2", 2, 2),
  new WaterSensor("sensor_3", 3, 3)
  
          );
  
          stream.flatMap(new MyFlatMap()).print();
  
          env.execute();
      }
  
      public static class MyFlatMap implements FlatMapFunction<WaterSensor, String> {
  
          @Override
          public void flatMap(WaterSensor value, Collector<String> out) throws Exception {
  
              if (value.id.equals("sensor_1")) {
                  out.collect(String.valueOf(value.vc));
              } else if (value.id.equals("sensor_2")) {
                  out.collect(String.valueOf(value.ts));
                  out.collect(String.valueOf(value.vc));
              }
          }
      }
  } 
  
  ~~~

### 聚合算子（`Aggregation`)

- 计算的结果不仅依赖当前数据，还跟之前的数据有关，相当于要把所有数据聚在一起进行汇总合并——这就是所谓的“聚合”（`Aggregation`），类似于`MapReduce`中的`reduce`操作。

#### 按键分区（keyBy）

- 对于`Flink`而言，`DataStream`是没有直接进行聚合的`API`的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。所以在`Flink`中，要做聚合，需要先进行分区；这个操作就是通过`keyBy`来完成的。

- `keyBy`是聚合前必须要用到的一个算子。`keyBy`通过指定键（`key`），可以将一条流从逻辑上划分成不同的分区（`partitions`）。这里所说的分区，其实就是并行处理的子任务。

- 基于不同的`key`，流中的数据将被分配到不同的分区中去；这样一来，所有具有相同的`key`的数据，都将被发往同一个分区。

- 在内部，是通过计算`key`的哈希值（`hash code`），对分区数进行取模运算来实现的。所以这里`key`如果是`POJO`的话，必须要重写`hashCode()`方法。

- `keyBy()`方法需要传入一个参数，这个参数指定了一个或一组`key`。有很多不同的方法来指定`key`：比如对于`Tuple`数据类型，可以指定字段的位置或者多个位置的组合；对于`POJO`类型，可以指定字段的名称（`String`）；另外，还可以传入`Lambda`表达式或者实现一个键选择器（`KeySelector`），用于说明从数据中提取`key`的逻辑。

  ~~~java
  public class TransKeyBy {
      public static void main(String[] args) throws Exception {
  
          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
  
          DataStreamSource<WaterSensor> stream = env.fromElements(
              new WaterSensor("sensor_1", 1, 1),
              new WaterSensor("sensor_1", 2, 2),
              new WaterSensor("sensor_2", 2, 2),
              new WaterSensor("sensor_3", 3, 3)
          );
  
          // 方式一：使用Lambda表达式
          KeyedStream<WaterSensor, String> keyedStream = stream.keyBy(e -> e.id);
  
          // 方式二：使用匿名类实现KeySelector
          KeyedStream<WaterSensor, String> keyedStream1 = stream.keyBy(new KeySelector<WaterSensor, String>() {
              @Override
              public String getKey(WaterSensor e) throws Exception {
                  return e.id;
              }
          });
          env.execute();
      }
  }
  ~~~

  - 需要注意的是，`keyBy`得到的结果将不再是`DataStream`，而是会将`DataStream`转换为`KeyedStream`。`KeyedStream`可以认为是“分区流”或者“键控流”，它是对`DataStream`按照`key`的一个逻辑分区，所以泛型有两个类型：除去当前流中的元素类型外，还需要指定key的类型。
  - `KeyedStream`也继承自`DataStream`，所以基于它的操作也都归属于`DataStream API`。但它跟之前的转换操作得到的`SingleOutputStreamOperato`r不同，只是一个流的分区操作，并不是一个转换算子。`KeyedStream`是一个非常重要的数据结构，只有基于它才可以做后续的聚合操作（比如sum，reduce）。

#### 简单聚合（`sum/min/max/minBy/maxBy`）

- 有了按键分区的数据流`KeyedStream`，我们就可以基于它进行聚合操作了。`Flink`为我们内置实现了一些最基本、最简单的聚合`API`，主要有以下几种：

  - `sum()`：在输入流上，对指定的字段做叠加求和的操作。
  - `min()`：在输入流上，对指定的字段求最小值。
  - `max()`：在输入流上，对指定的字段求最大值。
  - `minBy()`：与`min()`类似，在输入流上针对指定字段求最小值。不同的是，`min()`只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而`minBy()`则会返回包含字段最小值的整条数据。
  - `maxBy()`：与`max()`类似，在输入流上针对指定字段求最大值。两者区别与`min()/minBy()`完全一致。
    - 简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。

  ~~~java
  public class TransAggregation {
  
      public static void main(String[] args) throws Exception {
  
          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
  
          DataStreamSource<WaterSensor> stream = env.fromElements(
          new WaterSensor("sensor_1", 1, 1),
          new WaterSensor("sensor_1", 2, 2),
          new WaterSensor("sensor_2", 2, 2),
          new WaterSensor("sensor_3", 3, 3)
          );
  
          stream.keyBy(e -> e.id).max("vc");    // 指定字段名称
  
          env.execute();
      }
  }
  
  ~~~

  - 一个聚合算子，会为每一个`key`保存一个聚合的值，在`Flink`中我们把它叫作“状态”（`state`）。所以每当有一个新的数据输入，算子就会更新保存的聚合结果，并发送一个带有更新后聚合值的事件到下游算子。对于无界流来说，这些状态是永远不会被清除的，所以我们使用聚合算子，应该只用在含有有限个`key`的数据流上。

#### 归约聚合（`reduce`）

- `reduce`可以对已有的数据进行归约处理，把每一个新输入的数据和当前已经归约出来的值，再做一个聚合计算。

- `reduce`操作也会将`KeyedStream`转换为`DataStream`。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的

- 调用`KeyedStream`的`reduce`方法时，需要传入一个参数，实现`ReduceFunction`接口。接口在源码中的定义如下：

  ~~~
  public interface ReduceFunction<T> extends Function, Serializable {
      T reduce(T value1, T value2) throws Exception;
  }
  ~~~

- `ReduceFunction`接口里需要实现`reduce()`方法，这个方法接收两个输入事件，经过转换处理之后输出一个相同类型的事件。在流处理的底层实现过程中，实际上是将中间“合并的结果”作为任务的一个状态保存起来的；之后每来一个新的数据，就和之前的聚合状态进一步做归约。

- 我们可以单独定义一个函数类实现`ReduceFunction`接口，也可以直接传入一个匿名类。当然，同样也可以通过传入`Lambda`表达式实现类似的功能。

- 为了方便后续使用，定义一个`WaterSensorMapFunction`：

  ~~~
  public class WaterSensorMapFunction implements MapFunction<String,WaterSensor> {
      @Override
      public WaterSensor map(String value) throws Exception {
          String[] datas = value.split(",");
          return new WaterSensor(datas[0],Long.valueOf(datas[1]) ,Integer.valueOf(datas[2]) );
      }
  }
  ~~~

- 案例：使用`reduce`实现`max`和`maxBy`的功能。

  ~~~java
  StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
  
  env
     .socketTextStream("hadoop102", 7777)
     .map(new WaterSensorMapFunction())
     .keyBy(WaterSensor::getId)
     .reduce(new ReduceFunction<WaterSensor>()
     {
         @Override
         public WaterSensor reduce(WaterSensor value1, WaterSensor value2) throws Exception {
             System.out.println("Demo7_Reduce.reduce");
  
             int maxVc = Math.max(value1.getVc(), value2.getVc());
             //实现max(vc)的效果  取最大值，其他字段以当前组的第一个为主
             //value1.setVc(maxVc);
             //实现maxBy(vc)的效果  取当前最大值的所有字段
             if (value1.getVc() > value2.getVc()){
                 value1.setVc(maxVc);
                 return value1;
             }else {
                 value2.setVc(maxVc);
                 return value2;
             }
         }
     })
     .print();
  env.execute();
  
  ~~~

### 用户自定义函数（`UDF`）

- 用户自定义函数（`user-defined function`，`UDF`），即用户可以根据自身需求，重新实现算子的逻辑。
- 用户自定义函数分为：函数类、匿名函数、富函数类。

####  函数类（`Function Classes`）

- `Flink`暴露了所有`UDF`函数的接口，具体实现方式为接口或者抽象类，例如`MapFunction`、`FilterFunction`、`ReduceFunction`等。所以用户可以自定义一个函数类，实现对应的接口。

- 方式一：实现`FilterFunction`接口

  ~~~java
  public class TransFunctionUDF {
  
      public static void main(String[] args) throws Exception {
  
          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
  
          DataStreamSource<WaterSensor> stream = env.fromElements(
                  
  		new WaterSensor("sensor_1", 1, 1),
  		new WaterSensor("sensor_1", 2, 2),
  		new WaterSensor("sensor_2", 2, 2),
  		new WaterSensor("sensor_3", 3, 3)
          );
         
          DataStream<String> filter = stream.filter(new UserFilter());
        
          filter.print();
          env.execute();
      }
  
      public static class UserFilter implements FilterFunction<WaterSensor> {
          @Override
          public boolean filter(WaterSensor e) throws Exception {
              return e.id.equals("sensor_1");
          }
      }
  }
  ~~~

- 方式二：通过匿名类来实现`FilterFunction`接口：

  ~~~
  DataStream<String> stream = stream.filter(new FilterFunction< WaterSensor>() {
      @Override
      public boolean filter(WaterSensor e) throws Exception {
          return e.id.equals("sensor_1");
      }
  });
  ~~~

  ~~~java
  DataStreamSource<WaterSensor> stream = env.fromElements(        
  new WaterSensor("sensor_1", 1, 1),
  new WaterSensor("sensor_1", 2, 2),
  new WaterSensor("sensor_2", 2, 2),
  new WaterSensor("sensor_3", 3, 3)
  );
  
  DataStream<String> stream = stream.filter(new FilterFunctionImpl("sensor_1"));
  
  public static class FilterFunctionImpl implements FilterFunction<WaterSensor> {
      private String id;
  
      FilterFunctionImpl(String id) { this.id=id; }
  
      @Override
      public boolean filter(WaterSensor value) throws Exception {
          return thid.id.equals(value.id);
      }
  }
  ~~~

- 方式三：采用匿名函数（`Lambda`）
  
  ~~~java
  public class TransFunctionUDF {
  
      public static void main(String[] args) throws Exception {
  
          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
  
          DataStreamSource<WaterSensor> stream = env.fromElements(
                  
  new WaterSensor("sensor_1", 1, 1),
  new WaterSensor("sensor_1", 2, 2),
  new WaterSensor("sensor_2", 2, 2),
  new WaterSensor("sensor_3", 3, 3)
          );    
  
          //map函数使用Lambda表达式，不需要进行类型声明
          SingleOutputStreamOperator<String> filter = stream.filter(sensor -> "sensor_1".equals(sensor.id));
  
          filter.print();
  
          env.execute();
      }
  }
  ~~~

#### 富函数类（`Rich Function Classes`）

- 富函数类”也是`DataStream API`提供的一个函数类的接口，所有的`Flink`函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：`RichMapFunction`、`RichFilterFunction`、`RichReduceFunction`等。

- 与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。

- `Rich Function`有生命周期的概念。典型的生命周期方法有：

  - `open()`方法，是`Rich Function`的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如`map()`或者`filter()`方法被调用之前，`open()`会首先被调用。
  - `close()`方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作。

  ~~~java
  public class RichFunctionExample {
  
      public static void main(String[] args) throws Exception {
  
          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
          env.setParallelism(2);
  
          env
                  .fromElements(1,2,3,4)
                  .map(new RichMapFunction<Integer, Integer>() {
                      @Override
                      public void open(Configuration parameters) throws Exception {
                          super.open(parameters);
                          System.out.println("索引是：" + getRuntimeContext().getIndexOfThisSubtask() + " 的任务的生命周期开始");
                      }
  
                      @Override
                      public Integer map(Integer integer) throws Exception {
                          return integer + 1;
                      }
  
                      @Override
                      public void close() throws Exception {
                          super.close();
                          System.out.println("索引是：" + getRuntimeContext().getIndexOfThisSubtask() + " 的任务的生命周期结束");
                      }
                  })
                  .print();
  
          env.execute();
      }
  }
  
  ~~~

###  物理分区算子（`Physical Partitioning`）

- 常见的物理分区策略有：随机分配（`Random`）、轮询分配（`Round-Robin`）、重缩放（`Rescale`）和广播（`Broadcast`）。

#### 随机分区（`shuffle`）

- 最简单的重分区方式就是直接“洗牌”。通过调用`DataStream`的`.shuffle()`方法，将数据随机地分配到下游算子的并行任务中去。

- 随机分区服从均匀分布（`uniform distribution`），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据, 每次执行得到的结果也不会相同。

  ~~~java
  public class ShuffleExample {
      public static void main(String[] args) throws Exception {
  
          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
  
  		 env.setParallelism(2);
  
          DataStreamSource<Integer> stream = env.socketTextStream("hadoop102", 7777);;
  
          stream.shuffle().print()
  
          env.execute();
      }
  }
  ~~~

#### 轮询分区（`Round-Robin`）

- 轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用`DataStream`的`.rebalance()`方法，就可以实现轮询重分区。`rebalance`使用的是`Round-Robin`负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去。

  ~~~
  stream.rebalance()
  ~~~

#### 重缩放分区（`rescale`）

- 重缩放分区和轮询分区非常相似。当调用`rescale()`方法时，其实底层也是使用`Round-Robin`算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。`rescale`的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌。

  ~~~
  stream.rescale()
  ~~~

####  广播（`broadcast`）

- 这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。

- 可以通过调用`DataStream`的`broadcast()`方法，将输入数据复制并发送到下游算子的所有并行任务中去。

  ~~~
  stream.broadcast()
  ~~~

#### 全局分区（`global`）

- 全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用`.global()`方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。

  ~~~
  stream.global()
  ~~~

#### 自定义分区（`Custom`）

- 当`Flink`提供的所有分区策略都不能满足用户的需求时，我们可以通过使用`partitionCustom()`方法来自定义分区策略

  1. 自定义分区器

     ~~~java
     public class MyPartitioner implements Partitioner<String> {
     
         @Override
         public int partition(String key, int numPartitions) {
             return Integer.parseInt(key) % numPartitions;
         }
     }
     ~~~

  2. 使用自定义分区

     ~~~java
     public class PartitionCustomDemo {
         public static void main(String[] args) throws Exception {
     //        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
             StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(new Configuration());
     
             env.setParallelism(2);
     
             DataStreamSource<String> socketDS = env.socketTextStream("hadoop102", 7777);
     
             DataStream<String> myDS = socketDS
                     .partitionCustom(
                             new MyPartitioner(),
                             value -> value);
                     
     
             myDS.print();
     
             env.execute();
         }
     }
     ~~~

## 分流

- “分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个`DataStream`，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。

### 使用侧输出流

- 简单来说，只需要调用上下文`ctx`的`.output()`方法，就可以输出任意类型的数据了。而侧输出流的标记和提取，都离不开一个“输出标签”（`OutputTag`），指定了侧输出流的`id`和类型。

  ~~~java
  public class SplitStreamByOutputTag {    
  public static void main(String[] args) throws Exception {
          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
  
          SingleOutputStreamOperator<WaterSensor> ds = env.socketTextStream("hadoop102", 7777)
                .map(new WaterSensorMapFunction());
  
  
          OutputTag<WaterSensor> s1 = new OutputTag<>("s1", Types.POJO(WaterSensor.class)){};
          OutputTag<WaterSensor> s2 = new OutputTag<>("s2", Types.POJO(WaterSensor.class)){};
         //返回的都是主流
          SingleOutputStreamOperator<WaterSensor> ds1 = ds.process(new ProcessFunction<WaterSensor, WaterSensor>()
          {
              @Override
              public void processElement(WaterSensor value, Context ctx, Collector<WaterSensor> out) throws Exception {
  
                  if ("s1".equals(value.getId())) {
                      ctx.output(s1, value);
                  } else if ("s2".equals(value.getId())) {
                      ctx.output(s2, value);
                  } else {
                      //主流
                      out.collect(value);
                  }
  
              }
          });
  
          ds1.print("主流，非s1,s2的传感器");
          SideOutputDataStream<WaterSensor> s1DS = ds1.getSideOutput(s1);
          SideOutputDataStream<WaterSensor> s2DS = ds1.getSideOutput(s2);
  
          s1DS.printToErr("s1");
          s2DS.printToErr("s2");
          
          env.execute();
  }
  }
  ~~~

### 基本合流操作

- 在实际应用中，我们经常会遇到来源不同的多条流，需要将它们的数据进行联合处理。所以`Flink`中合流的操作会更加普遍，对应的`API`也更加丰富。

#### 联合（`Union`）

- 最简单的合流操作，就是直接将多条流合在一起，叫作流的“联合”（union）。联合操作要求必须流中的数据类型必须相同，合并之后的新流会包括所有流中的元素，数据类型不变。

  ~~~java
  public class UnionExample {
  
      public static void main(String[] args) throws Exception {
  
          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
  
          env.setParallelism(1);
  
          DataStreamSource<Integer> ds1 = env.fromElements(1, 2, 3);
          DataStreamSource<Integer> ds2 = env.fromElements(2, 2, 3);
          DataStreamSource<String> ds3 = env.fromElements("2", "2", "3");
  
          ds1.union(ds2,ds3.map(Integer::valueOf))
             .print();
  
          env.execute();
      }
  }
  ~~~

#### 连接（`Connect`）

- 流的联合虽然简单，不过受限于数据类型不能改变，灵活性大打折扣，所以实际应用较少出现。除了联合（`union`），`Flink`还提供了另外一种方便的合流操作——连接（`connect`）。

##### 连接流（`ConnectedStreams`）

~~~
public class ConnectDemo {

    public static void main(String[] args) throws Exception {

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

//        DataStreamSource<Integer> source1 = env.fromElements(1, 2, 3);
//        DataStreamSource<String> source2 = env.fromElements("a", "b", "c");

        SingleOutputStreamOperator<Integer> source1 = env
                .socketTextStream("hadoop102", 7777)
                .map(i -> Integer.parseInt(i));

        DataStreamSource<String> source2 = env.socketTextStream("hadoop102", 8888);

        /**
         * TODO 使用 connect 合流
         * 1、一次只能连接 2条流
         * 2、流的数据类型可以不一样
         * 3、 连接后可以调用 map、flatmap、process来处理，但是各处理各的
         */
        ConnectedStreams<Integer, String> connect = source1.connect(source2);

        SingleOutputStreamOperator<String> result = connect.map(new CoMapFunction<Integer, String, String>() {
            @Override
            public String map1(Integer value) throws Exception {
                return "来源于数字流:" + value.toString();
            }

            @Override
            public String map2(String value) throws Exception {
                return "来源于字母流:" + value;
            }
        });

        result.print();

        env.execute();    }
}
~~~

- 上面的代码中，`ConnectedStreams`有两个类型参数，分别表示内部包含的两条流各自的数据类型；由于需要“一国两制”，因此调用`.map()`方法时传入的不再是一个简单的`MapFunction`，而是一个`CoMapFunction`，表示分别对两条流中的数据执行`map`操作。这个接口有三个类型参数，依次表示第一条流、第二条流，以及合并后的流中的数据类型。需要实现的方法也非常直白：`.map1()`就是对第一条流中数据的`map`操作，`.map2()`则是针对第二条流。

#### `CoProcessFunction`

- 与`CoMapFunction`类似，如果是调用`.map()`就需要传入一个`CoMapFunction`，需要实现`map1()`、`map2()`两个方法；

- 而调用`.process()`时，传入的则是一个`CoProcessFunction`。它也是“处理函数”家族中的一员，用法非常相似。它需要实现的就是`processElement1()`、`processElement2()`两个方法，在每个数据到来时，会根据来源的流调用其中的一个方法进行处理。

- 值得一提的是，`ConnectedStreams`也可以直接调用`.keyBy()`进行按键分区的操作，得到的还是一个``ConnectedStreams`：

  ~~~
  connectedStreams.keyBy(keySelector1, keySelector2);
  ~~~

  ~~~java
  public class ConnectKeybyDemo {
      public static void main(String[] args) throws Exception {
          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
          env.setParallelism(2);
  
          DataStreamSource<Tuple2<Integer, String>> source1 = env.fromElements(
                  Tuple2.of(1, "a1"),
                  Tuple2.of(1, "a2"),
                  Tuple2.of(2, "b"),
                  Tuple2.of(3, "c")
          );
          DataStreamSource<Tuple3<Integer, String, Integer>> source2 = env.fromElements(
                  Tuple3.of(1, "aa1", 1),
                  Tuple3.of(1, "aa2", 2),
                  Tuple3.of(2, "bb", 1),
                  Tuple3.of(3, "cc", 1)
          );
  
          ConnectedStreams<Tuple2<Integer, String>, Tuple3<Integer, String, Integer>> connect = source1.connect(source2);
  
          // 多并行度下，需要根据 关联条件 进行keyby，才能保证key相同的数据到一起去，才能匹配上
          ConnectedStreams<Tuple2<Integer, String>, Tuple3<Integer, String, Integer>> connectKey = connect.keyBy(s1 -> s1.f0, s2 -> s2.f0);
  
          SingleOutputStreamOperator<String> result = connectKey.process(
                  new CoProcessFunction<Tuple2<Integer, String>, Tuple3<Integer, String, Integer>, String>() {
                      // 定义 HashMap，缓存来过的数据，key=id，value=list<数据>
                      Map<Integer, List<Tuple2<Integer, String>>> s1Cache = new HashMap<>();
                      Map<Integer, List<Tuple3<Integer, String, Integer>>> s2Cache = new HashMap<>();
  
                      @Override
                      public void processElement1(Tuple2<Integer, String> value, Context ctx, Collector<String> out) throws Exception {
                          Integer id = value.f0;
                          // TODO 1.来过的s1数据，都存起来
                          if (!s1Cache.containsKey(id)) {
                              // 1.1 第一条数据，初始化 value的list，放入 hashmap
                              List<Tuple2<Integer, String>> s1Values = new ArrayList<>();
                              s1Values.add(value);
                              s1Cache.put(id, s1Values);
                          } else {
                              // 1.2 不是第一条，直接添加到 list中
                              s1Cache.get(id).add(value);
                          }
  
                          //TODO 2.根据id，查找s2的数据，只输出 匹配上 的数据
                          if (s2Cache.containsKey(id)) {
                              for (Tuple3<Integer, String, Integer> s2Element : s2Cache.get(id)) {
                                  out.collect("s1:" + value + "<--------->s2:" + s2Element);
                              }
                          }
                      }
  
                      @Override
                      public void processElement2(Tuple3<Integer, String, Integer> value, Context ctx, Collector<String> out) throws Exception {
                          Integer id = value.f0;
                          // TODO 1.来过的s2数据，都存起来
                          if (!s2Cache.containsKey(id)) {
                              // 1.1 第一条数据，初始化 value的list，放入 hashmap
                              List<Tuple3<Integer, String, Integer>> s2Values = new ArrayList<>();
                              s2Values.add(value);
                              s2Cache.put(id, s2Values);
                          } else {
                              // 1.2 不是第一条，直接添加到 list中
                              s2Cache.get(id).add(value);
                          }
  
                          //TODO 2.根据id，查找s1的数据，只输出 匹配上 的数据
                          if (s1Cache.containsKey(id)) {
                              for (Tuple2<Integer, String> s1Element : s1Cache.get(id)) {
                                  out.collect("s1:" + s1Element + "<--------->s2:" + value);
                              }
                          }
                      }
                  });
  
          result.print();
  
          env.execute();
      }
  }
  ~~~

## 输出算子（`Sink`）

- `Flink`作为数据处理框架，最终还是要把计算处理的结果写入外部存储，为外部应用提供支持。

###  连接到外部系统

- `Flink`的`DataStream API`专门提供了向外部写入数据的方法：`addSink`。与`addSource`类似，`addSink`方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；`Flink`程序中所有对外的输出操作，一般都是利用`Sink`算子完成的。

- `Flink1.12`开始，同样重构了`Sink`架构

  ~~~
  stream.sinkTo(…)
  ~~~

- `Flink`官方为我们提供了一部分的框架的`Sink`连接器。

- 除`Flink`官方之外，`Apache Bahir`框架，也实现了一些其他第三方系统与`Flink`的连接器。

### 输出到文件

- `Flink`专门提供了一个流式文件系统的连接器：`FileSink`，为批处理和流处理提供了一个统一的`Sink`，它可以将分区文件写入Flink支持的文件系统。
- `FileSink`支持行编码（`Row-encoded`）和批量编码（`Bulk-encoded`）格式。这两种不同的方式都有各自的构建器（`builder`），可以直接调用`FileSink`的静态方法：
- 行编码： `FileSink.forRowFormat`（`basePath，rowEncoder`）。
- 批量编码： `FileSink.forBulkFormat`（`basePath，bulkWriterFactory`）。

~~~java
public class SinkFile {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 每个目录中，都有 并行度个数的 文件在写入
        env.setParallelism(2);

        // 必须开启checkpoint，否则一直都是 .inprogress
        env.enableCheckpointing(2000, CheckpointingMode.EXACTLY_ONCE);


        DataGeneratorSource<String> dataGeneratorSource = new DataGeneratorSource<>(
                new GeneratorFunction<Long, String>() {
                    @Override
                    public String map(Long value) throws Exception {
                        return "Number:" + value;
                    }
                },
                Long.MAX_VALUE,
                RateLimiterStrategy.perSecond(1000),
                Types.STRING
        );

        DataStreamSource<String> dataGen = env.fromSource(dataGeneratorSource, WatermarkStrategy.noWatermarks(), "data-generator");

        // 输出到文件系统
        FileSink<String> fieSink = FileSink
                // 输出行式存储的文件，指定路径、指定编码
                .<String>forRowFormat(new Path("f:/tmp"), new SimpleStringEncoder<>("UTF-8"))
                // 输出文件的一些配置： 文件名的前缀、后缀
                .withOutputFileConfig(
                        OutputFileConfig.builder()
                                .withPartPrefix("atguigu-")
                                .withPartSuffix(".log")
                                .build()
                )
                // 按照目录分桶：如下，就是每个小时一个目录
                .withBucketAssigner(new DateTimeBucketAssigner<>("yyyy-MM-dd HH", ZoneId.systemDefault()))
                // 文件滚动策略:  1分钟 或 1m
                .withRollingPolicy(
                        DefaultRollingPolicy.builder()
                                .withRolloverInterval(Duration.ofMinutes(1))
                                .withMaxPartSize(new MemorySize(1024*1024))
                                .build()
                )
                .build();


        dataGen.sinkTo(fieSink);

        env.execute();
    }
}
~~~

### 输出到`Kafka`

1. 添加`Kafka `连接器依赖

   - 由于我们已经测试过从`Kafka`数据源读取数据，连接器相关依赖已经引入，这里就不重复介绍了。

2. 启动`Kafka`集群

3. 编写输出到`Kafka`的示例代码

   ~~~java
   public class SinkKafka {
       public static void main(String[] args) throws Exception {
           StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
           env.setParallelism(1);
   
           // 如果是精准一次，必须开启checkpoint（后续章节介绍）
           env.enableCheckpointing(2000, CheckpointingMode.EXACTLY_ONCE);
   
   
           SingleOutputStreamOperator<String> sensorDS = env
                   .socketTextStream("hadoop102", 7777);
   
           /**
            * Kafka Sink:
            * TODO 注意：如果要使用 精准一次 写入Kafka，需要满足以下条件，缺一不可
            * 1、开启checkpoint（后续介绍）
            * 2、设置事务前缀
            * 3、设置事务超时时间：   checkpoint间隔 <  事务超时时间  < max的15分钟
            */
           KafkaSink<String> kafkaSink = KafkaSink.<String>builder()
                   // 指定 kafka 的地址和端口
                   .setBootstrapServers("hadoop102:9092,hadoop103:9092,hadoop104:9092")
                   // 指定序列化器：指定Topic名称、具体的序列化
                   .setRecordSerializer(
                           KafkaRecordSerializationSchema.<String>builder()
                                   .setTopic("ws")
                                   .setValueSerializationSchema(new SimpleStringSchema())
                                   .build()
                   )
                   // 写到kafka的一致性级别： 精准一次、至少一次
                   .setDeliveryGuarantee(DeliveryGuarantee.EXACTLY_ONCE)
                   // 如果是精准一次，必须设置 事务的前缀
                   .setTransactionalIdPrefix("atguigu-")
                   // 如果是精准一次，必须设置 事务超时时间: 大于checkpoint间隔，小于 max 15分钟
                   .setProperty(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, 10*60*1000+"")
                   .build();
   
   
           sensorDS.sinkTo(kafkaSink);
   
   
           env.execute();
       }}
   
   ~~~

   - 自定义序列化器，实现带key的record:

   ~~~java
   public class SinkKafkaWithKey {
       public static void main(String[] args) throws Exception {
           StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
           env.setParallelism(1);
   
           env.enableCheckpointing(2000, CheckpointingMode.EXACTLY_ONCE);
           env.setRestartStrategy(RestartStrategies.noRestart());
   
   
           SingleOutputStreamOperator<String> sensorDS = env
                   .socketTextStream("hadoop102", 7777);
   
   
           /**
            * 如果要指定写入kafka的key，可以自定义序列化器：
            * 1、实现 一个接口，重写 序列化 方法
            * 2、指定key，转成 字节数组
            * 3、指定value，转成 字节数组
            * 4、返回一个 ProducerRecord对象，把key、value放进去
            */
           KafkaSink<String> kafkaSink = KafkaSink.<String>builder()
                   .setBootstrapServers("hadoop102:9092,hadoop103:9092,hadoop104:9092")
                   .setRecordSerializer(
                           new KafkaRecordSerializationSchema<String>() {
   
                               @Nullable
                               @Override
                               public ProducerRecord<byte[], byte[]> serialize(String element, KafkaSinkContext context, Long timestamp) {
                                   String[] datas = element.split(",");
                                   byte[] key = datas[0].getBytes(StandardCharsets.UTF_8);
                                   byte[] value = element.getBytes(StandardCharsets.UTF_8);
                                   return new ProducerRecord<>("ws", key, value);
                               }
                           }
                   )
                   .setDeliveryGuarantee(DeliveryGuarantee.EXACTLY_ONCE)
                   .setTransactionalIdPrefix("atguigu-")
                   .setProperty(ProducerConfig.TRANSACTION_TIMEOUT_CONFIG, 10 * 60 * 1000 + "")
                   .build();
   
   
           sensorDS.sinkTo(kafkaSink);
   
   
           env.execute();
       }
   }
   
   ~~~

### 输出到`MySQL`（`JDBC`）

1. 添加依赖

   ~~~xml
   <dependency>
       <groupId>mysql</groupId>
       <artifactId>mysql-connector-java</artifactId>
       <version>8.0.27</version>
   </dependency>
   
   <dependency>
       <groupId>org.apache.flink</groupId>
       <artifactId>flink-connector-jdbc</artifactId>
       <version>3.1.0-1.17</version>
   </dependency>
   ~~~

2. 启动`MySQL`，在`test`库下建表`ws`

   ~~~sql
   CREATE TABLE `ws` (
     `id` varchar(100) NOT NULL,
     `ts` bigint(20) DEFAULT NULL,
     `vc` int(11) DEFAULT NULL,
     PRIMARY KEY (`id`)
   ) ENGINE=InnoDB DEFAULT CHARSET=utf8
   ~~~

3. 编写输出到`MySQL`的示例代码

   ~~~java
   public class SinkMySQL {
       public static void main(String[] args) throws Exception {
           StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
           env.setParallelism(1);
   
   
           SingleOutputStreamOperator<WaterSensor> sensorDS = env
                   .socketTextStream("hadoop102", 7777)
                   .map(new WaterSensorMapFunction());
   
   
           /**
            * TODO 写入mysql
            * 1、只能用老的sink写法： addsink
            * 2、JDBCSink的4个参数:
            *    第一个参数： 执行的sql，一般就是 insert into
            *    第二个参数： 预编译sql， 对占位符填充值
            *    第三个参数： 执行选项 ---》 攒批、重试
            *    第四个参数： 连接选项 ---》 url、用户名、密码
            */
           SinkFunction<WaterSensor> jdbcSink = JdbcSink.sink(
                   "insert into ws values(?,?,?)",
                   new JdbcStatementBuilder<WaterSensor>() {
                       @Override
                       public void accept(PreparedStatement preparedStatement, WaterSensor waterSensor) throws SQLException {
                           //每收到一条WaterSensor，如何去填充占位符
                           preparedStatement.setString(1, waterSensor.getId());
                           preparedStatement.setLong(2, waterSensor.getTs());
                           preparedStatement.setInt(3, waterSensor.getVc());
                       }
                   },
                   JdbcExecutionOptions.builder()
                           .withMaxRetries(3) // 重试次数
                           .withBatchSize(100) // 批次的大小：条数
                           .withBatchIntervalMs(3000) // 批次的时间
                           .build(),
                   new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()
                           .withUrl("jdbc:mysql://hadoop102:3306/test?serverTimezone=Asia/Shanghai&useUnicode=true&characterEncoding=UTF-8")
                           .withUsername("root")
                           .withPassword("000000")
                           .withConnectionCheckTimeoutSeconds(60) // 重试的超时时间
                           .build()
           );
   
   
           sensorDS.addSink(jdbcSink);
   
   
           env.execute();
       }
   }
   ~~~









  







​     
