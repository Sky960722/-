# `Flink` 概述

## `Flink`定义

- `Flink`官网：https://flink.apache.org/
- `Flink`核心目标：数据流上的有状态计算(`Stateful Computations over Data Streams`)
- `Apache Flink`是一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算

### 有状态数据流

- 把流处理需要的额外数据保存成一个“状态”，然后针对这条数据进行处理，并且更新状态。就是所谓的"有状态的流处理"
  - 状态在内存中：
    - 优点：速度快
    - 缺点：可靠性差
  - 状态在分布式系统中
    - 优点：可靠性高
    - 缺点：速度慢

## `Flink`特点

- 高吞吐和低延迟：每秒处理数百万个事件，毫秒级延迟
- 结果的准确性：`Flink`提供了事件时间(`event-time`)和处理时间(`processing-time`)语义。对于乱序事件流，事件时间语义仍然提供一致且准确的结果
- 精确一次(`exactly-once`)：状态一致性保证
- 可以连接到最常用的外部系统：如`kafka`、`Hive`、`JDBC`、`HDFS`、`Redis`等
- 高可用

## `Flink`项目配置
### `maven`配置

~~~
<properties>
        <flink.version>1.17.0</flink.version>
</properties>


    <dependencies>
        <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-streaming-java</artifactId>
            <version>${flink.version}</version>
        </dependency>

     <dependency>
            <groupId>org.apache.flink</groupId>
            <artifactId>flink-clients</artifactId>
            <version>${flink.version}</version>
     </dependency>
</dependencies>

~~~

### `WordCount`流处理代码

~~~
public class StreamWordCount {

    public static void main(String[] args) throws Exception {
    
        // 1. 创建流式执行环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        
        // 2. 读取文件
        DataStreamSource<String> lineStream = env.readTextFile("input/words.txt");
        
        // 3. 转换、分组、求和，得到统计结果
        SingleOutputStreamOperator<Tuple2<String, Long>> sum = lineStream.flatMap(new FlatMapFunction<String, Tuple2<String, Long>>() {
            @Override
            public void flatMap(String line, Collector<Tuple2<String, Long>> out) throws Exception {

                String[] words = line.split(" ");

                for (String word : words) {
                    out.collect(Tuple2.of(word, 1L));
                }
            }
        }).keyBy(data -> data.f0)
           .sum(1);

        // 4. 打印
        sum.print();
        
        // 5. 执行
        env.execute();
    }
}
~~~

- tips:

  - `Flink`本身是流批统一的处理架构，批量的数据集本质上也是流，没有必要用两套不同的`API`来实现。所以从`Flink 1.12`开始，官方推荐的做法是直接使用`DataStream API`，在提交任务时通过将执行模式设为`BATCH`来进行批处理：

    ~~~
    $ bin/flink run -Dexecution.runtime-mode=BATCH BatchWordCount.jar
    ~~~

# 系统架构

## 作业管理器（`JobManager`）

- `JobManager`是一个`Flink`集群中任务管理和调度的核心，是控制应用执行的主进程。也就是说，每个应用都应该被唯一的`JobManager`所控制执行。
- `JobManger`又包含3个不同的组件。

### JobMaster

- `JobMaster`是`JobManager`中最核心的组件，负责处理单独的作业（`Job`）。所以`JobMaster`和具体的`Job`是一一对应的，多个`Job`可以同时运行在一个`Flink`集群中, 每个`Job`都有一个自己的`JobMaster`。

- 在作业提交时，`JobMaster`会先接收到要执行的应用。`JobMaster`会把`JobGraph`转换成一个物理层面的数据流图，这个图被叫作“执行图”（`ExecutionGraph`），它包含了所有可以并发执行的任务。`JobMaster`会向资源管理器（`ResourceManager`）发出请求，申请执行任务必要的资源。一旦它获取到了足够的资源，就会将执行图分发到真正运行它们的`TaskManager`上。

  而在运行过程中，`JobMaster`会负责所有需要中央协调的操作，比如说检查点（`checkpoints`）的协调。

### 资源管理器（`ResourceManager`）

- `ResourceManager`主要负责资源的分配和管理，在`Flink` 集群中只有一个。所谓“资源”，主要是指`TaskManager`的任务槽（`task slots`）。任务槽就是`Flink`集群中的资源调配单元，包含了机器用来执行计算的一组`CPU`和内存资源。每一个任务（`Task`）都需要分配到一个`slot`上执行。
- 这里注意要把`Flink`内置的`ResourceManager`和其他资源管理平台（比如`YARN`）的`ResourceManager`区分开。

### 分发器（`Dispatcher`）

- `Dispatcher`主要负责提供一个`REST`接口，用来提交应用，并且负责为每一个新提交的作业启动一个新的`JobMaster` 组件
- `Dispatcher`也会启动一个`Web UI`，用来方便地展示和监控作业执行的信息。`Dispatcher`在架构中并不是必需的，在不同的部署模式下可能会被忽略掉。

## 任务管理器（`TaskManager`）

- `TaskManager`是`Flink`中的工作进程，数据流的具体计算就是它来做的。`Flink`集群中必须至少有一个`TaskManager`；每一个`TaskManager`都包含了一定数量的任务槽（`task slots`）。`Slot`是资源调度的最小单位，`slot`的数量限制了`TaskManager`能够并行处理的任务数量。
- 启动之后，`TaskManager`会向资源管理器注册它的`slots`；收到资源管理器的指令后，`TaskManager`就会将一个或者多个槽位提供给`JobMaster`调用，`JobMaster`就可以分配任务来执行了。
- 在执行过程中，`TaskManager`可以缓冲数据，还可以跟其他运行同一应用的`TaskManager`交换数据。

# `Flink`部署

## `Flink`集群角色

- `Flink`提交作业和执行任务，需要几个关键组件：
  - 客户端(`Client`)：代码由客户端获取并做转换，之后提交给`JobManager`
  - `JobManager`是`Flink`集群里的"管事人"，对作业进行中央调度管理；而它获取到要执行的作业后，会进一步处理转换，然后分发任务给众多的`TaskManager`
  - `TaskManager`，真正"干活的人"，数据的处理操作都是它们来做的

## 部署模式

- `Flink`为各种场景提供了不同的部署模式，主要有以下三种：会话模式(`Session Mode`)、单作业模式(`Per-Job Mode`)、应用模式(`Application Mode`)
- 主要区别：集群的生命周期以及资源的分配方式；以及应用的`main`方法到底哪里执行——客户端(`Client`)还是`JobManager`

### 会话模式(`Session Mode`)

- 需要先启动一个集群，保持一个会话，在这个会话中通过客户端提交作业。集群启动时所有资源都已经确定，所以所有提交的作业会竞争集群中的资源
- 会话模式比较适合单个规模小、执行时间短的大量作业

### 单作业模式(`Per-Job Mode`)

- 考虑为每个提交的作业启动一个集群，这就是所谓的单作业(`Per-Job`)模式
- 这些特性使得单作业模式在生产环境运行更加稳定，所以是实际应用的首选模式
- `Flink`本身无法直接这样运行，所以单作业模式一般需要借助一些资源管理框架来启动集群，比如`YARN`、`Kubernetes(K8S)`

### 应用模式(`Application Mode`)

- `JobManager`只为执行这一个应用而存在，执行结束之后`JobManager`关闭了，这就是所谓的应用模式
- 应用模式与单作业模式，都是提交作业之后才创建集群；单作业模式是通过客户端来提交的，客户端解析出的每一个作业对应一个集群，而应用模式下，是直接由`JobManager`执行应用程序的

## 运行模式

### `Standalone`运行模式

- 独立模式是独立运行的，不依赖任何外部的资源管理平台；当然独立也是有代价的；如果资源不足，或者出现故障，没有自动扩展或重分配资源的保证，必须手动处理。

#### 集群规划

| 节点服务器 | `hadoop102`                     | `hadoop103`   | `hadoop104`   |
| ---------- | ------------------------------- | ------------- | ------------- |
| 角色       | `JobManager`<br />`TaskManager` | `TaskManager` | `TaskManager` |

#### 下载并解压安装包

1. 下载安装包`flink-1.17.0-bin-scala_2.12.tgz`，将该`jar`包上传到`hadoop102`节点服务器的`/opt/software`路径上

2. 在`/opt/software`路径上解压`flink-1.17.0-bin-scala_2.12.tgz`到`/opt/module`路径上。

   ~~~
   tar -zxvf flink-1.17.0-bin-scala_2.12.tgz -C /opt/module/
   ~~~

#### 修改集群配置

1. 进入`conf`路径，修改`flink-conf.yaml`文件，指定`hadoop102`节点服务器为`JobManager `

   ~~~
   vim flink-conf.yaml
   ~~~

   - 修改如下内容：

   ~~~
   # JobManager节点地址.
   jobmanager.rpc.address: hadoop102
   jobmanager.bind-host: 0.0.0.0
   rest.address: hadoop102
   rest.bind-address: 0.0.0.0
   # TaskManager节点地址.需要配置为当前机器名
   taskmanager.bind-host: 0.0.0.0
   taskmanager.host: hadoop102
   ~~~

2. 修改`workers`文件，指定`hadoop102`、`hadoop103`和`hadoop104`为`TaskManager`

   ~~~
   vim workers
   ~~~

   - 修改如下内容

   ~~~
   hadoop102
   hadoop103
   hadoop104
   ~~~

3. 修改`masters`文件

   ~~~
   vim masters
   ~~~

   - 修改如下内容：

   ~~~
   hadoop102:8081
   ~~~

4. 另外，在`flink-conf.yaml`文件中还可以对集群中的`JobManager`和`TaskManager`组件进行优化配置，主要配置项如下

   - `jobmanager.memory.process.size`：对`JobManager`进程可使用到的全部内存进行配置，包括`JVM`元空间和其他开销，默认为`1600M`，可以根据集群规模进行适当调整。
   - `taskmanager.memory.process.size`：对`TaskManager`进程可使用到的全部内存进行配置，包括`JVM`元空间和其他开销，默认为`1728M`，可以根据集群规模进行适当调整。
   - `taskmanager.numberOfTaskSlots`：对每个`TaskManager`能够分配的`Slot`数量进行配置，默认为`1`，可根据`TaskManager`所在的机器能够提供给`Flink`的`CPU`数量决定。所谓`Slot`就是`TaskManager`中具体运行一个任务所分配的计算资源
   - `parallelism.default`：`Flink`任务执行的并行度，默认为`1`。优先级低于代码中进行的并行度配置和任务提交时使用参数指定的并行度数量。

5. 分发安装目录

   1. 配置修改完毕后，将`Flink`安装目录发给另外两个节点服务器。

      ~~~
      xsync flink-1.17.0/
      ~~~

   2. 修改`hadoop103`的 `taskmanager.host`

      ~~~
      vim flink-conf.yaml
      ~~~

      - 修改如下内容：

      ~~~
      # TaskManager节点地址.需要配置为当前机器名
      taskmanager.host: hadoop103
      ~~~

   3. 修改`hadoop104`的 `taskmanager.host`

      ~~~
      vim flink-conf.yaml
      ~~~

      - 修改如下内容：

      ~~~
      # TaskManager节点地址.需要配置为当前机器名
      taskmanager.host: hadoop104
      ~~~

6. 启动集群

   1. 在`hadoop102`节点服务器上执行`start-cluster.sh`启动`Flink`集群：

      ~~~
      bin/start-cluster.sh
      ~~~

   2. 查看进程情况：

      ~~~
      jpsall
      ~~~

7. 访问`Web UI`

   - 启动成功后，同样可以访问`http://hadoop102:8081`对`flink`集群和任务进行监控管理。

8. 向集群提交作业

   1. 环境准备

      ~~~
      nc -lk 7777
      ~~~

   2. 程序打包

      1. 在我们编写的`Flink`入门程序的`pom.xml`文件中添加打包插件的配置，具体如下：

         ~~~xml
         <build>
             <plugins>
                 <plugin>
                     <groupId>org.apache.maven.plugins</groupId>
                     <artifactId>maven-shade-plugin</artifactId>
                     <version>3.2.4</version>
                     <executions>
                         <execution>
                             <phase>package</phase>
                             <goals>
                                 <goal>shade</goal>
                             </goals>
                             <configuration>
                                 <artifactSet>
                                     <excludes>
                                         <exclude>com.google.code.findbugs:jsr305</exclude>
                                         <exclude>org.slf4j:*</exclude>
                                         <exclude>log4j:*</exclude>
                                     </excludes>
                                 </artifactSet>
                                 <filters>
                                     <filter>
                                         <!-- Do not copy the signatures in the META-INF folder.
                                         Otherwise, this might cause SecurityExceptions when using the JAR. -->
                                         <artifact>*:*</artifact>
                                         <excludes>
                                             <exclude>META-INF/*.SF</exclude>
                                             <exclude>META-INF/*.DSA</exclude>
                                             <exclude>META-INF/*.RSA</exclude>
                                         </excludes>
                                     </filter>
                                 </filters>
                                 <transformers combine.children="append">
                                     <transformer
                                             implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer">
                                     </transformer>
                                 </transformers>
                             </configuration>
                         </execution>
                     </executions>
                 </plugin>
             </plugins>
         </build>
         
         ~~~

      2. 打包完成后，在`target`目录下即可找到所需`JAR`包，`JAR`包会有两个，`FlinkTutorial-1.0-SNAPSHOT.jar`和`FlinkTutorial-1.0-SNAPSHOT-jar-with-dependencies.jar`，因为集群中已经具备任务运行所需的所有依赖，所以建议使用`FlinkTutorial-1.0-SNAPSHOT.jar`。

   3. 在`Web UI`上提交作业

         1. 任务打包完成后，打开`Flink`的`WEB UI`页面，在右侧导航栏点击`“Submit New Job”`，然后点击按钮`“+ Add New”`，选择要上传运行的`JAR`包
         2. 点击该`JAR`包，出现任务配置页面，进行相应配置。
            - 主要配置程序入口主类的全类名，任务运行的并行度，任务运行所需的配置参数和保存点路径等。配置完成后，即可点击按钮`“Submit”`，将任务提交到集群运行。

      4. 点击该任务，可以查看任务运行的具体情况，也可以通过点击“`Cancel Job`”结束任务运行。

   4. 命令行提交作业
   
         - 除了通过`WEB UI`界面提交任务之外，也可以直接通过命令行来提交任务。可以先把`jar`包直接上传到目录`flink-1.17.0`
   
         1. 将`flink`程序运行jar包上传到`/opt/module/flink-1.17.0`路径
   
         2. 进入到`flink`的安装路径下，在命令行使用`flink run`命令提交作业。
   
            ~~~
             bin/flink run -m hadoop102:8081 -c com.atguigu.wc.SocketStreamWordCount ./FlinkTutorial-1.0-SNAPSHOT.jar
            ~~~
   
            - 这里的参数 `-m`指定了提交到的`JobManager`，`-c`指定了入口类。
   
         3. 在浏览器中打开`Web UI`，`http://hadoop102:8081`查看应用执行情况。用`netcat`输入数据，可以在`TaskManager`的标准输出（`Stdout`）看到对应的统计结果。

##### 会话模式部署

- 提前启动集群，并通过`Web`面客户端提交任务（可以多个任务，但是集群资源固定）。

##### 单作业模式部署

- `Flink`的`Standalone`集群并不支持单作业模式部署。因为单作业模式需要借助一些资源管理平台。

#####  应用模式部署

- 应用模式下不会提前创建集群，所以不能调用`start-cluster.sh`脚本。可以使用同样在`bin`目录下的`standalone-job.sh`来创建一个`JobManager`。

- 具体步骤

  1. 环境准备。在`hadoop102`中执行以下命令启动`netcat`。

     ~~~
      nc -lk 7777
     ~~~

  2. 进入到`Flink`的安装路径下，将应用程序的`jar`包放到`lib/`目录下。

     ~~~
      mv FlinkTutorial-1.0-SNAPSHOT.jar lib/
     ~~~

  3. 执行以下命令，启动`JobManager`

     ~~~
      bin/standalone-job.sh start --job-classname com.atguigu.wc.SocketStreamWordCount
     ~~~

  4. 同样是使用bin目录下的脚本，启动TaskManager

     ~~~
     bin/taskmanager.sh start
     ~~~

  5. 如果希望停掉集群，同样可以使用脚本，命令如下。

     ~~~
      bin/taskmanager.sh stop
      bin/standalone-job.sh stop
     ~~~

### `YARN`运行模式

- `YARN`上部署的过程是：客户端把`Flink`应用提交给`Yarn`的`ResourceManager`，`Yarn`的`ResourceManager`会向`Yarn`的`NodeManager`申请容器。在这些容器上，`Flink`会部署`JobManager`和`TaskManager`的实例，从而启动集群。`Flink`会根据运行在`JobManger`上的作业所需要的`Slot`数量动态分配`TaskManager`资源。

#### 相关准备和配置

1. 配置环境变量，增加环境变量配置如下

   ~~~
   sudo vim /etc/profile.d/my_env.sh
   ~~~

   ~~~
   HADOOP_HOME=/opt/module/hadoop-3.3.4
   export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
   export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
   export HADOOP_CLASSPATH=`hadoop classpath`
   ~~~

2. 启动`Hadoop`集群，包括`HDFS`和`YARN`

   ~~~
    start-dfs.sh
    start-yarn.sh
   ~~~

##### 会话模式部署

- `YARN`的会话模式与独立集群略有不同，需要首先申请一个`YARN`会话（`YARN Session`）来启动`Flink`集群。具体步骤如下：

  1. 启动集群

     1. 启动`Hadoop`集群（`HDFS`、`YARN`）。

     2. 执行脚本命令向`YARN`集群申请资源，开启一个`YARN`会话，启动`Flink`集群。

        ~~~
        bin/yarn-session.sh -nm test
        ~~~

        - 可用参数解读：
          - `-d`：分离模式，如果你不想让`Flink YARN`客户端一直前台运行，可以使用这个参数，即使关掉当前对话窗口，`YARN session`也可以后台运行。
          - `-jm`（`--jobManagerMemory`）：配置`JobManager`所需内存，默认单位`MB`。
          - `-nm`（`--name`）：配置在`YARN UI`界面上显示的任务名。
          -  `-qu`（`--queue`）：指定`YARN`队列名。
          - `-tm`（`--taskManager`）：配置每个`TaskManager`所使用内存。
          - 注意：`Flink1.11.0`版本不再使用`-n`参数和`-s`参数分别指定`TaskManager`数量和`slot`数量，`YARN`会按照需求动态分配`TaskManager`和`slot`。所以从这个意义上讲，`YARN`的会话模式也不会把集群资源固定，同样是动态分配的。
          - `YARN Session`启动之后会给出一个`Web UI`地址以及一个`YARN application ID`，如下所示，用户可以通过`Web UI`或者命令行两种方式提交作业。

  2. 提交作业

     1. 通过`Web UI`提交作业

        - 这种方式比较简单，与上文所述`Standalone`部署模式基本相同。

     2. 通过命令行提交作业

        1.  将`FlinkTutorial-1.0-SNAPSHOT.jar`任务上传至集群

        2. 执行以下命令将该任务提交到已经开启的`Yarn-Session`中运行

           ~~~
           bin/flink run -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar
           ~~~

           - 客户端可以自行确定`JobManager`的地址，也可以通过`-m`或者`-jobmanager`参数指定`JobManager`的地址，`JobManager`的地址在YARN Session的启动页面中可以找到。

##### 单作业模式部署

- 在`YARN`环境中，由于有了外部平台做资源调度，所以我们也可以直接向`YARN`提交一个单独的作业，从而启动一个`Flink`集群。

  1. 执行命令提交作业

     ~~~
      bin/flink run -d -t yarn-per-job -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar
     ~~~

     - 注意：如果启动过程中报如下异常

     ~~~
     Exception in thread “Thread-5” java.lang.IllegalStateException: Trying to access closed classloader. Please check if you store classloaders directly or indirectly in static fields. If the stacktrace suggests that the leak occurs in a third party library and cannot be fixed immediately, you can disable this check with the configuration ‘classloader.check-leaked-classloader’.
     at org.apache.flink.runtime.execution.librarycache.FlinkUserCodeClassLoaders
     ~~~

     - 解决办法：在`flink`的`/opt/module/flink-1.17.0/conf/flink-conf.yaml`配置文件中设置

     ~~~
     vim flink-conf.yaml
     
     classloader.check-leaked-classloader: false
     ~~~

  2. 在`YARN`的`ResourceManager`界面查看执行情况。

  3. 可以使用命令行查看或取消作业，命令如下

     ~~~
     bin/flink list -t yarn-per-job -Dyarn.application.id=application_XXXX_YY
     
     bin/flink cancel -t yarn-per-job -Dyarn.application.id=application_XXXX_YY <jobId>
     ~~~

     - 这里的`application_XXXX_YY`是当前应用的`ID`，`<jobId>`是作业的`ID`。注意如果取消作业，整个`Flink`集群也会停掉。

##### 应用模式部署

- 应用模式同样非常简单，与单作业模式类似，直接执行`flink run-application`命令即可。

###### 命令行提交

1. 执行命令提交作业

   ~~~
   bin/flink run-application -t yarn-application -c com.atguigu.wc.SocketStreamWordCount FlinkTutorial-1.0-SNAPSHOT.jar 
   ~~~

2. 在命令行中查看或取消作业

   ~~~
    bin/flink list -t yarn-application -Dyarn.application.id=application_XXXX_YY
    
    bin/flink cancel -t yarn-application -Dyarn.application.id=application_XXXX_YY <jobId>
   ~~~

###### 上传`HDFS`提交

- 可以通过`yarn.provided.lib.dirs`配置选项指定位置，将`flink`的依赖上传到远程。

1. 上传`flink`的`lib`和`plugins`到`HDFS`上

   ~~~
    hadoop fs -mkdir /flink-dist
    hadoop fs -put lib/ /flink-dist
    hadoop fs -put plugins/ /flink-dist
   ~~~

2. 上传自己的`jar`包到`HDFS`

   ~~~
   hadoop fs -mkdir /flink-jars
   hadoop fs -put FlinkTutorial-1.0-SNAPSHOT.jar /flink-jars
   ~~~

3. 提交作业

   ~~~
   bin/flink run-application -t yarn-application	-Dyarn.provided.lib.dirs="hdfs://hadoop102:8020/flink-dist"	-c com.atguigu.wc.SocketStreamWordCount  hdfs://hadoop102:8020/flink-jars/FlinkTutorial-1.0-SNAPSHOT.jar
   ~~~

   - 这种方式下，`flink`本身的依赖和用户`jar`可以预先上传到`HDFS`，而不需要单独发送到集群，这就使得作业提交更加轻量了。

### 历史服务器

- 运行 `Flink job` 的集群一旦停止，只能去 `yarn` 或本地磁盘上查看日志，不再可以查看作业挂掉之前的运行的 `Web UI`。`Flink`提供了历史服务器，用来在相应的 `Flink` 集群关闭后查询已完成作业的统计信息。通过 `History Server` 我们才能查询这些已完成作业的统计信息，无论是正常退出还是异常退出。
- 此外，它对外提供了 `REST API`，它接受 `HTTP` 请求并使用 `JSON` 数据进行响应。`Flink` 任务停止后，`JobManager` 会将已经完成任务的统计信息进行存档，`History Server` 进程则在任务停止后可以对任务统计信息进行查询。

1. 创建存储目录

   ~~~
   hadoop fs -mkdir -p /logs/flink-job
   ~~~

2. 在` **flink-config.yaml`中添加如下配置

   ~~~
   jobmanager.archive.fs.dir: hdfs://hadoop102:8020/logs/flink-job
   historyserver.web.address: hadoop102
   historyserver.web.port: 8082
   historyserver.archive.fs.dir: hdfs://hadoop102:8020/logs/flink-job
   historyserver.archive.fs.refresh-interval: 5000
   ~~~

3. 启动历史服务器

   ~~~
   bin/historyserver.sh start
   ~~~

4. 停止历史服务器

   ~~~
   bin/historyserver.sh stop
   ~~~

5. 在浏览器地址栏输入：`http://hadoop102:8082`查看已经停止的`job` 的统计信息

## 作业提交流程

### Standalone会话模式作业提交流程

- 逻辑流图（StreamGraph）→ 作业图（JobGraph）→ 执行图（ExecutionGraph）→ 物理图（Physical Graph）

### 逻辑流图（`StreamGraph`）

- 这是根据用户通过 DataStream API编写的代码生成的最初的DAG图，用来表示程序的拓扑结构。这一步一般在客户端完成。

### 作业图（`JobGraph`）

- StreamGraph经过优化后生成的就是作业图（JobGraph），这是提交给 JobManager 的数据结构，确定了当前作业中所有任务的划分。
- 主要的优化为：将多个符合条件的节点链接在一起合并成一个任务节点，形成算子链，这样可以减少数据交换的消耗。JobGraph一般也是在客户端生成的，在作业提交时传递给JobMaster。
- 我们提交作业之后，打开Flink自带的Web UI，点击作业就能看到对应的作业图。

### 执行图（`ExecutionGraph`）

- JobMaster收到JobGraph后，会根据它来生成执行图（ExecutionGraph）。ExecutionGraph是JobGraph的并行化版本，是调度层最核心的数据结构。与JobGraph最大的区别就是按照并行度对并行子任务进行了拆分，并明确了任务间数据传输的方式。

### 物理图（`Physical Graph`）

- JobMaster生成执行图后，会将它分发给TaskManager；各个TaskManager会根据执行图部署任务，最终的物理执行过程也会形成一张“图”，一般就叫作物理图（Physical Graph）。这只是具体执行层面的图，并不是一个具体的数据结构。
- 物理图主要就是在执行图的基础上，进一步确定数据存放的位置和收发的具体方式。有了物理图，TaskManager就可以对传递来的数据进行处理计算了。

## Yarn应用模式作业提交流程

- ![提交流程](.\图片\提交流程.png)
