# Spark SQL

- `Spark SQL`是`Spark`用于结构化数据(`structured data`)处理的`Spark`模块

## Hive and SparkSQL

- `Hive`是早期唯一运行再Hadoop上的`SQL-on-Hadoop`工具。但是`MR`计算过程中大量的中间磁盘落地过程消耗了大量的`I/O`，降低了运行效率，为了提高`SQL-on-Hadoop`的效率，推出了`Shark`框架，让`Hive`运行在`Spark`引擎上。
- `Shark`和`Hive`之间的不兼容等问题，制约了`Spark`各个组建的相互集成，所以提出了`SparkSQL`框架

## SparkSQL特点

### 易整合

- 无缝的整合了`SQL`查询和`Spark`编程

### 统一的数据访问

- 使用相同的方式连接不同的数据源

### 兼容`Hive`

- 在已有的仓库上直接运行`SQL`或者`HiveQL`

### 标准数据连接

- 通过`JDBC`或者`ODBC`来连接

## `Spark-submit,Spark-shell,Spark-sql`

### Spark-submit

- 是用于提交和运行 `Spark` 应用程序的命令行工具。通过 `spark-submit`，你可以将你开发的 `Spark` 应用程序打包成一个 `JAR` 文件，并将其提交到 `Spark` 集群上进行执行。`spark-submit` 提供了一些选项和参数，用于配置和调优应用程序的执行环境、资源分配和应用程序的配置。

### spark-shell

- 是一个交互式的 `Scala shell`，用于在命令行界面中执行 `Spark` 应用程序和 `Scala` 代码片段。通过 `spark-shell`，你可以使用 `Scala` 编程语言与 `Spark` 进行交互。它提供了一个 `REPL（Read-Eval-Print Loop）`环境，可以在命令行中编写和执行 `Spark` 代码，并即时查看结果。

### spark-sql

- 是一个交互式的命令行工具，用于在命令行界面中执行 `Spark SQL` 查询。通过 `spark-sql`，你可以使用 `SQL` 语句来查询和操作 `Spark` 中的数据。它提供了一个交互式的环境，类似于传统的 `SQL` 命令行工具，但在 Spark 中支持更强大的分布式计算能力。

### Spark Beeline

- `Spark Beeline` 是 `Spark` 提供的一个用于连接和交互式查询 `Hive` 的命令行工具。它基于 `Apache Hive` 的 `Beeline` 客户端，并与 `Spark` 集成，允许用户通过命令行界面连接到 `Spark SQL` 和 `Hive`，并执行 `SQL` 查询。

## DataFrame

- 在`Spark`中，`DataFrame`是一种以`RDD`为基础的分布式数据集，类似于传统数据库中的二维表格。
- `DataFrame`与`RDD`主要区别
  - 前者带有`schema`元信息，即`DataFrame`所表示的二维表数据集的每一列都带有名称和类型。
  - 从`API`易用性的角度上看，`DataFrame API`提供的是一套高层的关系操作，比函数式的`RDD API`要更加友好，门槛更低

## DataSet

- `DataSet`是分布式数据集合。是`DataFrame`的一个扩展。提供了`RDD`的优势以及`Spark SQL`优化执行的优点
- `DataSet`是`DataFrame API`的一个扩展，是`Spark SQL`最新的数据抽象
- 用户友好的`API`风格，即具有类型安全检查也具有`DataFrame`的查询优化特性
- 用样例类来对`DataSet`中定义数据的结构信息，样例类中每个属性的名称直接映射到`DataSet`中的字段名称
- `DataSet`是强类型的。
- `DataFrame`转换为`DataSet`。`Row`是一个类型，跟`Car`、`Person`这些的类型一样，所有的表结构信息都用`Row`来表示

## `SparkSQL`核心编程

### 创建 `DataFrame`

1. 从`Spark`数据源进行创建

   ~~~scala
   val df = spark.read.json("data/user.json")
   ~~~

2. 从`RDD`进行转换

3. 从`Hive Table`进行查询返回

### `SQL` 语法

- `SQL`语法风格是指查询数据的时候使用`SQL`语句来查询，这种风格的查询必须要有临时视图或者全局视图来辅助

1. 读取`JSON`文件创建`DataFrame`

2. 对`DataFrame`创建一个临时表

   ~~~sql
   df.createOrReplaceTempView("people")
   ~~~

3. 通过`SQL`语句实现查询全表

   ~~~scala
   val sqlDF = spark.sql("SELECT * FROM people")
   ~~~

4. 结果展示

   ~~~scala
   salDF.show
   ~~~

   - 普通临时表是`Session`范围内的，如果想应用范围内有效，可以使用全局临时表

5. 对于`DataFrame`创建一个全局表

   ~~~
   df.createGlobalTempView("people")
   ~~~

6. 通过`SQL`语句实现查询全表

   ~~~
   spark.sql("select * from global_temp.people").show
   ~~~

### `DSL`语法

- `DataFrame`提供一个特定领域语言(`domain-specific language,DSL`)去管理结构化的数据。

1. 创建一个`DataFrame`

2. 查看`DataFrane`的`Schema`信息

   ~~~
   df.printSchema
   ~~~

3. 只查看`userName`列数据

   ~~~
   df.select("username").show
   ~~~

4. 查看`username`列数据以及`age+1`数据

   - 注意：涉及到运算的时候，每列都必须使用\$,或者采用引号表达式：单引号+字段名

   ~~~
   df.select($"username",$"age"+1).show
   df.select('username,'age+1).show
   df.select('username,'age+1 as "newage").show
   ~~~

5. 查看"age"大于"30"的数据

   ~~~
   df.filter($"age">30).show
   ~~~

6. 按照"age"分组，查看数据条数

   ~~~
   df.groupBy("age").count.show
   ~~~

### `RDD` 转换为 `DataFrame`

- 在`IDEA`中开发程序，如果需要`RDD`与`DF`或者`DS`之间互相操作，那么需要引入`import spark.implicits._`

- `spark-shell`中无需导入，自动完成此操作

  ~~~
  val idRDD = sc.textFile("data/id.txt")
  idRDD.toDF("id").show
  ~~~

- 实际开发中，一般通过样例类将`RDD`转换为`DataFrame`

  ~~~
  case class User(name:String,age:Int)
  sc.makeRDD((List(("zhangshan",30),("lisi",40))).map(t=>User(t._1,t._2)).toDF.show
  ~~~

### `DataFrame` 转换为 `RDD`

- `DataFrame`是对`RDD`的封装，可以直接获取内部的`RDD`

  ~~~
  case class User(name:String,age:Int)
  val df = sc.makeRDD((List(("zhangshan",30),("lisi",40))).map(t=>User(t._1,t._2)).toDF
  
  val rdd = df.rdd
  
  val array = rdd.collect
  ~~~

  - 注意：此时得到的`RDD`存储类型为`Row`

### `DataSet`

- `DataSet`是具有强类型的数据集合

#### 创建`DataSet`

1. 使用样例类创建`DataSet`

   ~~~
   case class Person(name:string,age:Long)
   
   val caseClassDS = Seq(Person("zhangsan",2)).toDS()
   
   caseClassDS.show
   ~~~

2. 使用基本类型的序列创建`DataSet`

   ~~~
   val ds = Seq(1,2,3,4,5).toDS
   
   ds.show
   ~~~

#### `RDD`转换为`DataSet`

- `SparkSQL`能够自动将包含有`case`类的`RDD`转换成`DataSet`,`case`类定义了`table`的结构，`case`类属性通过反射变成了表的列名

~~~
case class User(name:String,age:Int)

sc.makrRDD(List(("zhagnsan",30),("lisi",49))).map(t=>User(t._1,t._2)).toDS
~~~

#### `DataSet`转换为`RDD`

- `DataSet`是对`RDD`的封装，可以直接获取内部的`RDD`

  ~~~
  case class User(name:String,age:Int)
  
  sc.makeRDD(List(("zhangsan",30),("lisi",49))).map(t=>User(t._1,t._2)).toDS
  
  val rdd = resll.rdd
  
  rdd.collect
  ~~~

#### `DataFrame`和`DataSet`转换

- `DataFrame`是`DataSet`的特列，它们之间是可以互相转换的

  - `DataFrame`转换为`DataSet`

    ~~~
    case class User(name:String,age:Int)
    
    val df = sc.makeRDD(List(("zhangsan",30),("lisi",49)).toDF("name","age")
    
    val ds = df.as[User]
    ~~~

  - `DataSet`转换为`DataFrame`

    ~~~
    val ds = df.as[User]
    
    val df = ds.toDF
    ~~~

## RDD、DataFrame、DataSet三者的关系

### 三者的共性

- `RDD`、`DataFrame`、`DataSet`都是`spark`平台下的分布式弹性数据集，为处理超大型数据提供便利
- 三者都有惰性机制，在进行创建、转换，如`map`方法时，不会立即执行，只有在遇到`Action`如`foreach`时，三者才会开始遍历运算
- 三者有许多共同的函数
- 在对`DataFrame`和`Dataset`进行操作都需要这个包：`import spark.implicits._`（在创建好`SparkSession`对象后尽量直接导入）
- 三者都会根据`Spark`的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出
- 三者都有`partition`的概念
- `DataFrame`和`DataSet`均可使用模式匹配获取各个字段的值和类型

### 三者的区别

1. `RDD`
   - `RDD`一般和`spark mllib`同时使用
   - `RDD`不支持`sparksql`操作
2. `DataFrame`
   - 与`RDD`和`DataSet`不同,`DataFrame`每一行的类型固定为`Row`，每一列的值没法直接访问，只有通过解析才能获取各个字段的值
   - `DataFrame`与`DataSet`一般不与`spark mllib`同时使用
   - `DataFrame`与`DataSet`均支持`SparkSql`的操作，比如`select`,`groupby`之类，还能注册临时表/视窗，进行`sql`语句操作
   - `DataFrame`与`DataSet`支持一些特别方便的保存方式，比如保存成csv,可以带上表头，这样每一列的字段名一目了然
3. `DataSet`
   - `DataSet`和`DataFrame`拥有完全相同的成员函数，区别只是每一行的数据类型不同。
   - `DataFrame`也可以叫`DataSet[Row]`，每一行的类型是`Row`

## `IDEA`开发`SparkSQL`

### 添加依赖

~~~
<dependency>
	<groupId>org.apache.spark</groupId>
	<artifactId>spark-sql_2.12</artifactId>
	<version>3.0.0</version>
</dependency>
~~~

### 用户自定义函数

- 用户可以通过`spark.udf`功能添加自定义函数，实现自定义功能

#### UDF

~~~scala
def main(args: Array[String]): Unit = {
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("sparkSQL")
    val spark: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
    import spark.implicits._

    val df: DataFrame = spark.read.json("datas/user.json")
    df.createOrReplaceTempView("user")

    spark.udf.register("concat", (arg1: String, name: String) => {
      arg1 + ":" + name
    })

    spark.sql("select age,concat(\"姓名\",username) from user").show

    spark.stop()
  }
~~~

#### UDAF

- 从`Spark3.0`版本后，`UserDefinedAggregateFunction` 已经不推荐使用了。可以统一采用强类型聚合函数
  `Aggregator`

~~~scala
object Spark02_SparkSQL_UDAF {

  def main(args: Array[String]): Unit = {
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("sparkSQL")
    val spark: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()

    val df: DataFrame = spark.read.json("datas/user.json")
    df.createOrReplaceTempView("user")

    spark.udf.register("ageAvg",new MyAvgUDAF())


    spark.sql("select ageAvg(age) from user").show

    spark.stop()
  }

  class MyAvgUDAF extends UserDefinedAggregateFunction {

    //输入数据结构
    override def inputSchema: StructType = {
      StructType(
        Array(StructField("age", LongType))
      )
    }

    //缓冲区数据结构
    override def bufferSchema: StructType = {
      StructType(
        Array(
          StructField("total", LongType),
          StructField("count", LongType)
        )
      )
    }

    //输出数据结构
    override def dataType: DataType = LongType

    //是否稳定
    override def deterministic: Boolean = true

    //初始化
    override def initialize(buffer: MutableAggregationBuffer): Unit = {
      //      buffer(0) = 0L
      //      buffer(1) = 0L

      buffer.update(0, 0L)
      buffer.update(1, 0L)

    }

    //根据输入值更新缓冲区数据
    override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
      buffer.update(0, buffer.getLong(0) + input.getLong(0))
      buffer.update(1, buffer.getLong(1) + 1)

    }

    override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
      buffer1.update(0, buffer1.getLong(0) + buffer2.getLong(0))
      buffer1.update(1, buffer1.getLong(1) + buffer2.getLong(1))
    }

    override def evaluate(buffer: Row): Any = {
      buffer.getLong(0) / buffer.getLong(1)
    }
  }
}
~~~

~~~scala
object Spark02_SparkSQL_UDAF1 {

  def main(args: Array[String]): Unit = {
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("sparkSQL")
    val spark: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
    import spark.implicits._
    val df: DataFrame = spark.read.json("datas/user.json")
    df.createOrReplaceTempView("user")
    val ds: Dataset[User] = df.as[User]
    spark.udf.register("ageAvg", functions.udaf(new MyAvgUDAF()))

    ds.select(expr("ageAvg(age)").as("平均值")).show
    spark.sql("select ageAvg(age) from user").show

    spark.stop()
  }

  case class Buff(var total: Long, var count: Long)
  case class User(username: String, age: Long)
  class MyAvgUDAF extends Aggregator[Long, Buff, Long] {
    //初始值
    override def zero: Buff = {
      Buff(0L, 0L)
    }

    //计算
    override def reduce(b: Buff, a: Long): Buff = {
      b.total = b.total + a
      b.count = b.count + 1
      b
    }

    //缓冲区合并计算
    override def merge(b1: Buff, b2: Buff): Buff = {
      b1.total = b1.total + b2.total
      b1.count = b1.count + b2.count
      b1
    }

    //计算结果
    override def finish(reduction: Buff): Long = {
      reduction.total / reduction.count
    }

    //缓冲区的编码操作
    override def bufferEncoder: Encoder[Buff] = Encoders.product

    //输出的编码操作
    override def outputEncoder: Encoder[Long] = Encoders.scalaLong
  }
}
~~~

~~~scala
object Spark02_SparkSQL_UDAF2 {

  def main(args: Array[String]): Unit = {
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("sparkSQL")
    val spark: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
    import spark.implicits._

    val df: DataFrame = spark.read.json("datas/user.json")
    val ds: Dataset[User] = df.as[User]

    val udafCol: TypedColumn[User, Long] = new MyAvgUDAF().toColumn


    ds.select('username as ("姓名"), 'age).show
    ds.select(udafCol).show

    spark.stop()
  }

  case class User(username: String, age: Long)

  case class Buff(var total: Long, var count: Long)

  class MyAvgUDAF extends Aggregator[User, Buff, Long] {
    //初始值
    override def zero: Buff = {
      Buff(0L, 0L)
    }

    //计算
    override def reduce(b: Buff, a: User): Buff = {
      b.total = b.total + a.age
      b.count = b.count + 1
      b
    }

    //缓冲区合并计算
    override def merge(b1: Buff, b2: Buff): Buff = {
      b1.total = b1.total + b2.total
      b1.count = b1.count + b2.count
      b1
    }

    //计算结果
    override def finish(reduction: Buff): Long = {
      reduction.total / reduction.count
    }

    //缓冲区的编码操作
    override def bufferEncoder: Encoder[Buff] = Encoders.product

    //输出的编码操作
    override def outputEncoder: Encoder[Long] = Encoders.scalaLong
  }
}
~~~

## 数据的加载和保存

### 通用的加载和保存方式

- `SparkSQL`默认读取和保存的文件格式为`parquet`

### 加载数据

- `spark.read.load`是加载数据的通用方法

  ~~~sql
  spark.read.format("…")[.option("…")].load("…")
  ~~~

  - format("…")：指定加载的数据类型，包括"csv"、"jdbc"、"json"、"orc"、"parquet" 和 "textFile"。

  - load("…")：在"csv"、"jdbc"、"json"、"orc"、"parquet"和"textFile"格式下需要传入加载数据的路径。

  - option("…")：在"jdbc"格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable 我们前面都是使用read API  先把文件加载到 DataFrame 然后再查询，其实，我们也可以直接在文件上进行查询:    文件格式.`文件路径`

    ~~~
    spark.sql("select * from json.`/opt/module/data/user.").show
    ~~~

### 保存数据

- `df.write.save` 是保存数据的通用方法

  ~~~
  df.write.format("…")[.option("…")].save("…")
  ~~~

- format("…")：指定保存的数据类型，包括"csv"、"jdbc"、"json"、"orc"、"parquet"和 "textFile"。

- save ("…")：在"csv"、"orc"、"parquet"和"textFile"格式下需要传入保存数据的路径。

- option("…")：在"jdbc"格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable

  - 保存操作可以使用 SaveMode,  用来指明如何处理数据，使用 mode()方法来设置。

  - `SaveMode` 是一个枚举类，其中的常量包括：

    | Scala/Java                      | Any Language     | Meaning                    |
    | ------------------------------- | ---------------- | -------------------------- |
    | SaveMode.ErrorIfExists(default) | "error"(default) | 如果文件已经存在则抛出异常 |
    | SaveMode.Append                 | "append"         | 如果文件已经存在则追加     |
    | SaveMode.Overwrite              | "overwrite"      | 如果文件已经存在则覆盖     |
    | SaveMode.Ignore                 | "ignore"         | 如果文件已经存在则忽略     |

    ~~~
    df.write.mode(".json("/opt/module/data/output")
    ~~~

## Parquet

- Spark SQL 的默认数据源为 Parquet 格式。Parquet 是一种能够有效存储嵌套数据的列式存储格式。

- 数据源为 Parquet 文件时，Spark SQL 可以方便的执行所有的操作，不需要使用 format。修改配置项spark.sql.sources.default，可修改默认数据源格式。

  1. 加载数据

     ~~~
     val df = spark.read.load("examples/src/main/resources/users.parquet")
     df.show
     ~~~

  2. 保存数据

     ~~~
     var df = spark.read.json("/opt/module/data/input/people.json")
     
     df.write.mode("append").save("/opt/module/data/output")
     ~~~

## JSON

- Spark SQL  能够自动推测 JSON 数据集的结构，并将它加载为一个Dataset[Row].  可以通过 SparkSession.read.json()去加载 JSON  文件。

  - 注意：Spark 读取的 JSON 文件不是传统的JSON 文件，每一行都应该是一个 JSON 串。

    ~~~
    import spark.implicits._
    
    val path = "/opt/module/spark local/people.json"
    val peopleDF = spark.read.json(path)
    ~~~

## CSV

- Spark SQL可以配置CSV文件的列表信息，读取CSV文件,CSV文件的第一行设置为数据列

  ~~~
  spark.read.format("csv").option("sep", ";").option("inferSchema","true").option("header", "true").load("data/user.csv")
  ~~~

## MySQL

- Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。

- 如果使用spark-shell操作，可在启动shell时指定相关的数据库驱动路径或者将相关的数据库驱动放到spark的类路径下。

  ~~~
  bin/spark-shell --jars mysql-connector-java-5.1.27-bin.jar
  ~~~

  1. 导入依赖

  ~~~
  <dependency>
  <groupId>mysql</groupId>
  <artifactId>mysql-connector-java</artifactId>
  <version>5.1.27</version>
  </dependency>
  ~~~

  2. 读取数据

  ~~~
  object Spark04_SparkSQL_JDBC {
  
    def main(args: Array[String]): Unit = {
      val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("sparkSQL")
      val spark: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
      import spark.implicits._
  
      val df: DataFrame = spark.read
        .format("jdbc")
        .option("url", "jdbc:mysql://192.168.10.100:3306/sry")
        .option("driver", "com.mysql.jdbc.Driver")
        .option("user", "root")
        .option("password", "000000")
        .option("dbtable", "NewTable")
        .load()
      df.show
      df.write
        .format("jdbc")
        .option("url", "jdbc:mysql://192.168.10.100:3306/sry")
        .option("driver", "com.mysql.jdbc.Driver")
        .option("user", "root")
        .option("password", "000000")
        .option("dbtable", "NewTable1")
        .mode(SaveMode.Append)
        .save()
  
  
      spark.stop()
    }
  
  }
  ~~~

## Hive

- Apache Hive 是 Hadoop 上的 SQL 引擎，Spark SQL编译时可以包含 Hive 支持，也可以不包含。
- 若要把 Spark SQL  连接到一个部署好的 Hive  上，你必须把 hive-site.xml  复制到Spark 的配置文件目录中($SPARK_HOME/conf)。即使没有部署好 Hive，Spark SQL  也可以运行。

### 外部的HIVE

- 如果想连接外部已经部署好的`Hive`，需要通过以下几个步骤：
  - Spark 要接管 Hive 需要把hive-site.xml 拷贝到conf/目录下
  - 把 Mysql 的驱动 copy 到 jars/目录下
  - 如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下
  - 重启 spark-shell

### 运行Spark SQL CLI

- Spark SQL CLI可以很方便的在本地运行Hive元数据服务以及从命令行执行查询任务。

### 运行Spark beeline

- Spark Thrift Server 是Spark 社区基于HiveServer2 实现的一个Thrift 服务。旨在无缝兼容HiveServer2。

- 因为 Spark Thrift Server 的接口和协议都和HiveServer2 完全一致，因此我们部署好 Spark Thrift Server 后，可以直接使用hive 的 beeline 访问Spark Thrift Server 执行相关语句

- 如果想连接Thrift Server，需要通过以下几个步骤：

  - Spark 要接管 Hive 需要把hive-site.xml 拷贝到conf/目录下
  - 把 Mysql 的驱动 copy 到 jars/目录下
  - 如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下
  - 启动Thrift Server

  ~~~
  sbin/start-thriftserver.sh
  bin/beeline -u jdbc:hive2://linux1:10000 -n root
  ~~~

### 代码操作Hive

1. 导入依赖

   ~~~xml
   <dependency>
               <groupId>org.apache.spark</groupId>
               <artifactId>spark-hive_2.12</artifactId>
               <version>3.0.0</version>
           </dependency>
   
           <dependency>
               <groupId>org.apache.hive</groupId>
               <artifactId>hive-exec</artifactId>
               <version>1.2.1</version>
           </dependency>
           <dependency>
               <groupId>mysql</groupId>
               <artifactId>mysql-connector-java</artifactId>
               <version>5.1.27</version>
           </dependency>
   ~~~

2. 将hive-site.xml 文件拷贝到项目的 resources 目录中，代码实现

   ~~~scala
   object Spark05_SparkSQL_HIVE {
   
     def main(args: Array[String]): Unit = {
       val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("sparkSQL")
       val spark: SparkSession = SparkSession.builder().enableHiveSupport().config(sparkConf).getOrCreate()
       import spark.implicits._
       //使用SparkSQL连接外置的HIVE
       //1.拷贝Hive-site.xml文件到classpath
       //2.启用Hive的支持
       //3增加对应的依赖关系
       spark.sql("use default")
       spark.sql("show tables").show
   
   
       spark.stop()
     }
   
   }
   ~~~

3. 注意：在开发工具中创建数据库默认是在本地仓库，通过参数修改数据库仓库的地址:

   ~~~
   val sparkConf: SparkConf = new SparkConf()
     .setMaster("local[*]")
     .setAppName("sparkSQL")
     .set("spark.sql.warehouse.dir", "hdfs://linux1:8020/user/hive/warehouse")
   
   val spark: SparkSession = SparkSession.builder().config(sparkConf).getOrCreate()
   ~~~

4. 如果在执行操作时，出现如下错误：

   ~~~
   org.apache.hadoop.security.AccessControlException Permission denied: user=14481, access=WRITE, inode="/user/hive/warehouse/atguigu.db":atguigu:supergroup:drwxr-xr-x
   ~~~

   ~~~
    System.setProperty("HADOOP_USER_NAME", "atguigu")
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("sparkSQL")
    val spark: SparkSession = SparkSession.builder().enableHiveSupport().config(sparkConf).getOrCreate()
   ~~~

   















