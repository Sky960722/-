# RDD

## 概念

- `RDD(Resilient Distributed Dataset)` 叫做弹性分布式数据集，是`Spark`中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合
  - 弹性
    - 存储的弹性：内存与磁盘的自动切换
    - 容错的弹性：数据丢失可以自动恢复
    - 计算的弹性：计算出错重试机制
    - 分片的弹性：可根据需要重新分片
  - 分布式：数据存储在大数据集群不同节点上
  - 数据集：`RDD`封装了计算逻辑，并不保存数据
  - 数据抽象：`RDD`是一个抽象类，需要子类具体实现
  - 不可变：`RDD`封装了计算逻辑，是不可以改变的，想要改变，只能产生新的`RDD`,在新的`RDD`里面封装计算逻辑
  - 可分区、并行计算

## 核心属性

- 分区列表：`RDD`数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性
- 分区计算函数：`Spark`在计算时，是使用分区函数对每一个分区进行计算
- `RDD`之间的依赖关系：`RDD`是计算模型的封装，当需求中需要将多个计算模型进行组合时，就需要将多个`RDD`建立依赖关系
- 分区器(可选)：当数据为`KV`类型数据时，可以通过设定分区器自定义数据的分区
- 首选位置(可选)：计算数据时，可以根据计算节点的状态选择不同的节点位置进行计算

## 执行原理

- 从计算的角度来讲，数据处理过程中需要计算资源(内存&CPU)和计算模型(逻辑)。执行时，需要将计算资源和计算模型进行协调和整合
- `Spark`框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务分发到已经分配资源的计算节点上，按照指定的计算模型进行数据计算。最后得到计算结果

## `RDD`创建

- 在`Spark`中创建`RDD`的方式分为四种：

  1. 从集合(内存)中创建`RDD`,`Spark`主要提供了两个方法:`parallelize`和`makeRDD`

     ~~~scala
     val sparkConf =
     new SparkConf().setMaster("local[*]").setAppName("spark") val sparkContext = new 
     SparkContext(sparkConf)
     val rdd1 = sparkContext.parallelize( List(1,2,3,4)
     )
     val rdd2 = sparkContext.makeRDD( List(1,2,3,4)
     )
     rdd1.collect().foreach(println) rdd2.collect().foreach(println)
     sparkContext.stop()
     ~~~

     - 从底层代码实现来讲,`makeRDD`方法就是`parallelize`方法

     ~~~scala
     def makeRDD[T: ClassTag](
     seq: Seq[T],
     numSlices: Int = defaultParallelism): RDD[T] = withScope { parallelize(seq, numSlices)
     }
     ~~~

  2. 从外部存储(文件)创建`RDD`

     - 由外部存储系统的数据集创建`RDD`

     ~~~scala
     val sparkConf =
     new SparkConf().setMaster("local[*]").setAppName("spark") val sparkContext = new 
     SparkContext(sparkConf)
     val fileRDD: RDD[String] = sparkContext.textFile("input") fileRDD.collect().foreach(println)
     sparkContext.stop()
     ~~~

  3. 从其他`RDD`创建

     - 主要通过一个`RDD`运算完后，再产生新的`RDD`

  4. 直接创建`RDD`

     - 使用`new`的方式直接构造`RDD`，一般由`Spark`框架自身使用

## `RDD`并行度与分区

- 默认情况下，`Spark`可以将一个作业切分多个任务后，发送给`Executor`节点并行计算，而能够并行计算的任务数量称之为并行度

~~~
val sparkConf =
new SparkConf().setMaster("local[*]").setAppName("spark") val sparkContext = new 
SparkContext(sparkConf)
val dataRDD: RDD[Int] = sparkContext.makeRDD(
List(1,2,3,4), 4)
val fileRDD: RDD[String] = sparkContext.textFile(
"input", 2)
fileRDD.collect().foreach(println)
sparkContext.stop()
~~~

- 读取内存数据时，数据可以按照并行度的设定进行数据的分区操作，数据分区规则的`Spark`核心源码

~~~
def positions(length: Long, numSlices: Int): Iterator[(Int, Int)] = {
(0 until numSlices).iterator.map { i =>
val start = ((i * length) / numSlices).toInt
val end = (((i + 1) * length) / numSlices).toInt (start, end)
}
}
~~~

- 读取文件数据时，数据是按照Hadoop文件读取的规则进行切片分区，而切片规则和数据读取的规则有些差异

## `RDD` 转换算子

- `RDD`根据数据处理方式的不同将算子整体上分为`Value`类型、双`Value`类型和`Key-Value`类型

### `Value`类型

#### `map`

  - 函数签名

    ~~~scala
    def map[U: ClassTag](f: T => U): RDD[U]
    ~~~

  - 函数说明

    - 将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换。

#### `mapPartitions`

  - 函数签名

    ~~~
    def mapPartitions[U: ClassTag]( f: Iterator[T] => Iterator[U],
    preservesPartitioning: Boolean = false): RDD[U]
    ~~~

  - 函数说明

    - 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据。

  - `map` 和 `mapPartitions` 的区别

    - `Map` 算子是分区内一个数据一个数据的执行，类似于串行操作。而 `mapPartitions` 算子是以分区为单位进行批处理操作。

  - 功能的角度

    - `Map`算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据。
    - `MapPartitions`算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变，所以可以增加或减少数据

  - 性能的角度

    - `Map`算子因为类似于串行操作，所以性能比较低，而是`mapPartitions`算子类似于批处理，所以性能较高。
    - 但是`mapPartitions`算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。

#### `mapPartitionsWithIndex`

- 函数签名

  ~~~
  def mapPartitionsWithIndex[U: ClassTag](
  f: (Int, Iterator[T]) => Iterator[U],
  preservesPartitioning: Boolean = false): RDD[U]
  ~~~

- 函数说明

  - 将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据，在处理时同时可以获取当前分区索引。

#### `flatMap`

- 函数签名

  ~~~
  def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U]
  ~~~

- 函数说明

  - 将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射

#### `glom`

- 函数签名

  ~~~
  def glom(): RDD[Array[T]]
  ~~~

- 函数说明

  - 将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变

#### `groupBy`

- 函数签名

  ~~~
  def groupBy[K](f: T => K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])]
  ~~~

- 函数说明

- 将数据根据指定的规则进行分组,  分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为shuffle。极限情况下，数据可能被分在同一个分区中一个组的数据在一个分区中，但是并不是说一个分区中只有一个组

#### `filter`

- 函数签名

  ~~~
  def filter(f: T => Boolean): RDD[T]
  ~~~

- 函数说明

  - 将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。当数据进行筛选过滤后，分区不变，但是分区内的数据可能不均衡，生产环境下，可能会出现数据倾斜。

#### `sample`

- 函数签名

  ~~~
  def sample(
  withReplacement: Boolean,
  fraction: Double,
  seed: Long = Utils.random.nextLong): RDD[T]
  ~~~

- 函数说明

  - 根据指定的规则从数据集中抽取数据

#### `distinct`

- 函数签名

  ~~~
  def distinct()(implicit ord: Ordering[T] = null): RDD[T]
  def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]
  ~~~

- 函数说明

  - 将数据集中重复的数据去重

#### `coalesce`

- 函数签名

  ~~~
  def coalesce(numPartitions: Int, shuffle: Boolean = false,
  partitionCoalescer: Option[PartitionCoalescer] = Option.empty)
  (implicit ord: Ordering[T] = null)
  : RDD[T]
  ~~~

- 函数说明

  - 根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率当 spark 程序中，存在过多的小任务的时候，可以通过 coalesce 方法，收缩合并分区，减少分区的个数，减小任务调度成本

#### `repartition`

- 函数签名

  ~~~
  def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]
  ~~~

- 函数说明

  - 该操作内部其实执行的是 coalesce 操作，参数 shuffle 的默认值为 true。无论是将分区数多的RDD 转换为分区数少的RDD，还是将分区数少的 RDD 转换为分区数多的RDD，repartition操作都可以完成，因为无论如何都会经 shuffle 过程。

#### `sortBy`

- 函数签名

  ~~~
  def sortBy[K](
  f: (T) => K,
  ascending: Boolean = true,
  numPartitions: Int = this.partitions.length)
  (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]
  ~~~

- 函数说明

  - 该操作用于排序数据。在排序之前，可以将数据通过 `f` 函数进行处理，之后按照 `f` 函数处理的结果进行排序，默认为升序排列。排序后新产生的 `RDD` 的分区数与原``RDD` 的分区数一致。中间存在 `shuffle` 的过程

### 双Value类型

#### `intersection`

- 函数签名

  ~~~
  def intersection(other: RDD[T]): RDD[T]
  ~~~

- 函数说明

  - 对源`RDD` 和参数`RDD` 求交集后返回一个新的`RDD`

#### `union`

- 函数签名

  ~~~
  def union(other: RDD[T]): RDD[T]
  ~~~

- 函数说明

  - 对源`RDD`和参数`RDD`求并集后返回一个新的`RDD`

#### `subtract`

- 函数签名

  ~~~
  def subtract(other: RDD[T]): RDD[T]
  ~~~

- 函数说明

  - 以一个 `RDD` 元素为主，去除两个 `RDD` 中重复元素，将其他元素保留下来。求差集

#### zip

- 函数签名

  ~~~
  def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)]
  ~~~

- 函数说明

  - 将两个 `RDD` 中的元素，以键值对的形式进行合并。其中，键值对中的`Key` 为第 1 个 `RDD`中的元素，`Value` 为第 2 个 `RDD` 中的相同位置的元素。

### `Key - Value`类型

#### `partitionBy`

- 函数签名

  ~~~
  def partitionBy(partitioner: Partitioner): RDD[(K, V)]
  ~~~

- 函数说明

  - 将数据按照指定`Partitioner` 重新进行分区。`Spark` 默认的分区器是`HashPartitioner`

#### `reduceByKey`

- 函数签名

  ~~~
  def reduceByKey(func: (V, V) => V): RDD[(K, V)]
  def reduceByKey(func: (V, V) => V, numPartitions: Int): RDD[(K, V)]
  ~~~

- 函数说明

  - 可以将数据按照相同的`Key` 对`Value` 进行聚合

#### `groupByKey`

- 函数签名

  ~~~
  def groupByKey(): RDD[(K, Iterable[V])]
  def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]
  def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]
  ~~~

- 函数说明

  - 将数据源的数据根据 `key` 对 `value` 进行分组

- `reduceByKey` 和 `groupByKey` 的区别
  - 从 `shuffle`的角度：`reduceByKey` 和 `groupByKey` 都存在 `shuffle` 的操作，但是`reduceByKey`可以在 `shuffle` 前对分区内相同 `key` 的数据进行预聚合（`combine`）功能，这样会减少落盘的数据量，而`groupByKey` 只是进行分组，不存在数据量减少的问题，`reduceByKey` 性能比较高。
  - 从功能的角度：`reduceByKey` 其实包含分组和聚合的功能。`GroupByKey` 只能分组，不能聚合，所以在分组聚合的场合下，推荐使用 `reduceByKey`，如果仅仅是分组而不需要聚合。那么还是只能使用`groupByKey`

#### `aggregateByKey`

- 函数签名

  ~~~
  def aggregateByKey[U: ClassTag](zeroValue: U)(seqOp: (U, V) => U,
  combOp: (U, U) => U): RDD[(K, U)]
  ~~~

- 函数说明

  - 将数据根据不同的规则进行分区内计算和分区间计算

#### `foldByKey`

- 函数签名

  ~~~
  def foldByKey(zeroValue: V)(func: (V, V) => V): RDD[(K, V)]
  ~~~

- 函数说明

  - 当分区内计算规则和分区间计算规则相同时，`aggregateByKey` 就可以简化为`foldByKey`

#### `combineByKey`

- 函数签名

  ~~~
  def combineByKey[C]( createCombiner: V => C, mergeValue: (C, V) => C,
  mergeCombiners: (C, C) => C): RDD[(K, C)]
  ~~~

- 函数说明

  - 允许对具有相同键的元素进行自定义的聚合操作。它接收三个函数参数：
    1. createCombiner：对于每个键的第一个值，创建一个累加器的初始值。
    2. mergeValue：对于每个键的后续值，将其合并到累加器中。
    3. mergeCombiners：在不同分区上的累加器之间进行合并
  - 这个操作常用于需要对键值对进行聚合计算的场景，比如求和、求平均等。它的结果是一个新的RDD，其中每个键都对应一个聚合后的结果值。

#### `sortByKey`

- 函数签名

  ~~~
  def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.length)
  : RDD[(K, V)]
  ~~~

- 函数说明

  - 在一个`(K,V)`的`RDD`上调用，`K`必须实现`Ordered`接口(特质)，返回一个按照key进行排序的

#### `join`

- 函数签名

  ~~~
  def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]
  ~~~

- 函数说明

  - 在类型为`(K,V)`和`(K,W)`的`RDD`上调用，返回一个相同`key`对应的所有元素连接在一起的`(K,(V,W))`的RDD

#### `leftOuterJoin`

- 函数签名

  ~~~
  def leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]
  ~~~

- 函数说明

  - 类似于`SQL`语句的左外连接

#### `cogroup`

- 函数签名

  ~~~
  def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))]
  ~~~

- 函数说明

  - 在类型为`(K,V)`和`(K,W)`的`RDD`上调用，返回一个`(K,(Iterable<V>,Iterable<W>))`类型的`RDD`

## `RDD`行动算子

### `reduce`

- 函数签名

  ~~~
  def reduce(f: (T, T) => T): T
  ~~~

- 函数说明

  - 聚集`RDD` 中的所有元素，先聚合分区内数据，再聚合分区间数据

### `collect`

- 函数签名

  ~~~
  def collect(): Array[T]
  ~~~

- 函数说明

  - 在驱动程序中，以数组Array 的形式返回数据集的所有元素

### `count`

- 函数签名

  ~~~
  def count(): Long
  ~~~

- 函数说明

  - 返回`RDD`中元素的个数

### `first`

- 函数签名

  ~~~
  def first(): T
  ~~~

- 函数说明

  - 返回`RDD` 中的第一个元素

### `take`

- 函数签名

  ~~~
  def take(num: Int): Array[T]
  ~~~

- 函数说明

  - 返回一个由`RDD` 的前 `n` 个元素组成的数组

### `takeOrdered`

- 函数签名

  ~~~
  def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T]
  ~~~

- 函数说明

  - 返回该 `RDD` 排序后的前 `n` 个元素组成的数组

### `aggregate`

- 函数签名

  ~~~
  def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U
  ~~~

- 函数说明

  - 分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合

### `fold`

- 函数签名

  ~~~
  def fold(zeroValue: T)(op: (T, T) => T): T
  ~~~

- 函数说明

  - 折叠操作，aggregate的简化版操作

### `countByKey`

- 函数签名

  ~~~
  def countByKey(): Map[K, Long]
  ~~~

- 函数说明

  - 统计每种 key 的个数

### `save`

- 函数签名

  ~~~
  def saveAsTextFile(path: String): Unit def saveAsObjectFile(path: String): Unit
  def saveAsSequenceFile(
  path: String,
  codec: Option[Class[_ <: CompressionCodec]] = None): Unit
  ~~~

- 函数说明

  - 将数据保存到不同格式的文件中

### `foreach`

- 函数签名

  ~~~
  def foreach(f: T => Unit): Unit = withScope {
  val cleanF = sc.clean(f)
  sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF))
  }
  ~~~

- 函数说明

  - 分布式遍历RDD 中的每一个元素，调用指定函数

## `RDD` 序列化

### 闭包检查

- 从计算的角度, 算子以外的代码都是在`Driver`端执行, 算子里面的代码都是在`Executor`端执行。那么在`scala`的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给`Executor`端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。

### 序列化方法和属性

- 从计算的角度, 算子以外的代码都是在`Driver`端执行, 算子里面的代码都是在`Executor`端执行

### `Kryo`序列化框架

- Java 的序列化能够序列化任何的类。但是比较重（字节多），序列化后，对象的提交也比较大。Spark 出于性能的考虑，Spark2.0 开始支持另外一种Kryo 序列化机制。Kryo 速度是 Serializable 的 10 倍。当 RDD 在 Shuffle 数据的时候，简单数据类型、数组和字符串类型已经在 Spark 内部使用 Kryo 来序列化。

## `RDD` 依赖关系

### `RDD` 血缘关系

- RDD 只支持粗粒度转换，即在大量记录上执行的单个操作。将创建 RDD 的一系列Lineage（血统）记录下来，以便恢复丢失的分区。RDD 的Lineage 会记录RDD 的元数据信息和转换行为，当该RDD 的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。

###  `RDD` 依赖关系
- 两个相邻RDD之间的关系

### `RDD` 窄依赖

- 窄依赖表示每一个父(上游)RDD的Partition最多被子（下游）RDD的一个Partition使用

### `RDD` 宽依赖

- 宽依赖表示同一个父（上游）RDD的Partition被多个子（下游）RDD的Partition依赖，会引起Shuffle

### `RDD` 阶段划分

- `DAG（Directed Acyclic Graph）`有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。

### `RDD` 任务划分

- `RDD`任务切分中间分为：`Application`、`Job`、`Stage`和`Task`
  - `Application`：初始化一个 `SparkContext` 即生成一个`Application`；
  - `Job`：一个`Action` 算子就会生成一个`Job`；
  - `Stage：Stage `等于宽依赖`(ShuffleDependency)`的个数加 1；
  - `Task`：一个 `Stage` 阶段中，最后一个`RDD` 的分区个数就是`Task` 的个数。

###  `RDD` 持久化

#### `RDD Cache`缓存

- `RDD`通过`Cache`或者`Persist`方法将前面的计算结果缓存，默认情况下会把数据以缓存在`JVM`的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的`action`算子时，该`RDD`将会被缓存在计算节点的内存中，并供后面重用。

  ~~~
  // cache 操作会增加血缘关系，不改变原有的血缘关系
  println(wordToOneRdd.toDebugString)
  
  // 数据缓存。
  wordToOneRdd.cache()
  
  // 可以更改存储级别
  //mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)
  ~~~

#### `RDD CheckPoint`检查点

- 检查点其实就是通过将RDD中间结果写入磁盘由于血缘依赖过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果 检查点之后有节点出现问题， 可以从检查点开始重做血缘 ，减少了开销。

  ~~~
  // 设置检查点路径
  sc.setCheckpointDir("./checkpoint1")
  
  // 创建一个 RDD，读取指定位置文件:hello atguigu atguigu val lineRdd: RDD[String] = sc.textFile("input/1.txt")
  
  // 业务逻辑
  val wordRdd: RDD[String] = lineRdd.flatMap(line => line.split(" "))
  val wordToOneRdd: RDD[(String, Long)] = wordRdd.map { word => {
  (word, System.currentTimeMillis())
  }
  }
  // 增加缓存,避免再重新跑一个 job 做 checkpoint
  wordToOneRdd.cache()
  // 数据检查点：针对 wordToOneRdd 做检查点计算
  wordToOneRdd.checkpoint()
  
  // 触发执行逻辑
  wordToOneRdd.collect().foreach(println)
  ~~~

- 缓存和检查点区别

  1. Cache 缓存只是将数据保存起来，不切断血缘依赖。Checkpoint 检查点切断血缘依赖。
  2. Cache 缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint 的数据通常存储在HDFS 等容错、高可用的文件系统，可靠性高。
  3. 建议对checkpoint()的RDD 使用Cache 缓存，这样 checkpoint 的 job 只需从 Cache 缓存中读取数据即可，否则需要再从头计算一次RDD。

### `RDD` 分区器

- Spark目前支持Hash分区和Range分区，和用户自定义分区。Hash分区为当前的默认分区。分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle后进入哪个分区，进而决定了Reduce的个数。
  - 只有Key-Value类型的RDD才有分区器，非Key-Value类型的RDD分区的值是None
  - 每个RDD的分区ID范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的。

#### Hash分区：

- 对于给定的key，计算其hashCode,并除以分区个数取余

#### Range分区

- 将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序

## `RDD` 文件读取与保存

- Spark的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。
  - 文件格式分为：text文件、csv文件、sequence文件以及Object文件；
  - 文件系统分为：本地文件系统、HDFS、HBASE以及数据库。
    - sequence文件
      - SequenceFile 文件是Hadoop 用来存储二进制形式的key-value 对而设计的一种平面文件(Flat File)。在 SparkContext 中，可以调用sequenceFilekeyClass, valueClass(path)。
    - object对象文件
      - 对象文件是将对象序列化后保存的文件，采用 Java 的序列化机制。可以通过objectFile[T:ClassTag](path)函数接收一个路径，读取对象文件，返回对应的 RDD，也可以通过调用 saveAsObjectFile()实现对对象文件的输出。因为是序列化所以要指定类型。

# 累加器

## 实现原理

- 累加器用来把Executor 端变量信息聚合到Driver 端。在Driver 程序中定义的变量，在Executor 端的每个Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，传回Driver 端进行 merge。

## 基础编程

#### 系统累加器

~~~
val rdd = sc.makeRDD(List(1,2,3,4,5))
// 声明累加器
var sum = sc.longAccumulator("sum"); rdd.foreach(
num => {
// 使用累加器
sum.add(num)

}
)
// 获取累加器的值
println("sum = " + sum.value)

~~~

#### 自定义累加器

~~~
// 自定义累加器
// 1. 继承 AccumulatorV2，并设定泛型
// 2. 重写累加器的抽象方法
class WordCountAccumulator extends AccumulatorV2[String, mutable.Map[String, Long]]{

var map : mutable.Map[String, Long] = mutable.Map()


// 累加器是否为初始状态
override def isZero: Boolean = { map.isEmpty

}

// 复制累加器
override def copy(): AccumulatorV2[String, mutable.Map[String, Long]] = { new WordCountAccumulator
}

// 重置累加器
override def reset(): Unit = { map.clear()

}

// 向累加器中增加数据 (In)
override def add(word: String): Unit = {
// 查询 map 中是否存在相同的单词
// 如果有相同的单词，那么单词的数量加 1
// 如果没有相同的单词，那么在 map 中增加这个单词
map(word) = map.getOrElse(word, 0L) + 1L
}

//

override def merge(other: AccumulatorV2[String, mutable.Map[String, Long]]): Unit = {




val map1 = map
val map2 = other.value
// 两个 Map 的合并
map = map1.foldLeft(map2)( ( innerMap, kv ) => {
innerMap(kv._1) = innerMap.getOrElse(kv._1, 0L) + kv._2 innerMap

}
)
}
// 返回累加器的结果 （Out）
override def value: mutable.Map[String, Long] = map
}


~~~

# 广播变量

## 实现原理

- 广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表。

## 基础编程

~~~
val rdd1 = sc.makeRDD(List( ("a",1), ("b", 2), ("c", 3), ("d", 4) ),4) 
val list = List( ("a",4), ("b", 5), ("c", 6), ("d", 7) ) 
// 声明广播变量
val broadcast: Broadcast[List[(String, Int)]] = sc.broadcast(list) 
val resultRDD: RDD[(String, (Int, Int))] = rdd1.map { 
  case (key, num) => { 
   var num2 = 0 
   // 使用广播变量
    for ((k, v) <- broadcast.value) { 
     if (k == key) { 
      num2 = v 
      }
     }
      (key, (num, num2)) 
    }
  }
~~~



















