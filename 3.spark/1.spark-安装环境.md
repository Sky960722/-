# Spark

## 概念

- `Spark` 是一种由 Scala 语言开发的快速、通用、可扩展的大数据分析引擎
- `Spark Core`中提供了Spark最基础与最核心的功能
- `Spark SQL`是`Spark`用来操作结构化数据的组件。通过`Spark SQL`，用户可以使用`SQL`或者`Apache Hive`版本的`SQL`方言(`HQL`)来查询数据
- `Spark Streaming`是`Spark`平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的`API`

## 运行环境

### `Local`模式

- `Local`模式，就是不需要其他任何节点资源就可以在本地执行`Spark`代码的环境

#### 安装步骤

- 将 `spark-3.0.0-bin-hadoop3.2.tgz`文件上传到`Linux`并解压缩，放置在指定位置

  ~~~
  tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module
  cd /opt/module
  mv spark-3.0.0-bin-hadoop3.2 spark-local
  ~~~

- 启动`Local`环境

  ~~~
  bin/spark-shell
  ~~~

- 启动成功后，可以输入网址进行Web UI监控页面访问

  ~~~
  http://虚拟机地址:4040
  ~~~

- 退出本地模式

  ~~~
  :quit
  ~~~

- 提交应用

  ~~~
  bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master local[2] \
  ./examples/jars/spark-examples_2.12-3.0.0.jar \
  10
  ~~~

  - --class表示要执行程序的主类，此处可以更换为咱们自己写的应用程序
  2) --master local[2] 部署模式，默认为本地模式，数字表示分配的虚拟CPU核数量
  3) spark-examples_2.12-3.0.0.jar 运行的应用类所在的jar包，实际使用时，可以设定为咱们自己打的jar包
  4) 数字10表示程序的入口参数，用于设定当前应用的任务数量

### `Standalone` 模式

- 只使用Spark自身节点运行的集群模式，也就是所谓的独立部署（Standalone）模式。

- 集群规划

  |       | Linux1            | Linux2 | Linux3 |
  | ----- | ----------------- | ------ | ------ |
  | Spark | Worker<br/>Master | Worker | Worker |

#### 安装步骤

- 解压缩文件

  ~~~
  tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module
  cd /opt/module
  mv spark-3.0.0-bin-hadoop3.2 spark-standalone
  ~~~

- 修改配置文件

  - 进入解压缩后路径的conf目录，修改slaves.template文件名为slaves

    ~~~
    mv slaves.template slaves
    ~~~

  - 修改slaves文件，添加work节点

    ~~~
    hadoop102 
    hadoop103
    hadoop104
    ~~~

  - 修改spark-env.sh.template文件名为spark-env.sh

    ~~~
    mv spark-env.sh.template spark-env.sh
    ~~~

  - 修改spark-env.sh文件，添加JAVA_HOME环境变量和集群对应的master节点

    ~~~
    export JAVA_HOME=/opt/module/jdk1.8.0_144
    SPARK_MASTER_HOST=hadoop102
    SPARK_MASTER_PORT=7077
    ~~~

  - 分发spark-standalone目录

    ~~~
    xsync spark-standalone
    ~~~

- 启动集群

  - 执行脚本命令

    ~~~
    sbin/start-all.sh
    ~~~

  - 查看Master资源监控Web UI界面:

    ~~~
    http://linux1:8080
    ~~~

  - 提交应用

    ~~~
    bin/spark-submit \
    --class org.apache.spark.examples.SparkPi \
    --master spark://linux1:7077 \
    ./examples/jars/spark-examples_2.12-3.0.0.jar \
    10
    ~~~

    - --class表示要执行程序的主类
    - --master spark://linux1:7077 独立部署模式，连接到Spark集群
    - spark-examples_2.12-3.0.0.jar 运行类所在的jar包
    - 数字10表示程序的入口参数，用于设定当前应用的任务数量

- 提交参数说明

  - 在提交应用中，一般会同时一些提交参数

    ~~~
    bin/spark-submit \
    --class <main-class>
    --master <master-url> \
    ... # other options
    <application-jar> \ [application-arguments]
    ~~~

    | 参数                     | 解释                                                         | 可选值举例                                |
    | ------------------------ | ------------------------------------------------------------ | ----------------------------------------- |
    | --class                  | `Spark`程序中包含主函数的类                                  |                                           |
    | --master                 | `Spark`程序运行的模式(环境)                                  | 模式：local[*]、spark://linux1:7077、Yarn |
    | --executor memory 1G     | 指定每个 executor 可用内存为 1G                              |                                           |
    | --total executor cores 2 | 指定所有executor使用的cpu核数为2个                           |                                           |
    | --executor cores         | 指定每个executor使用的cpu核数                                |                                           |
    | application jar          | 打包好的应用jar，包含依赖。这个URL在集群中全局可见。 比如hdfs:// 共享存储系统，如果是file:// path，那么所有的节点的path都包含同样的jar |                                           |
    | application arguments    | 传给 main() 方法的参数                                       |                                           |

- 配置历史服务

  - 修改spark-defaults.conf.template文件名为spark-defaults.conf

    ~~~
    mv spark-defaults.conf.template spark-defaults.conf
    ~~~

  - 修改 spark default.conf 文件，配置日志存储路径

    ~~~
    spark.eventLog.enabled        true
    spark.eventLog.dir            hdfs://hadoop102:8020/directory
    ##注意：需要启动hadoop集群，HDFS上的directory目录需要提前存在。
    sbin/start dfs.sh
    hadoop fs mkdir /directory
    ~~~

  - 修改 spark env sh 文件 , 添加日志配置

    ~~~
    export SPARK_HISTORY_OPTS="
    -Dspark.history.ui.port=18080
    -Dspark.history.fs.logDirectory=hdfs://hadoop102:8020/directory
    -Dspark.history.retainedApplications=30"
    ~~~

    - 参数1含义：WEB UI访问的端口号为18080
    -  参数2含义：指定历史服务器日志存储路径
    -  参数3含义：指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。

  - 分发配置文件

    ~~~
    xsync conf
    ~~~

  - 重新启动集群和历史服务

    ~~~
    sbin/start-all.sh
    sbin/start-history-server.sh
    ~~~

  - 重新执行任务

    ~~~
    bin/spark-submit \
    --class org.apache.spark.examples.SparkPi \
    --master spark://linux1:7077 \
    ./examples/jars/spark-examples_2.12-3.0.0.jar \
    10
    ~~~

- 配置高可用(HA)

  - 高可用是因为当前集群中的Master节点只有一个，所以会存在单点故障问题。所以为了解决单点故障问题，需要在集群中配置多个Master节点，一旦处于活动状态的Master发生故障时，由备用Master提供服务，保证作业可以继续执行。这里的高可用一般采用Zookeeper设置

  - 集群规划

    |       | Linux1                          | Linux2                          | Linux3               |
    | ----- | ------------------------------- | ------------------------------- | -------------------- |
    | Spark | Master<br/>Zookeeper<br/>Worker | Master<br/>Zookeeper<br/>Worker | Zookeeper<br/>Worker |

  - 停止集群

    ~~~
    sbin/stop all.sh
    ~~~

  - 启动Zookeeper

    ~~~
    zk.sh start
    ~~~

  - 修改`spark-env.sh`文件添加如下配置

    ~~~
    注释如下内容：
    SPARK_MASTER_HOST linux1
    SPARK_MASTER_PORT=7077
    添加如下内容
    #Master 监控页面默认访问端口为 8080 ，但是可能会和 Zookeeper 冲突，所以改成 8989 ，也可以自
    定义，访问 UI 监控页面时请注意
    SPARK_MASTER_WEBUI_PORT= 8989
    export SPARK_DAEMON_JAVA_OPTS="
    -Dspark.deploy.recoveryMode=ZOOKEEPER
    -Dspark.deploy.zookeeper.url=hadoop102,hadoop103,hadoop104
    -Dspark.deploy.zookeeper.dir=/spark"
    ~~~

  - 分发配置文件

    ~~~
    xsync conf/
    ~~~

  - 启动集群

    ~~~
    sbin/start all.sh
    ~~~

  - 启动linux2的单独Master节点，此时linux2节点Master状态处于备用状态

    ~~~
    sbin/start master.sh
    ~~~

  - 提交应用到高可用集群

    ~~~
    bin/spark-submit \
    --class org.apache.spark.examples.SparkPi \
    --master spark://linux1:7077,linux2:7077 \
    ./examples/jars/spark-examples_2.12-3.0.0.jar \
    10
    ~~~

### `Yarn` 模式

#### 安装步骤

  - 解压缩文件

    ~~~
    tar zxvf spark 3.0.0 bin hadoop3.2.tgz C /opt/module
    cd /opt/module
    mv spark 3.0.0 bin hadoop3.2 spark yarn
    ~~~

  - 修改配置文件

    - 修改hadoop配置文件/opt/module/hadoop/etc/hadoop/yarn-site.xml, 并分发

    ~~~
    <!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认
    是 true -->
    <property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>false</value>
    </property>
    
    
    <!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是 true -->
    <property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value> roperty>
    
    ~~~

  - 修改 conf/spark-env.sh，添加 JAVA_HOME 和YARN_CONF_DIR 配置

    ~~~
    mv spark-env.sh.template spark-env.sh
    。。。
    export JAVA_HOME=/opt/module/jdk1.8.0_144 
    YARN_CONF_DIR=/opt/module/hadoop/etc/hadoop
    ~~~

  - 启动 HDFS 以及 YARN 集群

  - 提交应用

    ~~~
    bin/spark-submit \
    --class org.apache.spark.examples.SparkPi \
    --master yarn \
    --deploy-mode cluster \
    ./examples/jars/spark-examples_2.12-3.0.0.jar \
    10
    ~~~

- 配置历史服务器

  - 修改spark-defaults.conf.template文件名为spark-defaults.conf

    ~~~
    mv spark-defaults.conf.template spark-defaults.conf
    ~~~

  - 修改 spark-default.conf 文件，配置日志存储路径

    ~~~
    spark.eventLog.enabled        true
    spark.eventLog.dir            hdfs://hadoop102:8020/directory
    ##注意：需要启动hadoop集群，HDFS上的目录需要提前存在。
    sbin/start -dfs.sh
    hadoop fs -mkdir /directory
    ~~~

  - 修改 `spark-env.sh` 文件 , 添加日志配置

    ~~~
    export SPARK_HISTORY_OPTS="
    -Dspark.history.ui.port=18080
    -Dspark.history.fs.logDirectory=hdfs://linux1:8020/directory
    -Dspark.history.retainedApplications=30"
    ~~~

    - 参数 1 含义：WEB UI 访问的端口号为 18080
    - 参数 2 含义：指定历史服务器日志存储路径
    - 参数 3 含义：指定保存Application 历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。

  - 修改`spark-defaults.conf`

    ~~~
    spark.yarn.historyServer.address=linux1:18080
    spark.history.ui.port=18080
    ~~~

  - 启动历史服务

    ~~~
    sbin/start-history-server.sh
    ~~~

  - 重新提交应用

    ~~~
    bin/spark-submit \
    --class org.apache.spark.examples.SparkPi \
    --master yarn \
    --deploy-mode client \
    ./examples/jars/spark-examples_2.12-3.0.0.jar \
    10
    ~~~

## 端口号

- Spark 查看当前 Spark-shell 运行任务情况端口号：4040（计算）
- Spark Master 内部通信服务端口号：7077
- Standalone 模式下，Spark Master Web 端口号：8080（资源）
- Spark 历史服务器端口号：18080
- Hadoop YARN 任务运行情况查看端口号：8088

## Spark 运行架构

- Spark框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。
- Driver表示master，负责管理整个集群中的作业任务调度。Executor 则是 slave，负责实际执行任务。

### 核心组件

#### Driver

- Spark驱动器节点，用于执行Spark任务中的main方法，负责实际代码的执行工作。Driver在Spark作业执行时主要负责
  - 将用户程序转化为作业（job）
  - 在Executor之间调度任务(task)
  - 跟踪Executor的执行情况
  - 通过UI展示查询运行情况

#### Executor

- Spark Executor是集群中工作节点（Worker）中的一个JVM进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立。Spark 应用启动时，Executor节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有Executor节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他Executor节点上继续运行。
- Executor有两个核心功能：
  - 负责运行组成Spark应用的任务，并将结果返回给驱动器进程
  - 它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在Executor进程内的，因此任务可以在运行时充分利用缓存数据加速运算。

#### Master & Worker

- Master是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于Yarn环境中的RM
- Worker也是进程，一个Worker运行在集群中的一台服务器上，由Master分配资源对数据进行并行的处理和计算，类似于Yarn环境中NM。

#### ApplicationMaster

- Hadoop用户向YARN集群提交应用程序时,提交程序中应该包含ApplicationMaster，用于向资源调度器申请执行任务的资源容器Container，运行用户自己的程序任务job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。
- ResourceManager（资源）和Driver（计算）之间的解耦合靠的就是ApplicationMaster。

### 核心概念

#### Executor 与 Core

- Spark Executor是集群中运行在工作节点（Worker）中的一个JVM进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点Executor的内存大小和使用的虚拟CPU核（Core）数量。

- 应用程序相关启动参数如下：

  | 名称              | 说明                               |
  | ----------------- | ---------------------------------- |
  | --num-executors   | 配置Executor的数量                 |
  | --executor-memory | 配置每个Executor的内存大小         |
  | --executor-cores  | 配置每个Executor的虚拟CPU core数量 |

#### 并行度(`Parallelism`)

- 在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，记住，这里是并行，而不是并发。这里我们将整个集群并行执行任务的数量称之为并行度。

#### 有向无环图（ `DAG`）

- DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。

#### 提交流程

- Spark应用程序提交到Yarn环境中执行的时候，一般会有两种部署执行的方式：Client和Cluster。两种模式主要区别在于：Driver程序的运行节点位置。

##### `Yarn Client` 模式

- `Client`模式将用于监控和调度的`Driver`模块在客户端执行，而不是在Yarn中，所以一般用于测试。
  - `Driver`在任务提交的本地机器上运行
  - `Driver`启动后会和`ResourceManager`通讯申请启动ApplicationMaster
  - `ResourceManager`分配``container`，在合适的`NodeManager`上启动`ApplicationMaster`，负责向`ResourceManager`申请`Executor`内存
  - `ResourceManager`接到`ApplicationMaster`的资源申请后会分配`container`，然后`ApplicationMaster`在资源分配指定`NodeManager`上启动`Executor`进程
  - `Executor`进程启动后会向`Driver`反向注册，`Executor`全部注册完成后`Driver`开始执行`main`函数
  - 之后执行到`Action`算子时，触发一个`Job`，并根据宽依赖开始划分`stage`，每个`stage`生成对应的`TaskSet`，之后将`task`分发到各个`Executor`上执行。

##### `Yarn Cluster` 模式

- Cluster模式将用于监控和调度的Driver模块启动在Yarn集群资源中执行。一般应用于实际生产环境
  - 在`YARN Cluster`模式下，任务提交后会和`ResourceManager`通讯申请启动`ApplicationMaster`
  - 随后`ResourceManager`分配`container`，在合适的`NodeManager`上启动`ApplicationMaster`，此时的`ApplicationMaster`就是`Driver`。
  - `Driver`启动后向`ResourceManager`申请`Executor`内存，`ResourceManager`接到`ApplicationMaster`的资源申请后会分配`container`，然后在合适的`NodeManager`上启动`Executor`进程
  - `Executor`进程启动后会向`Driver`反向注册，`Executor`全部注册完成后`Driver`开始执行`main`函数，
  - 之后执行到`Action`算子时，触发一个`Job`，并根据宽依赖开始划分`stage`，每个`stage`生成对应的`TaskSet`，之后将`task`分发到各个`Executor`上执行。

## Spark 核心编程

- `Spark`计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是：
  - `RDD` : 弹性分布式数据集
  - 累加器：分布式共享只写变量
  - 广播变量：分布式共享只读变量









