# Spark SQL

## SparkSQL特点

- 写更少的代码，因为 `SQL` 化的代码量远远小于原生代码
- 读更少的数据（`SparkSQL` 的表数据在内存中存储不使用原生态的 `JVM` 对象存储方式，而是采用内存列存储）
- 提供更好的性能（字节码生成技术、`SQL` 优化）
- 号称比 `MapReduce` 快 100 倍
- 强大的 `SQL` 优化器 `Catalyst`

## `SparkSQL Cil`工具

### `spark-sql`

- 是一个交互式的命令行工具，用于在命令行界面中执行 `Spark SQL` 查询。通过 `spark-sql`，你可以使用 `SQL` 语句来查询和操作 `Spark` 中的数据。它提供了一个交互式的环境，类似于传统的 `SQL` 命令行工具，但在 Spark 中支持更强大的分布式计算能力。

### `Spark Beeline`

- `Spark Beeline` 是 `Spark` 提供的一个用于连接和交互式查询 `Hive` 的命令行工具。它基于 `Apache Hive` 的 `Beeline` 客户端，并与 `Spark` 集成，允许用户通过命令行界面连接到 `Spark SQL` 和 `Hive`，并执行 `SQL` 查询。

## `Spark SQL` 数据抽象

- `SparkSQL` 提供了两个新的抽象，分别是 `DataFrame` 和 `DataSet`；同样的数据都给到这三个数据结构（`RDD`、`DataFrame`、`DataSet`），经过系统的计算逻辑，都得到相同的结果。不同是它们的执行效率和执行方式；在后期的 `Spark` 版本中， `DataSet` 会逐步取代 `RDD` 和 `DataFrame` 成为唯一的 `API` 接口。

## `DataFrame`

- `DataFrame`不继承 `RDD`，自己实现了 `RDD` 的大部分功能。与 `RDD` 类似，`DataFrame` 也是一个分布式数据集：`DataFrame` 可以看做分布式 `Row` 对象的集合，提供了由列组成的详细 `Schema` 信息，使其可以得到优化。`DataFrame` 不仅有比 `RDD` 更多的算子，还可以进行执行计划的优化 `DataFrame` 更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即 `schema`。
- `DataFrame` 也支持嵌套数据类型（`struct`、 `array` 和 `map`）。`DataFrame API` 提供的是一套高层的关系操作，比函数式的 `RDD API` 要更加友好，门槛更低。

## `DataSet`

- `DataSet` 是在 `Spark1.6` 中添加的新的接口；与 `RDD` 相比，保存了更多的描述信息,概念上等同于关系型数据库中的二维表；与 `DataFrame` 相比，保存了类型信息，是强类型的，提供了编译时类型检查；调用 `Dataset` 的方法先会生成逻辑计划，然后 `Spark` 的优化器进行优化，最终生成物理计划，然后提交到集群中运行。
- `DataSet` 包含了 `DataFrame` 的功能，在 `Spark2.0` 中两者得到了统一： `DataFrame`表示为 `DataSet`[`Row`]，即 `DataSet` 的子集。

## `Row&Schema`

- 在理解上，可以理解 $DataFrame = RDD[Row] + Schema$；`DataFrame` 的前身是 `SchemaRDD` ，`Row` 是一个泛化的无类型 `JVM object`，可以理解为一行具体的数据集合。

- 而 `DataFrame` 是带有 `Schema` 信息的 `RDD`，`Spark` 通过 `Schema` 就能够读懂数据。

- 什么是 `schema`？

- `DataFrame` 中提供了详细的数据结构信息，从而使得 `SparkSQL` 可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么，`DataFrame` 中的数据结构信息，即为 `schema`。 具体的信息可以参照 `org.apache.spark.sql.types.StructField` 类

  ~~~scala
  def main(args: Array[String]): Unit = {
  	import org.apache.spark.sql.Row
  	val row = Row(1,"abc", 1.2)
  	// Row 的访问方法
  	println(row(0))
  	println(row(1))
  	println(row(2))
  }
  ~~~

- ~~~scala
  /**
  * A field inside a StructType. * @param name The name of this field. * @param dataType The data type of this field. * @param nullable Indicates if values of this field can be `null` values. * @param metadata The metadata of this field. The metadata should
  be preserved during transformation if the content of the column is not
  modified, e.g, in selection. */
  case class StructField(
  name: String, dataType: DataType, nullable: Boolean = true, metadata: Metadata = Metadata.empty)
  ~~~

- 常见的创建 `Schema` 的几种方式：

  ~~~scala
  def main(args: Array[String]): Unit = {
  	// 多种方式定义 schema，其核心是 StructType
  	import org.apache.spark.sql.types._
  	// 来自官方帮助文档
  	val schema1 = StructType(StructField("name", StringType, false) ::
  	StructField("age", IntegerType, false) ::
  	StructField("height", IntegerType,false) :: Nil)
  	val schema2 = StructType( Seq(StructField("name", StringType, false), StructField("age", IntegerType, 								false), StructField("height", IntegerType, false)))
  	val schema3 = StructType( List(StructField("name", StringType, false), StructField("age",IntegerType, false), StructField("height",IntegerType, false)))
  	val schema4 = new StructType()
  					.add(StructField("name", StringType, false))
  					.add(StructField("age", IntegerType, false))
  					.add(StructField("height", IntegerType, false))
  	val schema5 = new StructType()
  					.add("name", StringType, true, "comment1")
  					.add("age", IntegerType, false, "comment2")
  					.add("height", IntegerType, true, "comment3")
  }
  ~~~

## `RDD、DataFrame、Dataset` 的异同

### 三者的共性：

- `RDD`、`DataFrame`、`Dataset` 都是 Spark 平台下的分布式弹性数据集，为处理海量数据提供便利
- 三者都有许多相同的概念，如分区、持久化、容错等；有许多共同的函数，如 map、filter，sortBy 等
- 三者都有惰性机制，只有在遇到 Action 算子时，才会开始真正的计算
- 对 `DataFrame` 和 `Dataset` 进行操作许多操作都需要这个包进行支持，如：import spark.implicits._ 

### 三者的区别：

- 与 `RDD` 和 `Dataset` 不同，`DataFrame` 每一行的类型固定为 `Row`，只有通过解析才能获取各个字段的值
- `DataFrame` 与 `Dataset` 均支持 `SparkSQL` 的操作
- `Dataset` 和 `DataFrame` 拥有完全相同的成员函数，区别只是每一行的数据类型不同；
- `DataFrame` 定义为 `Dataset[Row]`。每一行的类型是 `Row`，然后在 `Row` 包含具体的字段信息
- Dataset 每一行的类型都是一个 case class，在自定义了 case class 之后可以很自由的获得每一行的信息；

## `Spark SQL` 创建

### `SparkSession` 详解

- `Spark` 使用全新的 `SparkSession` 接口替代 `SQLContext` 及 `HiveContext` 接口完成数据的加载、转换、处理等功能。同时 `SparkSession` 也封装了 `SQLContext` 及 `HiveContext`，实现了 `SQLContext` 及 `HiveContext` 所有功能，另外通过 `SparkSession `可以获取到 `SparkConetxt`

- 代码示例

  ~~~scala
  import org.apache.spark.sql.SparkSession
  val spark = SparkSession
  			.builder()
  			.appName("Spark SQL basic example")
  			.master("local")
  			.config("spark.some.config.option", "some-value")
  			.getOrCreate()
  // 用于隐式转换，比如将 RDD 转换为 DataFrame
  import spark.implicits._
  // 具体的业务逻辑从这里开始
  ……
  }
  ~~~

### `DataFrame` & `Dataset` 的创建

- `DataFrame` 简称 `df`，`DataSet` 简称 `ds`，在新版本中开发不需要刻意区分是 `df` 还是 `ds`，我们要知道的是 `df` 只是 `ds` 的一种特殊情况，可以通过 `ds.toDF` 将 `ds` 转换为 `df`。

#### 由 `range` 生成 `ds`

~~~scala
def main(args: Array[String]): Unit = {
	import org.apache.spark.sql.SparkSession
	val spark = SparkSession
		.builder()
		.appName("range to ds")
		.master("local")
		.getOrCreate()
	// 隐式转换
	import spark.implicits._ val 
	numDS = spark.range(5, 100, 5)
	// orderBy 转换操作；desc：function；show：Action
	import org.apache.spark.sql.functions._ numDS.orderBy(desc("id")).show(5)
	// 统计信息
	numDS.describe().show
	// 显示 schema 信息
	numDS.printSchema
	// 使用 RDD 执行同样的操作
	numDS.rdd.map(_.toInt).stats
	// 检查分区数
	numDS.rdd.getNumPartitions
}
~~~

#### 由集合生成 `df`

~~~scala
val lst = List(("Jack", 28, 184), ("Tom", 10, 144), ("Andy", 16, 165))
val df1 = spark.createDataFrame(lst). // 改单个字段名时简便
			.withColumnRenamed("_1", "name1")
			.withColumnRenamed("_2", "age1")
			.withColumnRenamed("_3", "height1")
df1.orderBy("age1").show(10)
// desc 是函数，在 IDEA 中使用是需要导包
import org.apache.spark.sql.functions._ df1.orderBy(desc("age1")).show(10)
// 修改整个 DF 的列名
val df2 = spark.createDataFrame(lst).toDF("name", "age", "height")
~~~

#### 由集合生成 `ds`

~~~scala
// 注意 Seq 中元素的类型
val seq1 = Seq(Person("Jack", 28, 184), Person("Tom", 10, 144), Person("Andy", 16, 165))
val ds1 = spark.createDataset(seq1)
// 显示 schema 信息
ds1.printSchema
ds1.show
// 可以看出有无 schema 的区别
val seq2 = Seq(("Jack", 28, 184), ("Tom", 10, 144), ("Andy", 16, 165))
val ds2 = spark.createDataset(seq2)
ds2.show


case class Person(name: String, age: Int, height: Int)
~~~

#### 从文件创建 `df`(以 `csv` 文件为例)

~~~scala
val df1 = spark.read.csv("data/people1.csv")
df1.printSchema()
df1.show()
val df2 = spark.read.csv("data/people2.csv")
df2.printSchema()
df2.show()
// 指定参数
// spark 2.3.0
val schema = "name string, age int, job string" 
val df3 = spark.read
	.options(Map(("delimiter", ";"), ("header", "true")))
	.schema(schema)
	.csv("data/people2.csv")
df3.printSchema()
df3.show
// 自动类型推断
val df4 = spark.read
	.option("delimiter", ";")
	.option("header", "true")
	.option("inferschema", "true")
	.csv("data/people2.csv")
df4.printSchema()
df4.show
~~~

### `rdd`、`df` 和 `ds` 三者之间的转换

~~~scala
def main(args: Array[String]): Unit = {
val spark = SparkSession.builder()
.master("local")
.appName("transformation")
.getOrCreate()
// 隐式转换
import spark.implicits._ val sc = spark.sparkContext
val arr = Array(("Jack", 28, 184), ("Andy", 16, 177), ("Bob", 42, 165))
// //DataFrame = data + schema
// // 数据的组装(data)
// val value: RDD[Row] = sc.makeRDD(arr)
// .map(f => Row(f._1, f._2, f._3))
// // 信息的组装（schema）
// val schema = new StructType()
// .add("name","string",false)
// .add("age","int",false)
// .add("height","int",false)
//
// val df: DataFrame = spark.createDataFrame(value, schema)
// import org.apache.spark.sql.functions._
// df.orderBy(desc("age")).show()
//
// // rdd => df toDF
val df = spark.createDataFrame(arr).toDF("name","age","height")
// sc.makeRDD(arr).toDF("name","age","height")
// rdd => ds toDS
val ds = sc.makeRDD(arr)
.map(f => Student(f._1, f._2, f._3))
.toDS()
// df => rdd rdd
val rdd1: RDD[Row] = df.rdd
// ds => rdd rdd
val rdd: RDD[Student] = ds.rdd
// df => ds
val value: Dataset[Row] = df.as("demo")
// ds => df
val frame: DataFrame = ds.toDF()
}
case class Student(name:String,age:Int,height:Int)
~~~

![rdd、dfds](.\Image\rdd、df 和 ds`三者之间的转换.png)

- `SparkSQL` 提供了一个领域特定语言(`DSL`)以方便操作结构化数据。核心思想还是 SQL；仅仅是一个语法的问题。

## `Spark SQL`操作详解

### `Transformation` 操作

- `map`、`filter`、`flatMap`、`mapPartitions`、`sample`、 `randomSplit`、 `limit`、`distinct`、`dropDuplicates`、`describe`，而以上这些都是企业中比较常用的，这里在一个文件中统一论述。

  ~~~scala
  val df1 = spark.read.json("src/main/resources/people.json")
  // 使用 map 去除某些字段
  df1.map(row =>
  	row.getAs[Long](1)).withColumnRenamed("value","age").show()
  //df1.map(row => row.getAs[String]("address")).show()
  //df1.map(row => row.getString[String](0)).show()
  // randomSplit,按照数组中的权重将数据集划分为不同的比例，可用于机器
  学习
  val df2 = df1.randomSplit(Array(0.5, 0.6, 0.7))
  df2(0).count
  df2(1).count
  df2(2).count
  // 取 10 行数据生成新的 DataSet
  val df3 = df1.limit(5).show()
  // distinct，去重
  val df4 = df1.union(df1)
  df4.distinct.count
  // 这个方法，不需要传入任何的参数，默认根据所有列进行去重，然后按数据行的顺序保留每行数据出现的第一条。
  df4.dropDuplicates.show
  // 传入的参数是一个序列。你可以在序列中指定你要根据哪些列的重复元素对数据表进行去重，然后也是返回每一行数据出现的第一条
  //def dropDuplicates(colNames: Seq[String])
  df4.dropDuplicates("name", "age").show
  df4.dropDuplicates("name").show
  // 返回全部列的统计（count、mean、stddev、min、max）
  df4.describe().show
  // 返回指定列的统计
  df4.describe("age").show
  df4.describe("name", "age").show
  ~~~

### 存储相关

- `persist`、`checkpoint`、`unpersist`、`cache`

- 备注：`Dataset` 默认的存储级别是 `MEMORY_AND_DISK`

  ~~~scala
  val df1 = spark.read.json("src/main/resources/people.json")
  import org.apache.spark.storage.StorageLevel
  spark.sparkContext.setCheckpointDir("src/main/resources/data
  /checkpoint")
  df1.show()
  df1.checkpoint()
  // 默认的存储级别是 MEMORY_AND_DISK
  df1.cache()
  df1.persist(StorageLevel.MEMORY_ONLY)
  println(df1.count())
  df1.unpersist(true)
  ~~~

### `select` 相关

- 列的多种表示：`select`、`selectExpr`、`drop`、`withColumn`、`withColumnRenamed`、`cast`（内置函数）

  ~~~scala
  import spark.implicits._
  import org.apache.spark.sql.functions._ val df1 = spark.read.json("src/main/resources/people.json")
  // 列的多种表示方法。使用'、""、$""、col()、df("")
  // 注意：不要混用；必要时使用 spark.implicitis._；并非每个表示在所有的
  地方都有效
  df1.select('name, 'age, 'address).show
  df1.select("name", "age", "address").show
  df1.select($"name", $"age", $"address").show
  df1.select(col("name"), col("age"), col("address")).show
  df1.select(df1("name"), df1("age"), df1("address")).show
  // 下面的写法无效并且会报错
  // df1.select("name", "age"+10, "address").show
  // df1.select("name", "age+10", "address").show
  // 这样写才符合语法
  df1.select($"name", $"age"+10, $"address").show
  df1.select('name, 'age+10, 'address).show
  // 可使用 expr 表达式(expr 里面只能使用引号)
  df1.select(expr("name"), expr("age+100"), expr("address")).show
  df1.selectExpr("name as ename").show
  df1.selectExpr("power(age, 2)", "address").show
  df1.selectExpr("round(age, -3) as newAge", "name", "address").show
  // drop、withColumn、 withColumnRenamed、casting
  // drop 删除一个或多个列，得到新的 DF
  df1.drop("name")
  df1.drop("name", "age")
  // withColumn，修改列值
  val df2 = df1.withColumn("age", $"age"+10)
  df2.show
  // withColumnRenamed，更改列名
  df1.withColumnRenamed("name", "ename")
  // 备注：drop、withColumn、withColumnRenamed 返回的是 DF
  // 类型转化的两种方式
  df1.selectExpr("cast(age as string)").printSchema
  import org.apache.spark.sql.types._ df1.select('age.cast(StringType)).printSchema
  ~~~

### `where` 相关的

~~~scala
val df1 = spark.read.json("src/main/resources/people.json")
// 过滤操作
df1.filter("age>30").show
df1.filter("age>30 and name=='Tom'").show
// 底层调用的就是 filter 算子
df1.where("age>30").show
df1.where("age>30 and name=='Tom'").show
~~~

### `groupBy` 相关的

- `groupBy`、`agg`、`max`、`min`、`avg`、`sum`、`count`（后面 5 个为内置函数)

~~~scala
import spark.implicits._
import org.apache.spark.sql.functions._ val df1 = spark.read.json("src/main/resources/people.json")
// 内置的 sum max min avg count
df1.groupBy("address").sum("age").show
df1.groupBy("address").max("age").show

df1.groupBy("address").min("age").show
df1.groupBy("address").avg("age").show
df1.groupBy("address").count.show
// 类似 having 子句
df1.groupBy("address").avg("age").where("avg(age) > 20").show
df1.groupBy("address").avg("age").where($"avg(age)" > 20).show
// agg
df1.groupBy("address").agg("age"->"max", "age"->"min", "age"->"avg", "age"->"sum", "age"->"count").show
// 这种方式更好理解
df1.groupBy("address").agg(max("age"), min("age"), avg("age"), sum("age"), count("age")).show
// 给列取别名
df1.groupBy("address").agg(max("age"), min("age"), avg("age"), sum("age"), count("age")).withColumnRenamed("min(age)", "minAge").show
// 给列取别名，最简便
df1.groupBy("address").agg(max("age").as("maxAge"), min("age").as("minAge"), avg("age").as("avgAge"), sum("age").as("sumAge"), count("age").as("countAge")).show
~~~

### `orderBy` 相关的

~~~scala
import spark.implicits._
val df1 = spark.read.json("src/main/resources/people.json")
// sort，以下语句等价
df1.sort("age").show
df1.sort($"age").show
df1.sort($"age".asc).show
df1.sort($"age".desc).show
df1.sort(-$"age").show
df1.sort(-'age, -'name).show
// orderBy,底层调用的还是 sort
df1.orderBy("age").show
~~~

### `join` 相关的

- 目前 `Apache Spark 3.x` 版本中，一共支持以下七种 `Join` 类型：
  - INNER JOIN
  - CROSS JOIN
  - LEFT OUTER JOIN
  - RIGHT OUTER JOIN
  - FULL OUTER JOIN
  - LEFT SEMI JOIN
  - LEFT ANTI JOIN

#### `INNER JOIN`

- 在 `Spark` 中，如果没有指定任何 `Join` 类型，那么默认就是 `INNER JOIN`。`INNER JOIN` 只会返回满足 `Join` 条件（ `join condition`）的数据，这个在企业中用的应该比较多，具体如下：

  ~~~scala
  customer.join(order,"customerId").show
  ~~~

- 备注：以上是单字段关联，如果是多字段使用如下的形式：Seq(“customerId”, “name”)

#### `CROSS JOIN`

- 这种类型的 `Join` 也称为笛卡儿积（`Cartesian Product`），`Join` 左表的每行数据都会跟右表的每行数据进行 `Join`，产生的结果行数为 `m*n`，所以在生产环境下尽量不要用这种 `Join`。下面是 `CROSS JOIN` 的使用例子：

  ~~~scala
  customer.crossJoin(order).show()
  // 如果两张表出现相同的字段，可以使用下面的方式进去筛选
  customer.crossJoin(order)
  .select(customer(“age”), order(“amount”))
  .show
  ~~~

### 集合相关的

- `union`、`unionAll`、`intersect`、`except`

~~~scala
val lst = List(StudentAge(1,"Alice", 18), StudentAge(2,"Andy",19), StudentAge(3,"Bob", 17), StudentAge(4,"Justin", 21), StudentAge(5,"Cindy", 20)
)
val ds1 = spark.createDataset(lst)
ds1.show()
val rdd = spark.sparkContext.makeRDD(List(StudentHeight("Alice", 160),StudentHeight("Andy", 159), StudentHeight("Bob", 170), StudentHeight("Cindy", 165), StudentHeight("Rose", 160))
)
val ds2 = rdd.toDS
// union、unionAll、intersect、except。集合的交、并、差
val ds3 = ds1.select("name")
val ds4 = ds2.select("sname")
// union 求并集，不去重
ds3.union(ds4).show
// 底层依旧调用的是 union
ds3.unionAll(ds4).show
// intersect 求交
ds3.intersect(ds4).show
// except 求差
ds3.except(ds4).show
}

// 定义第一个数据集
case class StudentAge(sno: Int, name: String, age: Int)
// 定义第二个数据集
case class StudentHeight(sname: String, height: Int)
~~~

### 空值处理

- `na.fill`、`na.drop`、`na.replace`、`na.filter`

~~~scala
import spark.implicits._
import org.apache.spark.sql.functions._ val df1 = spark.read.json("src/main/resources/people.json")
// NA 表示缺失值，即“Missing value”，是“not available”的缩写
// 删出含有空值的行
df1.na.drop.show
// 删除某列的空值和 null
df1.na.drop(Array("age")).show
// 对全部列填充
df1.na.fill("NULL").show
// 对指定单列填充；对指定多列填充
df1.na.fill("NULL", Array("address")).show
df1.na.fill(Map("age"->0, "address"->"NULL")).show
// 对指定的值进行替换
df1.na.replace(Array("address"), Map("NULL" -> "Shanghai"))
.na.replace(Array("age"), Map(0 -> 100))
.show
// 查询空值列或非空值列。isNull、isNotNull 为内置函数
df1.filter("address is null").show
df1.filter($"address".isNull).show
df1.filter(col("address").isNull).show
df1.filter("address is not null").show
df1.filter(col("address").isNotNull).show
~~~



## `JOIN` 执行的 5 种策略

- `Spark` 提供了 5 种 `JOIN` 机制来执行具体的 `JOIN` 操作。该 5 种 JOIN 机制如下所示：
  - `Shuffle Hash Join`
  - `Broadcast Hash Join`
  - `Sort Merge Join`
  - `Cartesian Join`
  - `Broadcast Nested Loop Join`
- 源码见于：`org.apache.spark.sql.execution.SparkStrategy`

### `Shuffle Hash Join（SHJ）`

- 当要 `JOIN` 的表数据量比较大时，可以选择 `Shuffle Hash Join`。这样可以将大表进行按照 `JOIN` 的 `key` 进行重分区，保证每个相同的 `JOIN key` 都发送到同一个分区中。
- `Shuffle Hash Join` 的基本步骤主要有以下两点：
  1. 首先，对于两张参与 `JOIN` 的表，分别按照 `join key` 进行重分区，该过程会涉及 `Shuffle`，其目的是将相同 `join key` 的数据发送到同一个分区，方便分区内进行 `join`。
  2. 其次，对于每个 `Shuffle` 之后的分区，会将小表的分区数据构建成一个 `Hash table`，然后根据 `join key` 与大表的分区数据记录进行匹配。
- 条件与特点：
  1. 仅支持等值连接，`join key` 不需要排序
  2. 支持除了全外连接(`full outer`)之外的所有 `join` 类型
  3. 需要对小表构建 `Hash table`，属于内存密集型的操作，如果构建侧数据比较大，可能会造成 `OOM`
  4. 将参数 `spark.sql.join.prefersortmergeJoin` (`default true`)置为 `false`

### `Broadcast Hash Join（BHJ）`

- 也称之为 `Map` 端 `JOIN`。当有一张表较小时，我们通常选择 `Broadcast Hash Join`，这样可以避免 `Shuffle` 带来的开销，从而提高性能。比如事实表与维表进行 `JOIN` 时，由于维表的数据通常会很小，所以可以使用 `BHJ` 将维表进行 `Broadcast`。这样可以避免数据的 `Shuffle`(在 `Spark` 中 `Shuffle` 操作是很耗时的)，从而提高 `JOIN` 的效率。在进行 `BroadcastJoin` 之前，`Spark` 需要把处于 `Executor` 端的数据先发送到 `Driver` 端，然后 `Driver` 端再把数据广播到 `Executor` 端。如果我们需要广播的数据比较多，会造成 `Driver` 端出现`OOM`。

- `Broadcast Hash Join` 主要包括两个阶段：

  1. `Broadcast` 阶段 ：小表被缓存在 `executor` 中
  2. `Hash Join` 阶段：在每个 `executor` 中执行 `Hash Join`

- 条件与特点：

  1. 仅支持等值连接，`join key` 不需要排序
  2. 支持除了全外连接(`full outer`)之外的所有 `join` 类型
  3. `BHJ` 相比其他的 `JOIN` 机制而言，效率更高。但是，`BHJ` 属于网络密集型的操作(数据冗余传输)，除此之外，需要在 `Driver` 端缓存数据，所以当小表的数据量较大时，会出现 `OOM` 的情况

  4. 被广播的小表的数据量要小于 `spark.sql.autoBroadcastJoinThreshold` 值，默认是 `10MB`，但是被广播表的大小阈值不能超过 `8GB`，否则会报错。
  5. 基表不能被 `broadcast`，比如左连接时，只能将右表进行广播。形如：`fact_table.join`(`broadcast`(`dimension_table`)，可以不使用 `broadcast` 提示，当满足条件时会自动转为该 `JOIN` 方式。

### `Sort Merge Join（SMJ）`

- 该 `JOIN` 机制是 `Spark` 默认的，可以通过参数 `spark.sql.join.preferSortMergeJoin`进行配置，默认是 `true`，即优先使用 `SMJ`。一般在两张大表进行 `JOIN` 时，使用该方式。`SMJ`可以减少集群中的数据传输，该方式不会先加载所有数据的到内存，然后进行 `Hash Join`，但是在 `JOIN` 之前需要对 `join key` 进行排序。

- `Sort Merge Join` 主要包括三个阶段：

  1. `Shuffle Phase` : 两张大表根据 `Join key` 进行 `Shuffle` 重分区
  2. `Sort Phase`: 每个分区内的数据进行排序
  3. `Merge Phase`: 对来自不同表的排序好的分区数据进行 `JOIN`，通过遍历元素，连接具有相同 `Join key` 值的行来合并数据集

- 条件与特点：

  1. 仅支持等值连接

  2. 支持所有 `join` 类型

  3. `Join Keys` 是排序的

  4. 参数 `spark.sql.join.prefersortmergeJoin` (默认 `true`)设定为 `true`

### `Cartesian Join（Shuffle-and-replicate nested loop join）`

- 如果 `Spark` 中两张参与 `Join` 的表没指定 `join key`（`ON` 条件），那么会产生 `Cartesian product join`，这个 `Join` 得到的结果其实就是两张行数的乘积。
- 条件：
  1. 仅支持内连接
  2. 支持等值和不等值连接
  3. 开启参数 `spark.sql.crossJoin.enabled=true`

### `Broadcast Nested Loop Join（BNLJ）`

- 该方式是在没有合适的 `JOIN` 机制可供选择时，最终会选择该种 `join` 策略。优先级为：$Broadcast Hash Join > Sort Merge Join > Shuffle Hash Join > cartesian Join > Broadcast Nested Loop Join$
- 在 `Cartesian` 与 `Broadcast Nested Loop Join` 之间，如果是内连接，或者非等值连接，则优先选择 `Cartesian Join` 策略；当是非等值连接并且一张表可以被广播时，会选择 `Broadcast Nested Loop`。
- 条件与特点：
  1. 支持等值和非等值连接
  2. 支持所有的 JOIN 类型，主要优化点如下：
     1. 当右外连接时要广播左表
     2. 当左外连接时要广播右表
     3. 当内连接时，要广播左右两张表

## `Spark` 是如何选择 `JOIN` 策略的

### 等值连接的情况

- 有 `join` 提示(`hints`)的情况，按照下面的顺序：
  1. `Broadcast Hint`：如果 `join` 类型支持，则选择 `broadcast hash join`
  2. `Sort merge hint`：如果 `join key` 是排序的，则选择 `sort-merge join`
  3. `shuffle hash hint`：如果 join 类型支持， 选择 `shuffle hash join`
  4. `shuffle-and-replicate nested loop join(cartesian product join)`： 如果是内连接，选择笛卡尔积方式
- 没有 `join` 提示(`hints`)的情况，则逐个对照下面的规则自动选择：
  1. 如果 `join` 类型支持，并且其中一张表能够被广播 (`spark.sql.autoBroadcastJoinThreshold` 值，默认是 `10MB`)，则选择 `broadcast hash join`
  2. 如果参数 `spark.sql.join.preferSortMergeJoin` 设定为 `false`，且一张表足够小(可以构建一个 `hash map`) ，则选择 `shuffle hash join`
  3. 如果 `join keys` 是排序的，则选择 `sort-merge join`
  4. 如果是内连接，选择 `cartesian join`
  5. 如果可能会发生 `OOM` 或者没有可以选择的执行策略，则最终选择 `broadcast nested loop join`

### 非等值连接情况

- 有 `join` 提示(`hints`)，按照下面的顺序：
  1. `broadcast hint`：选择 `broadcast nested loop join`
  2. `shuffle-and-replicate nested loop join hint`: 如果是内连接，则选择 `cartesian product join` 

- 没有 `join` 提示(`hints`)，则逐个对照下面的规则自动选择：
  1. 如果一张表足够小(可以被广播)，则选择 `broadcast nested loop join`
  2. 如果是内连接，则选择 `cartesian product join`
  3. 如果可能会发生 `OOM` 或者没有可以选择的执行策略，则最终选择 `broadcast nested loop join`

## `Action` 操作

- `show`、`collect`、`collectAsList`、`head`、`first`、`count`、`take`、`takeAsList`、`reduce`

  ~~~scala
  def main(args: Array[String]): Unit = {
  	val spark = SparkSession.builder()
  		.master("local")
  		.appName("like rdd")
  		.getOrCreate()
  	// 隐式转换
  	import spark.implicits._
  	// show：显示结果，默认显示 20 行，截取（true）
  	spark.read.json("src/main/resources/data/people.json").show(100, false)
  	val df = spark.read.json("src/main/resources/data/people.json")
  	println(df.count())
  	df.collect().foreach(println)
  	df.collectAsList().forEach(println)
  	df.head(3).foreach(println)
  	println(df.first())
  	// 底层调用的就是 take
  	df.take(3).foreach(println)
  	df.takeAsList(3).forEach(println)
  }
  ~~~

## 获取结构属性的操作

- `printSchema`、`explain`、`columns`、`dtypes`、`col`

  ~~~scala
  val df1 = spark.read.json("src/main/resources/data/people.csv")
  // 结构属性
  df1.columns // 查看列名
  df1.dtypes // 查看列名和类型
  df1.explain() // 参看执行计划
  df1.col("name") // 获取某个列
  df1.printSchema // 常用
  ~~~

## `Spark SQL` 中的相关概念

### `sql` 算子和 `table` 算子

- `spark sql` 中常见的 `Transformation` 算子和 `Action` 算子，这类算子我们称作是 `DSL` 算子，即所有的算子由用户拼接操作，可高度定制化。但是既然是 `Spark SQL` 我们是否可以直接写 `sql` 呢？答案肯定是可以的。`SparkSession` 提供了 `sql` 的算子

  ~~~scala
  def sql(sqlText: String): DataFrame = withActive {
  	val tracker = new QueryPlanningTracker
  	val plan = tracker.measurePhase(QueryPlanningTracker.PARSING) {
  	sessionState.sqlParser.parsePlan(sqlText)
  	}
  	Dataset.ofRows(self, plan, tracker)
  }
  ~~~

- 由上我们可以看出，我们只需要传递一个 `sql` 字符串就可以实现 `spark sql` 的功能，相比 `DSL` 算子而言就会方便很多。同时上述算子返回的虽然是 `DataFrame`，但是从源码中我们可以看到其实是 `Dataset.ofRow`转换的，因此也是类型安全的。

## 有管理表和无管理表

- 管理表是由 `Spark SQL` 管理其元数据和数据存储的表。当你创建一个管理表时，`Spark` 会将表的元数据（如表结构、列类型等）和数据存储在默认的存储位置中，通常是在 `Hive` 的仓库中（默认是在 `HDFS` 上）。当你删除一个管理表时，`Spark` 会自动删除表的元数据和数据。
- 无管理表是指 `Spark SQL` 中的表，其元数据由 `Spark` 管理，但数据存储在外部存储系统中，例如 `HDFS`、`S3`、`HBase` 等。创建无管理表时，你需要指定数据的存储位置（`path`），`Spark` 会将元数据保存在其内部，但不会管理数据的生命周期。这意味着当你删除一个无管理表时，`Spark` 只会删除元数据，而不会删除实际的数据。

## 视图及创建

- `Spark` 还可以基于已有的表创建视图。视图既可以是全局的（对给定集群的所有 `SparkSession` 可见），也可以是会话级别的（只对单个 `SparkSession` 可见）。
- 会话级别的视图就是临时视图，这些视图随 `Spark` 应用结束而消失。创建视图的语法与创建表的语法相似,创建好的视图也可以像表那样进行查询。视图和表的区别是，视图不会存放实际数据。
- `Spark` 应用结束后，表依然存在，但临时视图会消失。即视图是基于真实表的一张虚拟的表，其数据来源均建立在真实表的基础。视图提供的是对查询操作的封装，本身不包含数据，所呈现的数据是根据视图定义从基础表中检索出来的，如果基础表的数据新增或删除，视图呈现的也是更新后的数据。视图定义后，编写完所需的查询，可以方便地重用该视图，创建好视图后，可以像表那样对其发起查询。
- 备注：`spark sql` 现在已经没有临时表的概念，统统用临时视图代替。另外相关的 `api` 已经是过期状态。

## 缓存

- 用户可以像对 `RDD` 缓存算子那样缓存 `Spark SQL`中表与视图，或删除缓存。在 `Spark 3.0` 中，除了缓存命令原有的一些选项，还可以将表声明为 `LAZY`，这样它就不会立即被缓存，直到第一次遇到 `Action` 算子时才会真正被缓存

  ~~~scala
  def main(args: Array[String]): Unit = {
  	val spark = SparkSession.builder()
  		.master("local")
  		.config("spark.sql.legacy.createHiveTableByDefault", "false") // 设置为 false，创建本地表
  		.appName("cache")
  		.getOrCreate()
  	// 创建缓存的两种方式
  	// 第一种方式使用 df
  	val df = spark.read.option("header", true).csv("src/main/resources/data/people.csv")
  	df.createTempView("people")
  	// spark.sqlContext.cacheTable("people")
  	// spark.sqlContext.uncacheTable("people")
  	// df.cache
  	// 遇见 Action 算子才会触发缓存
  	// df.show()
  	
  	// 第二种方式：sql
  	// 即使是视图，也要写成 cache table，否则会报错
  	spark.sql("CACHE TABLE people")
  	spark.sql("CACHE TABLE activity_cached as select * from people where age > 15")
  	spark.sql("select * from activity_cached").show
  	// CACHE TABLE 是即时生效(eager)的，如果你想等到一个 action 操作再缓存数据可以使用 CACHE LAZY TABLE, 
  	// 这样操作会直到一个 action 操作才被触发，例如 count(*)
  	df.createTempView("people1")
  	spark.sql("CACHE LAZY TABLE people1")
  	// 取消缓存表
  	spark.sql("UNCACHE TABLE people1")
  }
  ~~~

## spark sql 中的元数据管理

- `SparkSQL` 中的 `DataSet` 和 `Dataframe API` 支持结构化分析，结构化分析的一个重要的方面是管理元数据。这些元数据可能是一些临时元数据（比如临时表）、`Hive` 上注册的 `UDF` 以及持久化的元数据（比如 `Hivemeta store` 或者 `HCatalog`）。
- `Spark 2.0`中添加了标准的 `API`（称为 `catalog`）来访问 `Spark SQL` 中的元数据。这个 `API` 既可以操作 `Spark SQL`，也可以操作 `Hive` 元数据（如果配置的话）
- `Catalog` 就是 `Spark 2.0` 之后提供的访问元数据的类，其提供一些 `API` 用来对数据库、表、视图、缓存、列、函数（`UDF`/`UDAF`）进行操作

### 引入 `catalog`

~~~scala
val spark = SparkSession.builder()
	.master("local")
	.appName("catalog")
	.getOrCreate()
val catalog: Catalog = spark.catalog
~~~

### 相关的 `API`

#### 数据库相关

~~~scala
class Database(
	val name: String, 
	@Nullable val description: String, 
	val locationUri: String)
	extnds DefinedByConstructorParams {

	override def toString: String = { "Database[" + s"name='$name', " + Option(description).map { d => 	s"description='$d', " }.getOrElse("") + s"path='$locationUri']"
	}
}
~~~

- 从源码中我们可以看出 `Catalog`使用三个字段表示一个数据库：

  - `name`：数据库名字

  - `description`：数据库描述，可以认为是注释

  - `locationUri`：数据库的数据保存位置

  - 备注：暂时不支持使用 `catalog` 创建 `database`，只能使用 `spark.sql("create database ds_spark")`这种方式去创建数据库。

    ~~~scala
    def main(args: Array[String]): Unit = {
    	val spark = SparkSession.builder()
    		.master("local")
    		.appName("catalog database")
    		.getOrCreate()
    	val catalog: Catalog = spark.catalog
    	// 返回当前使用的数据库，相当于 select database();
    	println(catalog.currentDatabase)
    	// 设置当前使用的数据库，相当于 use database_name
    	catalog.setCurrentDatabase("default")
    	// 查看所有数据库，包含了 database 的完整的信息，相当于 show databases;
    	catalog.listDatabases().show(false)
    	// 获取某数据库的元数据，返回值是 Database 类型的，如果指定的数据库不存在则会@throws[NoSuchDatabaseException]("databasedoes not exist")
    	catalog.getDatabase("zhangsan")
    	// 判断某个数据库是否已经存在，返回 boolean 值。为了避免抛异常
    	对单个数据库进行 getDatabase 获取元数据之前还是先使用
    	databaseExists 确定数据库已经存在。
    	println(catalog.databaseExists("zhangan"))
    }
    ~~~

#### 表相关

~~~scala
class Table(
	val name: String, 
	@Nullable val database: String,
	@Nullable val description: String, 
	val tableType: String, 
	val isTemporary: Boolean)
	extends DefinedByConstructorParams {
		override def toString: String = { "Table[" + s"name='$name', " + Option(database).map { d => s"database='$d', " }.getOrElse("") + Option(description).map { d => s"description='$d', " }.getOrElse("") + s"tableType='$tableType', " + s"isTemporary='$isTemporary']"
	}
}
~~~

- 从源码中我们可以看出 `Catalog` 使用五个字段表示一张表：
  - `name`：表的名字
  - `database`：表所属的数据库的名字
  - `description`：表的描述信息
  - `tableType`：用于区分是表还是视图，两个取值：`table` 或 `view`。
  - `isTemporary`：是否是临时视图

~~~scala
def main(args: Array[String]): Unit = {
	val spark = SparkSession.builder()
		.master("local")
		.appName("catalog table")
		.getOrCreate()
	val catalog: Catalog = spark.catalog
	// 查看所有表或视图，相当于 show tables，同时可以指定数据库的名字，展示的就是指定库下的表或视图
	catalog.listTables().show()
	val schema = new StructType()
		.add("id", IntegerType, nullable = false)
		.add("name", StringType, nullable = true)
	var options = new util.HashMap[String, String]()
	// 从源码中我们可以看到 判断是内部表还是外表的依据就是有没有 PATH 字段
	// options.put("path", "src/main/resources/data/")
	catalog.createTable("ds_learn", "csv", schema, "ds learn", options).show()
	catalog.listTables().show()
	// 获取表的元信息，不存在则会抛出异常。
	println(catalog.getTable("ds_learn"))
	// 判断表或视图是否存在，返回 boolean 值。
	println(catalog.tableExists("ds_learn"))
	// 使用 createOrReplaceTempView 类似 API 注册的临时视图可以使用此方法删除，
	// 如果这个视图已经被缓存过的话会自动清除缓存。
	val df = spark.read.option("header", true).csv("src/main/resources/data/people.csv")
	// 创建临时视图
	df.createTempView("tmp_view1")
	catalog.dropTempView("tmp_view1")
	// catalog.dropGlobalTempView("tmp_view1")
	// 缓存
	catalog.cacheTable("ds_learn")
	catalog.isCached("ds_learn")
	// 清空指定表缓存
	catalog.uncacheTable("ds_learn")
	// 清空所有缓存
	catalog.clearCache()
	// Spark 为了性能考虑，对表的元数据做了缓存，所以当被缓存的表已经改变时也必须刷新元数据重新缓存。
	catalog.refreshTable("ds_learn")
}
~~~

#### 函数相关

~~~scala
class Function(
	val name: String, 
	@Nullable val database: String, 
	@Nullable val description: String, 
	val className: String, 
	val isTemporary: Boolean)
	extends DefinedByConstructorParams {
		override def toString: String = { "Function[" + s"name='$name', " + Option(database).map { d => s"database='$d', " }.getOrElse("") + Option(description).map { d => s"description='$d', " }.getOrElse("") +  s"className='$className', " + s"isTemporary='$isTemporary']"
	}
}
~~~

- 从源码中我们可以看出 `Catalog` 使用五个字段表示一个函数：
  - `name`：函数的名字
  - `database`：函数注册在哪个数据库下，函数是跟数据库绑定的
  - `description`：对函数的描述信息，可以理解成注释
  - `className`：函数其实就是一个 `class`，调用函数就是调用类的方法，`className` 表示函数对应的 `class` 的全路径类名
  - `isTemporary`：是否是临时函数。

~~~scala
def main(args: Array[String]): Unit = {
	val spark = SparkSession.builder()
		.master("local")
		.appName("catalog function")
		.getOrCreate()
	val catalog: Catalog = spark.catalog
	// 列出当前数据库下的所有函数，包括注册的临时函数。
	catalog.listFunctions().show(false)
	// 列出指定数据库下注册的所有函数，包括临时函数，
	// 如果指定的数据库不存在的话则会抛出 @throws[AnalysisException]("database does not exist")表示数据库不存在。
	catalog.listFunctions("default").show(false)
	// 获取函数的元信息，函数不存在则会抛出异常。
	println(catalog.getFunction("abs"))
	// 判断函数是否存在，返回 boolean 值。
	println(catalog.functionExists("abs"))
}
~~~

#### 列相关

~~~scala
class Column(
	val name: String, 
	@Nullable val description: String, 
	val dataType: String, 
	val nullable: Boolean, 
	val isPartition: Boolean,
	val isBucket: Boolean)
	extends DefinedByConstructorParams {
		override def toString: String = { "Column[" + s"name='$name', " + Option(description).map { d => s"description='$d', " }.getOrElse("") + s"dataType='$dataType', " + s"nullable='$nullable', " + s"isPartition='$isPartition', " + s"isBucket='$isBucket']"
	}
}
~~~

- 从源码中我们可以看出 `Catalog` 使用六个字段表示一个列：
  - `name`：列的名字
  - `description`：列的描述信息，与注释差不多
  - `dataType`：列的数据类型
  - `nullable`：列是否允许为 `null`
  - `isPartition`：是否是分区列
  - `isBucket`：是否是桶列

~~~scala
def main(args: Array[String]): Unit = {
	val spark = SparkSession.builder()
		.master("local")
		.appName("catalog table")
		.getOrCreate()
	val catalog: Catalog = spark.catalog
	val df = spark.read.option("header", true).csv("src/main/resources/data/people.csv")
	// 创建临时视图
	df.createTempView("tmp_view1")
	catalog.listColumns("tmp_view1").show()
}
~~~

## 数据源详解

### 数据源之通用操作 load 和 save 操作

- 对于 `Spark SQL` 的 `DataFrame` 来说，无论是从什么数据源创建出来的 `DataFrame`，都有一些共同的 `load` 和 `save` 操作。

  - `load` 操作主要用于加载数据，创建出 `DataFrame`；

  - `save`操作，主要用于将 `DataFrame` 中的数据保存到文件中。如果不指定 `format`，那么默认的就是 `parquet` 文件。

- 也可以手动指定用来操作的数据源类型。数据源通常需要使用其全限定名来指定，比如 `parquet` 是 `org.apache.spark.sql.parquet`。但是 `Spark SQL` 内置了一些数据源类型，比如 `json`，`parquet`，`jdbc` 等等。实际上，通过这个功能，就可以在不同类型的数据源之间进行转换了。比如将 `json` 文件中的数据保存到 `parquet` 文件中。

- ~~~scala
  val df = spark.read.format("json").load("people.json")
  df.select("name","age").write.save("namesAndAges.parquet")
  ~~~

- `Spark SQL` 对于 `save` 操作，提供了不同的 `save mode`，主要用来处理当目标位置，已经有数据时，应该如何处理。而且 `save` 操作并不会执行锁操作，并且不是原子的，因此是有一定风险出现脏数据的。

- | `Save Mode`                     | 意义                                                         |
  | ------------------------------- | ------------------------------------------------------------ |
  | `SaveMode.ErrorIfExists` (默认) | 如果目标位置已经存在数据，那么抛出一个异常                   |
  | `SaveMode.Append`               | 如果目标位置已经存在数据，那么将数据追加进去                 |
  | `SaveMode.Overwrite             | 如果目标位置已经存在数据，那么就将已经存在的数据删除，用新数据进行覆盖 |
  | `SaveMode.Ignore`               | 如果目标位置已经存在数据，那么就忽略，不做任何操作。         |

- ~~~scala
  def mode(saveMode: String): DataFrameWriter[T] = {
  	saveMode.toLowerCase(Locale.ROOT) match {
  		case "overwrite" => mode(SaveMode.Overwrite)
  		case "append" => mode(SaveMode.Append)
  		case "ignore" => mode(SaveMode.Ignore)
  		case "error" | "errorifexists" | "default" => mode(SaveMode.ErrorIfExists)
  		case _ => throw new IllegalArgumentException(s"Unknown
  		save mode: $saveMode. Accepted " + "save modes are 'overwrite', 'append', 'ignore', 'error', 'errorifexists', 'default'.")
  	}
  }	
  ~~~

## 常见的数据格式

- `SparkSQL` 内建支持多种数据格式比如：`txt`、`csv`、`excel`(非内建)、`json`、`Parquet`、`ORC` 等

## 函数

### `UDF` 自定义函数

- `UDF(User Defined Function)`，自定义函数。函数的输入、输出都是一条数据记录， 类似于 `Spark SQL` 中普通的数学或字符串函数。

- 用 `Scala` 编写的 `UDF` 与普通的 `Scala` 函数几乎没有任何区别，唯一需要多执行的一个步骤是要在 `SQLContext` 注册它。如下就是一个简单的 `UDF`

- ~~~scala
  def main(args: Array[String]): Unit = {
  	val spark: SparkSession = SparkSession
  		.builder()
  		.appName("udf")
  		.master("local[*]")
  		.getOrCreate()
  	// 隐式转换
  	import spark.implicits._ 
  	val df = spark.sparkContext.makeRDD(List(("java", "Goslin"), ("hadoop", "zhangsan"), ("scala", "lisi"), ("spark", "wangwu"), ("hudi", "zhaoliu")))
  				.toDF("title", "author")
  	df.createTempView("books")
  	spark.udf.register("len", len _)
  	spark.sql("select title, author from books where len(title) >5").show()
  spark.udf.register("longLength", lengthLongerThan _)
  	// 若使用 df，则可以以字符串的形式将 UDF 传入
  	df.filter("longLength(title, 5)").show
  	spark.sql("select title, author from books where longLength(title, 10)").show
  }
  
  def len(bookTitle: String):Int = bookTitle.length
  def lengthLongerThan(bookTitle: String, length: Int): Boolean =
  bookTitle.length > length
  ~~~

### `UDAF` 自定义聚合函数

- `UDAF（User Defined Aggregation Funcation）`，用户自定义聚合函数。函数本身作用于数据集合，能够在聚合操作的基础上进行自定义操作（多条数据输入，一条数据输出）；类似于在 `group by` 之后使用的 `sum`、`avg` 等函数；

- 使用源码中推荐的抽象类 `Aggregator[IN, BUF, OUT]`。

- 代码示例

  ~~~scala
  object TypeSafeUDAFTest {
  	def main(args: Array[String]): Unit = {
  		val spark = SparkSession.builder()
  		.appName(s"${this.getClass.getCanonicalName}")
  		.master("local[*]")
  		.getOrCreate()
  	import spark.implicits._ 
  	val sales = Seq(
  		Sales(1, "Widget Co", 1000.00, 0.00, "AZ", "2019-01-02"), Sales(2, "Acme Widgets", 2000.00, 500.00, "CA", "2019-02-01"), Sales(3, "Widgetry", 1000.00, 200.00, "CA", "2020-01-11"), Sales(4, "Widgets R Us", 2000.00, 0.0, "CA", "2020-02-19"), Sales(5, "Ye Olde Widgete", 3000.00, 0.0, "MA", "2020-02-28")
  )
  	val ds = spark.createDataset(sales)
  	// name 会作为列名
  	val rate: TypedColumn[Sales, Double] = new
  	TypeSafeUDAF().toColumn.name("rate")
  	ds.select(rate).show
  }
  
  // 商品类
  case class Sales(id: Int, name: String, sales: Double, discount:
  Double, state: String, stime: String)
  // 中间结果类
  case class SalesBuffer(var sales2019: Double, var sales2020:Double)
  
  class TypeSafeUDAF extends Aggregator[Sales, SalesBuffer, Double] {
  	// 定义初值
  	override def zero: SalesBuffer = SalesBuffer(0, 0)
  	// 定义 buffer 编码器
  	override def bufferEncoder: Encoder[SalesBuffer] = Encoders.product
  	// 定义输出结果的编码器
  	override def outputEncoder: Encoder[Double] = Encoders.scalaDouble
  	// 分区内数据合并
  	override def reduce(buf: SalesBuffer, inputRow: Sales): SalesBuffer = {
  		val sales: Double = inputRow.sales
  		val year: String = inputRow.stime.take(4)
  		year match {
  			case "2019" => buf.sales2019 += sales
  			case "2020" => buf.sales2020 += sales
  			case _ => println("ERROR!")
  		}
  		buf
  	}
  	// 分区间数据合并
  	override def merge(part1: SalesBuffer, part2: SalesBuffer):SalesBuffer = {
  		val sales1 = part1.sales2019 + part2.sales2019
  		val sales2 = part1.sales2020 + part2.sales2020
  		SalesBuffer(sales1, sales2)
  	}
  	// 最终结果的计算
  	override def finish(reduction: SalesBuffer): Double = {
  		if (math.abs(reduction.sales2019) < 0.0000000001)
  			0
  		else {
  			(reduction.sales2020 - reduction.sales2019) / reduction.sales2019
  		}
  	}
  }
  ~~~

## `Catalyst`

- 我们写的 `SQL` 语句，会经过一个优化器（`Catalyst`），转化为 `RDD`，交给集群执行。

- `SQL` 到 `RDD` 中间经过了一个 `Catalyst`，它就是 `Spark SQL` 的核心，是针对 `Spark SQL`语句执行过程中的查询优化框架，基于 `Scala` 函数式编程结构。

- `RDD` 的运行流程：`RDD`-> `DAGScheduler`-> `TaskScheduler` -> `worker`，任务会按照代码所示运行, 依赖开发者的优化, 开发者的会在很大程度上影响运行效率。而 `SparkSQL`的 `Dataset` 和 `SQL` 并不是直接生成计划交给集群执行, 而是经过 `Catalyst` 的优化器, 这个

  优化器能够自动帮助开发者优化代码
